{
  "hash": "9172e6810d54913e5e51da42581ee6a0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Arctic regions geospatial wrangling\nsubtitle: Week 7 - Discussion section\nweek: 7\nimage: images/Medium_WW214867.webp\nsidebar: false\njupyter: mpc-env-kernel\n---\n\n\nIn this discussion section you will wrangle geospatial data about Arctic communities\n<!--\n and:\n\n- Breaking down a question into accessible data wrangling steps\n- Importing and plotting differnt types of geospatial data\n- Finding additional guidance online to carry out your data wrangling plans \n-->\n\n## Setup\n\n:::{.callout-tip appearance=\"minimal\"}\n1. Access the workbench-1 server.\n\n2. Create a new Python notebook inside your `eds-220-sections` directory and rename it to `section-7-arctic-communities.ipynb`. \n\n3. Use the terminal to push this file to your remote repository. \n:::\n\n## General directions\n:::{.callout-tip appearance=\"minimal\"}\n- Add comments as appropriate along your code following the course commenting standards.\n- Include markdown cells in between your code cells to add titles and information to each exercise\n- Commit every time you finish a major step. **Remember to write informative commits in the imperative mood.**\n:::\n\n## About the data\nArctic communities hold immense value in traditional knowledge and environmental stewardship, offering unique insights into sustainable practices and ecosystem management in one of the planet's most extreme environments. For this section you will use a dataset derived from the [list of Arctic communities and their location](https://search.dataone.org/view/doi%3A10.18739%2FA28S4JQ80) @brook_approximate_2023 created by the [Alaska Native Tribal Health Consortium](https://www.anthc.org) and [Natural Earth's medium-scale cultural boundaries data for countries (1:50m)](https://www.naturalearthdata.com/downloads/50m-cultural-vectors/) following the procedure in the [Reprojecting notes](/book/chapters/lesson-12-merge-data/lesson-12-merge-data.qmd).\n\n![Image Source: [Arctic Communities WWF.](https://www.arcticwwf.org/our-priorities/arctic-communities/) ©Staffan Widstrand/WWF.](/discussion-sections/images/Medium_WW214867.webp){width='90%'}\n\n\nThe data is in the `arctic_communities.geojson` file located in the `data/` directory for the EDS 220 class within workbench-1. Each geospatial feature in the data represents an Arctic territory with the following attributes:\n\n| Attribute | Description |\n|------|-----|\n| admin |  name of the territory  |\n| country | two-letter code|\n| n_communities | number of Arctic communities in the territory | \n\n## 1. Data loading and exploration\nRead in the data into a variable named `df` and examine it with your team.\n\n## 2. Brainstorm\nThe goal of these exercises is to refine the Arctic communities choropleth map created in the [Reprojecting lesson](/book/chapters/lesson-12-merge-data/lesson-12-merge-data.qmd) to restrict the plotting to the Arctic relevant regions:\n\n![](/discussion-sections/images/update_arctic_maps.png)\n\na. Individually, write down high-level steps on how you would explore and wrangle the data to produce the updated map. Do not code anything yet. \n\nb. Discuss your high-level steps with your team. What do you see as potential challenges to implementing your plan?\n\nThe next exercises will guide you through selecting relevant Arctic regions. There are *many* ways of doing this. The one presented here might not be the same way you thought about doing it - that’s ok! This one was designed to practice creating functions.\n\n## 2. Check geometry types\na. Run `df.geom_type`. Write a brief explanation about the output in a markdown cell.\n\nb. Create an `if-else` statement that: \n\n    i. prints \"All features are polygons.\" if all the features in the `df` are polygons and \n\n    ii. prints \"Multiple feature types: \" followed by the unique geometry types (no repetition) in the geodataframe if not all the features are polygons.\n\nc. Wrap up your code into a function named `check_polygons` that receives a single geodataframe as its parameter and prints out a message stating whether all the geometry types are polygons or not.\n\n\n## 3. Explode polygons\n\na. Overwrite the `df` geodataframe with the output from the [`explode`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explode.html) method with the `index_parts` parameter set to `False`. \nRead the documentation for the method and use a markdown cell to write a brief explanation of what is being done.\n\nb. Reset the index of `df`.\n\nc. Use your `check_polygons` function to verify that `df` only has features of type polygon.\n\n<p style=\"text-align: center;\">\n**Don't forget to write informative commits in the imperative every time you finish a major step.**\n</p>\n\n## 4. Compute minimum y-coordinate for polygons\n\nAt this point, every row in your `df` should be a single polygon. \n\na. Select the first row of `df` using `iloc`. What kind of Python object is this? \n\nb. Select the geometry of the first row of `df`. What kind of Python object is this?\n\nc. Use the [`bounds`](https://shapely.readthedocs.io/en/2.0.6/reference/shapely.bounds.html) attribute for `shapely Polygons` to select the southern-most bound of the first polygon in `df`.\n\nd. Create a function `min_y` that receives **a single row** of a geodataframe as its parameter and returns the minimum y-coordinate of its bounding box. \n\n\ne. Use the `min_y` function and the [`apply`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) method for data frames to create a new column `miny` in `df` which has the minimum `y` coordinate.\n\n## 6. Filter, update CRS, and reproduce map\n\na. Select the polygons with a bounding box at or above 40 degrees of latitude into a new variable named `arctic`. \n\nb. Reproduce the Arctic communities map by updating the CRS to EPSG:3413.\n\n",
    "supporting": [
      "ds6-arctic-communities_files"
    ],
    "filters": [],
    "includes": {}
  }
}