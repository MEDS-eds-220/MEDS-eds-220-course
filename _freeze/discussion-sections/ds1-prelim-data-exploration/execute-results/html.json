{
  "hash": "3a7160709901b7f91ad7352c66ecd16e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Prey species in the California drylands\nsubtitle: Week 1 - Discussion section\nweek: 1\nimage: images/Tecopa_site2.JPG\nsidebar: false\njupyter:\n  jupytext:\n    text_representation:\n      extension: .qmd\n      format_name: quarto\n      format_version: '1.0'\n      jupytext_version: 1.17.3\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n---\n\nThis discussion section will guide you through preliminary data exploration for a real world dataset about animal observations in the California drylands. In this discussion section, you will:\n\n- Collaborate with your new team!\n- Practice version control using git via the terminal\n- Obtain information about a dataset from an online data repository\n- Use the `pandas.read_csv()` function for loading files directly from a URL\n- Use `pandas.DataFrame` methods to do preliminary analysis\n\n## Teamwork\n:::{.callout-tip appearance=\"minimal\"}\nTo help everyone talk through ideas, catch errors early, and learn multiple ways to solve a problem, we will be working in  teams each week. \n\nThere are two different roles every person in the team can have during discussion section: the **driver** or the **navigator**. There will be one driver and three navigators per discussion section. \n\nThe driver will:\n\n - plug their computer into the monitor at your table \n - be responsible for writing the code that the team comes up with \n \n *Note: everyone in the team must write the code individually as well, but the person coding on the monitor will be typing on the notebook that the whole group works on together.*\n \nThe navigators will:\n\n- read the prompts\n- spot errors\n- ask \"what if\" questions such as:\n\n\n>1. “Can you narrate what you’re typing so I can follow?”\n2. “I’m thinking the next step is … because …”\n3. “Let’s pause and predict the output before we run it.”\n4. “I’m not sure—can we check the documentation together?”\n5. “What evidence do we have that this worked?”\n\nHappy coding!\n:::\n\n## General directions\n\n:::{.callout-tip appearance=\"minimal\"}\n- On each exercise, include markdown cells in between your code cells to add titles and information.\n- Indications about when to commit and push changes are included, but you are encouraged to commit and push more often. \n- You won't need to upload any data.\n:::\n\n## Setup\n\n:::{.callout-tip appearance=\"minimal\"}\n1. In the workbench-1 server, start a new JupyterLab session or access an active one.\n\n2. In the terminal, use `cd` to navigate into the `eds-220-sections` directory. Use `pwd` to verify `eds-220-sections` is your current working directory.\n\n3. Create a new Python notebook inside your `eds-220-sections` directory and rename it to `section-1-data-selection-drylands.ipynb`. \n\n4. Use the terminal to stage, commit, and push this file to the remote repository. Remember:\n    1. `git status` : check git status\n    2. `git add FILE-NAME` : stage updated file\n    3. `git status` : check git status again to confirm\n    4. `git commit -m \"Commit message\"` : commit with message\n    5. `git pull` : check local repo is up to date (best practice)\n    5. `git push` : push changes to upstream repository\n\n<p style=\"text-align: center;\">\n**CHECK IN WITH YOUR TEAM** \n</p>\n<p style=\"text-align: center;\">\n**MAKE SURE YOU'VE ALL SUCCESSFULLY SET UP YOUR NOTEBOOKS BEFORE CONTINUING**\n</p>\n:::\n\n## About the data\nFor these exercises we will use data about [prey items for endangered terrestrial vertebrate species within central California drylands](https://knb.ecoinformatics.org/view/doi%3A10.5063%2FF1VM49RH)@king_compiled_2023 @lortie_importance_2023.\n\nThis dataset is stored in the [Knowledge Network for Biocomplexity (KNB)](https://knb.ecoinformatics.org) data repository. This is an international repository intended to facilitate ecological and environmental research. It has thousands of open datasets and is hosted by the [National Center for Ecological Analysis and Synthesis (NCEAS)](http://NCEAS.ucsb.edu).\n\n![Data collection plot at Mojave Desert near Tecopa. Photo courtesy of Dr. Rachel King.](/discussion-sections/images/Tecopa_site2.JPG)\n\n## 1. Archive exploration\nWhen possible, data exploration should start at the data repository. Take some time to look through the dataset's description in the KNB data repository. Discuss the following questions with your team:\n\na. What is this data about?\nb. Is this data collected in-situ by the authors or is it a synthesis of multiple datasets?\nc. During what time frame were the observations in the dataset collected?\nd. Does this dataset come with an associated metadata file?\ne. Does the dataset contain sensitive data?\n\nIn your notebook: use a markdown cell to add a brief description of the dataset, including a citation, date of access, and a link to the archive. \n\n<p style=\"text-align: center;\">\n**check git status -> stage changes -> check git status -> commit with message -> pull -> push  changes**\n</p>\n\n## 2. Metadata exploration\n\nYou may have noticed there are two metadata files: `Compiled_occurrence_records_for_prey_items_of.xml` and `metadata_arth_occurrences.csv`. \nThe `.xml` document file type is `EML` which stands for [EML: Ecological Metadata Language](https://eml.ecoinformatics.org). This is a machine-readable file that has metadata about the whole dataset. In this section we will only use the metadata in the CSV file.\n\nBack in your notebook, import the `pandas` package using standard abbreviation in a code cell. Then follow these steps to read in the metadata CSV using the `pandas.read_csv()` function:\n\na. Navigate to the [data package site](https://knb.ecoinformatics.org/view/doi%3A10.5063%2FF1VM49RH) and copy the URL to access the `metadata_arth_occurrences` CSV file. To copy the URL: \n\n- hover over the Download button –> right click –> “Copy Link\".\n\nb. Read in the data from the URL using the `pd.read_csv()` function like this:\n\n    ```python\n    # Access metadata from repository\n    pd.read_csv('the URL goes here')\n    ```\n\nc. Take a minute to look at the descriptions for the columns. \n\n**Note:** Not all datasets have column descriptions in a CSV file. Often they come with a `.doc` or `.txt` file with information. \n\n\n## 3. Data loading\na. Follow steps (a) and (b) from the previous exercise to read in the drylands prey data file `arth_occurrences_with_env.csv` using `pd.read_csv()`. Store the dataframe to a variable called `prey` like this:\n\n```python\n# Load data\nprey = pd.read_csv('the URL goes here')\n```\n\nb. What is the type of the `prey` variable? Use a Python function get this information.\n\n<p style=\"text-align: center;\">\n**check git status -> stage changes -> check git status -> commit with message -> pull -> push  changes**\n</p>\n\n<p style=\"text-align: center;\">\n**CHECK IN WITH YOUR TEAM** \n</p>\n<p style=\"text-align: center;\">\n**MAKE SURE YOU'VE ALL SUCCESSFULLY ACCESSED THE DATA BEFORE CONTINUING**\n</p>\n\n## 4. Look at your data\n\na. Run `prey` in a cell. What do you notice in the columns section?\n\nb. To see all the column names in the same display we need to set a `pandas` option. Run the following command and then look at the `prey` data again:\n```python\npd.set_option(\"display.max.columns\", None)\n```\n\nc. Add a comment explaining what `pd.set_option(\"display.max.columns\", None)` does. \n\n<p style=\"text-align: center;\">\n**check git status -> stage changes -> check git status -> commit with message -> pull -> push  changes**\n</p>\n\n## 5. `pd.DataFrame` preliminary exploration\n\nRun each of the following methods for `prey` in a different cell and write a brief description of what they do as a comment: \n\na. `head()`\nb. `tail()`\nc. `info()`\nd. `nunique()`\n\nFor example:\n\n```\n# head()\n# returns the first five rows of the data frame\nprey.head()\n```\n\nIf you're not sure about what the method does, try looking it up in the [`pandas.DataFrame` documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\ne. Check the [documentation for `head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html#pandas.DataFrame.head). If this function has any optional parameters, change the default value to get a different output. \n\nPrint each of the following attributes of `prey` in a different cell and write a brief explanation of what they are as a comment:\n\nf. `shape`\ng. `columns`\nh. `dtypes`\n\nIf you're not sure about what information is the attribute showing, look it up in the [`pandas.DataFrame` documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)!\n\n<p style=\"text-align: center;\">\n**check git status -> stage changes -> check git status -> commit with message -> pull -> push  changes**\n</p>\n\n## 6. Update column names\nChange the column names of `institutionCode` and `datasetKey` to `institution_code` and  `dataset_key`, respectively. Make sure you're actually updating the dataframe. HINT: look for the documentation on the `rename` method for `pandas.DataFrames`.\n\n\n## 7. Subsetting \nComplete the following subsetting tasks.\n\n1. Select only the preservered specimes from `prey`. \n2. Select observations between 1992 and 2000\n3. Select rows 5, 15, 25, and 35\n4. Select all rows for columns `species` and `eventDate`.\n\n\n\n<p style=\"text-align: center;\">\n**check git status -> stage changes -> check git status -> commit with message -> pull -> push  changes**\n</p>\n\n",
    "supporting": [
      "ds1-prelim-data-exploration_files"
    ],
    "filters": [],
    "includes": {}
  }
}