[
  {
    "objectID": "commits-guidelines.html",
    "href": "commits-guidelines.html",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Write commit messages in the imperative mood (i.e., as if you‚Äôre giving a command). This is a widely accepted convention.\nExample: - Good: Add function to process user input - Bad: Added function to process user input or Adding function to process user input\n\n\n\nThe first line of the commit message should summarize the changes in around 50 characters or less. This makes it easy to scan in logs or on GitHub.\nExample:\nFix bug in data processing pipeline\n\n\n\nIf the commit requires further explanation, add a blank line after the summary and provide a more detailed description in the body. Focus on what and why, not necessarily how.\nExample:\nAdd user authentication check before data access\n\nPreviously, users could access the data without proper authentication.\nThis commit adds a check to ensure that only authenticated users can\naccess the sensitive data.\n\n\n\nMake sure the commit message explains the reason behind the change, not just what was changed. This helps future developers (or your future self) understand the motivation.\nExample:\nRefactor database query to improve performance\n\nThe previous query was causing significant lag for larger datasets.\nThis change optimizes the query and reduces execution time by 40%.\n\n\n\nUse a consistent format for commit messages throughout the project. If your team follows a particular convention (e.g., prefixing messages with issue numbers), stick to it.\nExample:\n# Consistent format with issue tracking\n[ISSUE-123] Fix broken link in user dashboard\n\n\n\nAlways start the commit message with a capital letter to keep it professional and uniform.\nExample: - Good: Update documentation for new feature - Bad: update documentation for new feature\n\n\n\nVague or uninformative commit messages are not helpful. Avoid commit messages like ‚ÄúFixes,‚Äù ‚ÄúUpdates,‚Äù ‚ÄúMiscellaneous changes,‚Äù or ‚ÄúWIP.‚Äù\nExample:\nBad: \"Fix stuff\"\nGood: \"Fix missing validation for email input\"\n\n\n\nA commit should represent a single logical change. Avoid lumping multiple unrelated changes into a single commit. Instead, break them up into separate, focused commits.\nExample: - Good: Commit the bug fix in one commit and the new feature in another. - Bad: Combine multiple changes in one commit with a message like ‚ÄúFix bug and add new feature.‚Äù\n\n\n\nIf applicable, reference issue numbers, bug reports, or pull requests in the commit message to tie the commit to the broader project context.\nExample:\nFix typo in data validation logic (#456)\n\n\n\nEven for small changes, write clear and meaningful messages. Something like Fix typo or Update README is fine for minor commits as long as it‚Äôs clear what the change is about.\n\n\n\nEnsure the code works as expected before committing, and write a commit message that reflects the final, working state of the change.\n\n\n\nCommit messages should be professional. Avoid using emojis, abbreviations, or casual language.\nExample: - Bad: üî• Fix all the things! üöÄ - Good: Fix critical bugs in data parsing logic\n\n\n\nRefactor user input validation to reduce redundancy\n\nThe validation logic for user input was repeated in multiple places.\nThis commit refactors the validation into a separate function that\ncan be reused across different modules. This improves code maintainability\nand reduces potential errors.\n\nCloses #123"
  },
  {
    "objectID": "commits-guidelines.html#use-the-imperative-mood",
    "href": "commits-guidelines.html#use-the-imperative-mood",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Write commit messages in the imperative mood (i.e., as if you‚Äôre giving a command). This is a widely accepted convention.\nExample: - Good: Add function to process user input - Bad: Added function to process user input or Adding function to process user input"
  },
  {
    "objectID": "commits-guidelines.html#keep-the-first-line-short-and-descriptive",
    "href": "commits-guidelines.html#keep-the-first-line-short-and-descriptive",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "The first line of the commit message should summarize the changes in around 50 characters or less. This makes it easy to scan in logs or on GitHub.\nExample:\nFix bug in data processing pipeline"
  },
  {
    "objectID": "commits-guidelines.html#provide-more-detail-in-the-body-if-necessary",
    "href": "commits-guidelines.html#provide-more-detail-in-the-body-if-necessary",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "If the commit requires further explanation, add a blank line after the summary and provide a more detailed description in the body. Focus on what and why, not necessarily how.\nExample:\nAdd user authentication check before data access\n\nPreviously, users could access the data without proper authentication.\nThis commit adds a check to ensure that only authenticated users can\naccess the sensitive data."
  },
  {
    "objectID": "commits-guidelines.html#explain-the-why-and-the-impact",
    "href": "commits-guidelines.html#explain-the-why-and-the-impact",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Make sure the commit message explains the reason behind the change, not just what was changed. This helps future developers (or your future self) understand the motivation.\nExample:\nRefactor database query to improve performance\n\nThe previous query was causing significant lag for larger datasets.\nThis change optimizes the query and reduces execution time by 40%."
  },
  {
    "objectID": "commits-guidelines.html#be-consistent",
    "href": "commits-guidelines.html#be-consistent",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Use a consistent format for commit messages throughout the project. If your team follows a particular convention (e.g., prefixing messages with issue numbers), stick to it.\nExample:\n# Consistent format with issue tracking\n[ISSUE-123] Fix broken link in user dashboard"
  },
  {
    "objectID": "commits-guidelines.html#capitalize-the-first-letter-of-the-summary",
    "href": "commits-guidelines.html#capitalize-the-first-letter-of-the-summary",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Always start the commit message with a capital letter to keep it professional and uniform.\nExample: - Good: Update documentation for new feature - Bad: update documentation for new feature"
  },
  {
    "objectID": "commits-guidelines.html#avoid-commit-messages-like-fixes-or-miscellaneous",
    "href": "commits-guidelines.html#avoid-commit-messages-like-fixes-or-miscellaneous",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Vague or uninformative commit messages are not helpful. Avoid commit messages like ‚ÄúFixes,‚Äù ‚ÄúUpdates,‚Äù ‚ÄúMiscellaneous changes,‚Äù or ‚ÄúWIP.‚Äù\nExample:\nBad: \"Fix stuff\"\nGood: \"Fix missing validation for email input\""
  },
  {
    "objectID": "commits-guidelines.html#group-related-changes-together",
    "href": "commits-guidelines.html#group-related-changes-together",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "A commit should represent a single logical change. Avoid lumping multiple unrelated changes into a single commit. Instead, break them up into separate, focused commits.\nExample: - Good: Commit the bug fix in one commit and the new feature in another. - Bad: Combine multiple changes in one commit with a message like ‚ÄúFix bug and add new feature.‚Äù"
  },
  {
    "objectID": "commits-guidelines.html#reference-relevant-issues-or-pull-requests",
    "href": "commits-guidelines.html#reference-relevant-issues-or-pull-requests",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "If applicable, reference issue numbers, bug reports, or pull requests in the commit message to tie the commit to the broader project context.\nExample:\nFix typo in data validation logic (#456)"
  },
  {
    "objectID": "commits-guidelines.html#use-short-informative-messages-for-small-changes",
    "href": "commits-guidelines.html#use-short-informative-messages-for-small-changes",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Even for small changes, write clear and meaningful messages. Something like Fix typo or Update README is fine for minor commits as long as it‚Äôs clear what the change is about."
  },
  {
    "objectID": "commits-guidelines.html#test-before-committing",
    "href": "commits-guidelines.html#test-before-committing",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Ensure the code works as expected before committing, and write a commit message that reflects the final, working state of the change."
  },
  {
    "objectID": "commits-guidelines.html#avoid-emoticons-and-slang",
    "href": "commits-guidelines.html#avoid-emoticons-and-slang",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Commit messages should be professional. Avoid using emojis, abbreviations, or casual language.\nExample: - Bad: üî• Fix all the things! üöÄ - Good: Fix critical bugs in data parsing logic"
  },
  {
    "objectID": "commits-guidelines.html#example-of-a-well-structured-commit-message",
    "href": "commits-guidelines.html#example-of-a-well-structured-commit-message",
    "title": "Guidelines for Writing Good Commit Messages",
    "section": "",
    "text": "Refactor user input validation to reduce redundancy\n\nThe validation logic for user input was repeated in multiple places.\nThis commit refactors the validation into a separate function that\ncan be reused across different modules. This improves code maintainability\nand reduces potential errors.\n\nCloses #123"
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html",
    "title": "Preliminary data exploration with pandas",
    "section": "",
    "text": "This discussion section will guide you through preliminary data exploration for a real world dataset about animal observations in the California drylands. In this discussion section, you will:"
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#setup",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#setup",
    "title": "Preliminary data exploration with pandas",
    "section": "Setup",
    "text": "Setup\n\n\n\n\n\n\n\nIn the Taylor server, start a new JupyterLab session or access an active one.\nIn the terminal, use cd to navigate into the eds-220-sections directory. Use pwd to verify eds-220-sections is your current working directory.\nCreate a new Python Notebook inside your eds-220-sections directory and rename it to section-1-data-selection-drylands.ipynb.\nUse the terminal to stage, commit, and push this file to the remote repository. Remember:\n\ngit status : check git status\ngit add FILE-NAME : stage updated file\ngit status : check git status again to confirm\ngit commit -m \"Commit message\" : commit with message\ngit pull : check local repo is up to date (best practice)\ngit push : push changes to upstream repository\n\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOU‚ÄôVE ALL SUCCESSFULLY SET UP YOUR NOTEBOOKS BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#general-directions",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#general-directions",
    "title": "Preliminary data exploration with pandas",
    "section": "General directions",
    "text": "General directions\n\n\n\n\n\n\n\nAdd comments in each one of your code cells.\nOn each exercise, include markdown cells in between your code cells to add titles and information.\nIndications about when to commit and push changes are included, but you are encouraged to commit and push more often.\nYou won‚Äôt need to upload any data."
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#about-the-data",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#about-the-data",
    "title": "Preliminary data exploration with pandas",
    "section": "About the data",
    "text": "About the data\nFor these exercises we will use data about prey items for endangered terrestrial vertebrate species within central California drylands[1] [2].\nThis dataset is stored in the Knowledge Network for Biocomplexity (KNB) data repository. This is an international repository intended to facilitate ecological and environmental research. It has thousands of open datasets and is hosted by the National Center for Ecological Analysis and Synthesis (NCEAS).\n\n\n\nData collection plot at Mojave Desert near Tecopa. Photo courtesy of Dr.¬†Rachel King."
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#archive-exploration",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#archive-exploration",
    "title": "Preliminary data exploration with pandas",
    "section": "1. Archive exploration",
    "text": "1. Archive exploration\nWhen possible, data exploration should start at the data repository. Take some time to look through the dataset‚Äôs description in the KNB data repository. Discuss the following questions with your team:\n\nWhat is this data about?\nIs this data collected in-situ by the authors or is it a synthesis of multiple datasets?\nDuring what time frame were the observations in the dataset collected?\nDoes this dataset come with an associated metadata file?\nDoes the dataset contain sensitive data?\n\nIn your notebook: use a markdown cell to add a brief description of the dataset, including a citation, date of access, and a link to the archive.\n\ncheck git status -&gt; stage changes -&gt; check git status -&gt; commit with message -&gt; pull -&gt; push changes"
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#metadata-exploration",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#metadata-exploration",
    "title": "Preliminary data exploration with pandas",
    "section": "2. Metadata exploration",
    "text": "2. Metadata exploration\nYou may have noticed there are two metadata files: Compiled_occurrence_records_for_prey_items_of.xml and metadata_arth_occurrences.csv. The .xml document file type is EML which stands for EML: Ecological Metadata Language. This is a machine-readable file that has metadata about the whole dataset. In this section we will only use the metadata in the CSV file.\nBack in your notebook, import the pandas package using standard abbreviation in a code cell. Then follow these steps to read in the metadata CSV using the pandas.read_csv() function:\n\nNavigate to the data package site and copy the URL to access the metadata_arth_occurrences CSV file. To copy the URL:\n\n\nhover over the Download button ‚Äì&gt; right click ‚Äì&gt; ‚ÄúCopy Link‚Äù.\n\n\nRead in the data from the URL using the pd.read_csv() function like this:\n# Access metadata from repository\npd.read_csv('the URL goes here')\nTake a minute to look at the descriptions for the columns.\n\nNote: Not all datasets have column descriptions in a CSV file. Often they come with a .doc or .txt file with information."
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#data-loading",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#data-loading",
    "title": "Preliminary data exploration with pandas",
    "section": "3. Data loading",
    "text": "3. Data loading\n\nFollow steps (a) and (b) from the previous exercise to read in the drylands prey data file arth_occurrences_with_env.csv using pd.read_csv(). Store the dataframe to a variable called prey like this:\n\n# Load data\nprey = pd.read_csv('the URL goes here')\n\nWhat is the type of the prey variable? Use a Python function get this information.\n\n\ncheck git status -&gt; stage changes -&gt; check git status -&gt; commit with message -&gt; pull -&gt; push changes\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOU‚ÄôVE ALL SUCCESSFULLY ACCESSED THE DATA BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#look-at-your-data",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#look-at-your-data",
    "title": "Preliminary data exploration with pandas",
    "section": "4. Look at your data",
    "text": "4. Look at your data\n\nRun prey in a cell. What do you notice in the columns section?\nTo see all the column names in the same display we need to set a pandas option. Run the following command and then look at the prey data again:\n\npd.set_option(\"display.max.columns\", None)\n\nAdd a comment explaining what pd.set_option(\"display.max.columns\", None) does.\n\n\ncheck git status -&gt; stage changes -&gt; check git status -&gt; commit with message -&gt; pull -&gt; push changes"
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#pd.dataframe-preliminary-exploration",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#pd.dataframe-preliminary-exploration",
    "title": "Preliminary data exploration with pandas",
    "section": "5. pd.DataFrame preliminary exploration",
    "text": "5. pd.DataFrame preliminary exploration\nRun each of the following methods for prey in a different cell and write a brief description of what they do as a comment:\n\nhead()\ntail()\ninfo()\nnunique()\n\nFor example:\n# head()\n# returns the first five rows of the data frame\nprey.head()\nIf you‚Äôre not sure about what the method does, try looking it up in the pandas.DataFrame documentation.\n\nCheck the documentation for head(). If this function has any optional parameters, change the default value to get a different output.\n\nPrint each of the following attributes of prey in a different cell and write a brief explanation of what they are as a comment:\n\nshape\ncolumns\ndtypes\n\nIf you‚Äôre not sure about what information is the attribute showing, look it up in the pandas.DataFrame documentation.\n\ncheck git status -&gt; stage changes -&gt; check git status -&gt; commit with message -&gt; pull -&gt; push changes"
  },
  {
    "objectID": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#update-column-names",
    "href": "discussion-sections-upcoming/ds1-prelim-data-exploration.html#update-column-names",
    "title": "Preliminary data exploration with pandas",
    "section": "6. Update column names",
    "text": "6. Update column names\nChange the column names of institutionCode and datasetKey to institution_code and dataset_key, respectively. Make sure you‚Äôre actually updating the dataframe. HINT: look for the documentation on the rename method for pandas.DataFrames.\n\ncheck git status -&gt; stage changes -&gt; check git status -&gt; commit with message -&gt; pull -&gt; push changes"
  },
  {
    "objectID": "book/preface.html",
    "href": "book/preface.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the ‚ÄòEDS 220 - Working with Environmental Datasets‚Äô course notes!\nDesigned for the UCSB Masters in Environmental Data Science (MEDS), this course will guide you through widely used environmental data formats and Python libraries for analyzing diverse environmental datasets.\nThe notes are organized following the increasing dimensions of different environmental datasets, from familiar tabular data to intricate multi-dimensional arrays. Through hands-on code and activities, you‚Äôll analyze real-world environmental datasets sourced from leading open data repositories and cloud platforms.\nWho is this course for? EDS 220 is tailored for beginner Python programmers eager to deepen their skills. If you‚Äôre familiar with the basics of Python and have experience working in Jupyter notebooks, you‚Äôre in the right place. You are also encouraged to bring along your git skills and GitHub profile, ready to practice the essential git pull - git push workflow as you progress.\nOverall, these notes are just the beginning! They offer a solid foundation but are, inevitably, a partial exposition of the incredibly vast ecosystem of data formats, repositories, and Python tools available in environmental data science. By the end of this course, you‚Äôll have a strong grasp of the fundamentals and also the confidence to dive deeper into using Python for environmental data science and continue your MEDS journey.\n\n\nA big thanks to the creators of the open-source Python libraries, datasets, and educational materials that have helped shape this course. Your contributions have made our learning journey richer and more impactful. Attribution is included in any materials where content is adapted from other resources.\n\n\n\nüìù If you have suggestions on how to correct, improve, or expand these notes, please feel free to email Carmen Galaz Garc√≠a at galazgarcia@bren.ucsb.edu or file a GitHub issue.\nüåü If these course materials have been useful to you, consider adding a star to the project‚Äôs repository.\n\n\n\nThis work, including the course notes, discussion sections, and assignments, is licensed under Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) License. For attribution, please cite these materials as:\n\nC. Galaz Garc√≠a, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\n\nor use the bib reference:\n@misc{galaz_garcia_eds_2024,\n    title = {EDS 220 - Working with Environmental Datasets, Course Notes},\n    url = {https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html},\n    author = {Galaz Garc√≠a, Carmen},\n    year = {2024},\n}",
    "crumbs": [
      "notes",
      "About"
    ]
  },
  {
    "objectID": "book/preface.html#acknowledgements",
    "href": "book/preface.html#acknowledgements",
    "title": "About",
    "section": "",
    "text": "A big thanks to the creators of the open-source Python libraries, datasets, and educational materials that have helped shape this course. Your contributions have made our learning journey richer and more impactful. Attribution is included in any materials where content is adapted from other resources.",
    "crumbs": [
      "notes",
      "About"
    ]
  },
  {
    "objectID": "book/preface.html#contribute",
    "href": "book/preface.html#contribute",
    "title": "About",
    "section": "",
    "text": "üìù If you have suggestions on how to correct, improve, or expand these notes, please feel free to email Carmen Galaz Garc√≠a at galazgarcia@bren.ucsb.edu or file a GitHub issue.\nüåü If these course materials have been useful to you, consider adding a star to the project‚Äôs repository.",
    "crumbs": [
      "notes",
      "About"
    ]
  },
  {
    "objectID": "book/preface.html#attribution",
    "href": "book/preface.html#attribution",
    "title": "About",
    "section": "",
    "text": "This work, including the course notes, discussion sections, and assignments, is licensed under Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) License. For attribution, please cite these materials as:\n\nC. Galaz Garc√≠a, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\n\nor use the bib reference:\n@misc{galaz_garcia_eds_2024,\n    title = {EDS 220 - Working with Environmental Datasets, Course Notes},\n    url = {https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html},\n    author = {Galaz Garc√≠a, Carmen},\n    year = {2024},\n}",
    "crumbs": [
      "notes",
      "About"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html",
    "href": "book/appendices/A-python-environments.html",
    "title": "Conda environments",
    "section": "",
    "text": "This hands-on lesson gives a brief introduction to Conda environments, focusing on practical usage. You can use this lesson as a standalone introduction to environments, ideally preceding the installation of the course-specific environment (see Setup section). The last section includes a table with the Conda commands used in this lesson for quick reference.\n\n\nBy the end of this lesson, students will be able to:\n\nDescribe what Conda environments are and their role in managing Python packages and dependencies\nUse standard Conda commands to list, activate, deactivate, create, and delete environments\nBuild Conda environments and install packages using the command line\nGenerate and edit a .yml file with environment specifications to enhance project reproducibility\n\n\n\n\nEnvironments are a way to keep the packages and Python versions you use for different projects organized.\n\n\n\nSome environments can have lots of packages and dependencies, while others can keep it simple.\n\n\nThe main reasons to create an environment for each of your projects are:\n\nTo not interfere with your computer‚Äôs pre-installed Python\nPackages usually depend on other packages to work properly, this is called a package dependency. Dependencies across different packages need to be carefully managed and may potentially be different across projects.\nReproducibility! Being able to share your code and what it needs to run it with others\n\n\n\n\nConda is an environment and package management system: it can both create and administer the environments and install compatible versions of software and their dependencies within an environment.\nEnvironments created with Conda are usually called Conda environments. A Conda environment doesn‚Äôt need to be a Python environment, Conda can manage packages for any programming language.\n\n\n\n\n\n\n\n\nConda channels are the remote locations where packages are stored. Think of them as shops for getting packages. By default, packages are downloaded to your computer and updated from the conda default channel. But there are others! Conda-forge and bioconda are two popular ones. We can choose which Conda channel to install a package from.\n\n\n\npip is a package management system only for Python. We can use it to install packages from the Python Package Index (PyPI). We can use pip inside a Conda environment when a package is not available from a Conda channel.\n\n\n\n\n\n\n\n\nThe following exercises will guide you through the basic commands to work with Conda environments. Unless otherwise specified, all the commands should be run in the command line. For a deeper dive after completing this introduction, check out the Conda documentation.\n\n\nTo list all the Conda environments available in your computer and their location we use:\nconda env list\nThe output should look something like this:\n# conda environments:\n#\nbase                  *  /Users/galaz-garcia/opt/anaconda3\neds220-env               /Users/galaz-garcia/opt/anaconda3/envs/eds220-env\nNotice the file path next to the environment name. This is the absolute path to the environment‚Äôs installation directory on my local machine. This is where Conda has created the environment and stored the packages!\nWhat is my currently active environment?\n\nWhen you run conda env list, the asterisk next to the environment path indicates which environment is active. In the previous example, I am using the base Python environment.\nThe currently active environment also appears in the terminal in parenthesis at the beginning of each line, something like this:\n\n(base) my-computer:MEDS-eds-220-course galaz-garcia$\n\n\n\nTo create a new environment called test-env and, within it, install the most recent version of Python we simply run:\nconda create --name test-env python\nWhen you run this command, it will print out information about the packages that will be installed and ask you whether to proceed or not. Type y and press enter to create the environment and install the packages.\n\n\n\n\n\n\nCheck-in\n\n\n\nCheck whether the new test-env environment is listed by Conda. Is it activated or not?\n\n\n\n\n\nTo activate the test-env environment we use the command\nconda activate test-env\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify that test-env is now your current environment.\n\n\n\n\n\nTo see which packages are installed within the currently active environment we run:\nconda list\nThe output will be a long list that looks something like this:\n# packages in environment at /Users/galaz-garcia/opt/anaconda3/envs/test-env:\n#\n# Name                    Version                   Build  Channel\nbzip2                     1.0.8                h6c40b1e_6  \npip                       24.2            py312hecd8cb5_0  \nsqlite                    3.45.3               h6c40b1e_0  \n[...]\nThe name and version of each installed package in the environment appear in their respective columns. The ‚ÄòBuild‚Äô column shows a build string for each package. This string encodes important information about how the package was compiled, including any updates or changes made without altering the package version. It also reflects any compatibility or performance optimizations that were integrated during the build process.\nFor example, the same package with the same version might have different builds (and therefore different build strings) depending on whether it was installed on macOS, Windows, or Linux. This difference arises because the package may need to be compiled differently to optimize performance and ensure compatibility with each operating system.\nThe channel column shows the source from which the package was downloaded and installed. In this example the entries on this column are blank, signaling these packages were downloaded from the defeault Conda channel. We‚Äôll see other channels appear in the next examples.\nWe can also request information about a specific package. For example, if we run\nconda list pip\nwe obtain only the information about the pip package:\n# packages in environment at /Users/galaz-garcia/opt/anaconda3/envs/test-env:\n#\n# Name                    Version                   Build  Channel\npip                       24.2            py312hecd8cb5_0  \nWe can use the same command to verify whether a package is installed. For example, if we run\nconda list numpy\nwe will get empty columns:\n# packages in environment at /Users/galaz-garcia/opt/anaconda3/envs/test-env:\n#\n# Name                    Version                   Build  Channel\nbecause the numpy package is not intalled in this environment.\n\n\n\nSuppose we want to install numpy in our currently active environment test-env. To do this we can simply use the command:\nconda install numpy\nSince we have not specified another channel, this command will install numpy from the default channel.\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify that numpy is now installed.\n\n\nNow, suppose we want to install rioxarray, this is a Python package for raster data analysis. This is a relatively new package, so it might not be available from the default Conda channels. A sensible measure would be to first look if rioxarray is available in the defaults channels, which we can do by running:\nconda search rioxarray\nThe output will look similar to\nLoading channels: done\nNo match found for: rioxarray. Search: *rioxarray*\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n  - rioxarray\n\nCurrent channels:\n\n  - https://repo.anaconda.com/pkgs/main/osx-64\n  - https://repo.anaconda.com/pkgs/main/noarch\n  - https://repo.anaconda.com/pkgs/r/osx-64\n  - https://repo.anaconda.com/pkgs/r/noarch\n\nTo search for alternate channels that may provide the Conda package you're\nlooking for, navigate to\n\n    https://anaconda.org\n\nand use the search bar at the top of the page.\nAs stated, rioxarray is not available on the default channels for my Conda installation. Notice a couple of the channels it is seraching on are specific to macOS (my current operating system). The last two channels are actually channels for R packages. If we had tried to install rioxarray using conda install rioxarray we would have obtained an error message with similar information (you can try it if you want!).\nWe may have better luck searching for rioxarray in the conda-forge channel, which we can do like this:\nconda search --channel conda-forge rioxarray\nThe result will be a list of available versions and builds of this package in the conda-forge channel:\nLoading channels: done\n# Name                       Version           Build  Channel             \nrioxarray                      0.0.3            py_0  conda-forge         \nrioxarray                      0.0.4            py_0  conda-forge         \nrioxarray                      0.0.5            py_0  conda-forge     \n[...]\nrioxarray                     0.16.0    pyhd8ed1ab_0  conda-forge         \nrioxarray                     0.17.0    pyhd8ed1ab_0  conda-forge   \n\n\n\n\n\n\nCheck-in\n\n\n\nDo you see any versions of rioxarray with multiple builds?\n\n\nSince we now know that we can find rioxarray in the conda-forge channel, we can go ahead and install it from this channel using the command\nconda install --channel conda-forge rioxarray\nYou‚Äôll see a moving bar that says Conda is solving (the) environment - this is Conda doing its package management job! It means Conda is working on:\n\nfinding the dependencies for the packages you are trying to install,\nfinding versions of these dependencies that are compatible with each other,\nmaking sure these new packages are compatible with the packages already present in the environment,\ndowngrade, upgrade, or remove packages as needed in the environment.\n\nOverall, Conda tries to install what we need with the least disruption. Solving an environment can often take time but it is crucial so that the environment remains stable and functional.\n\n\n\n\n\n\nCheck-in\n\n\n\n\nCheck whether pandas is installed in the environment.\nWhich channels were used to install dependencies for rioxarray?\n\n\n\n\n\n\n\nTo deactivate our current environment and return to our base environment we simply run\nconda deactivate\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify we are no longer in the test-env. What environment are we on?\n\n\n\n\n\nSince this was a test environment, we can go ahead and delete all of it using the command:\nconda remove --name test-env  --all\nIt will ask you whether you want to proceed. Type y and press enter to go ahead and delete the environment.\nWe can run conda env list to verify the test-env does not exist anymore.\n\n\n\nWe can also create a new environment by specifying the package versions and channel priorities. For example, to install the numpy and rioxarray versions we previously had in a new environment called my-env we run:\nconda create --name my-env rioxarray=0.17.0 numpy=1.26.4 --channel conda-forge\nNotice we are explicitely requiring certain versions of our packages, and the --channel conda-forge option is asking Conda to prioritize getting the packages from conda-forge.\n\n\n\n\n\n\nCheck-in\n\n\n\nTake a look at the dependencies that were installed with these two packages.\n\n\n\n\n\nOne of the main goals of using an environment to manage the packages used in your project is being able to share the environment so that other people can rerun your code and reproduce your results.\n‚ÄúSharing the environment‚Äù means sharing instructions that Conda can use to recreate the environment you have. These instructions are, basically, a list of the packages installed in the environment, their versions, which channels were used to install them, and, optionally, build specifications.\nThere are several ways of creating these instructions to reconstruct an environment depending on the level of cross-platform compatibility we are looking for. However, each of the following methods to export the environment specifications will generate a new YAML file. This file type with .yml extension is human-readable markup language that is commonly used for configuration in software applications.\n\n\n\n\n\n\nNavigate to project directory\n\n\n\nEach of the commands for exporting the environment specifications will create a .yml file in our terminal‚Äôs current working directory. Remember you can use pwd (Mac & Linux) or echo %cd% (Windows CMD) on the command line to check which is your current working directory. If you need to change directories you can use cd navigation in the terminal.\nThe best practice is to have the environment.yml file in the directory that holds your project. This way, you can use version control on it and push it to your remote repository.\n\n\n\n\nThe first method to export the environment specifications is running\nconda env export &gt; environment.yml\nThe environment.yml is the name of the YAML file that will have the environment specifications.\nOpen the environment.yml file that was generated, it will look similar to this excerpt:\nname: my-env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - affine=2.4.0=pyhd8ed1ab_0\n  - attrs=24.2.0=pyh71513ae_0\n  [...]\n  - zlib=1.3.1=h87427d6_1\n  - zstd=1.5.6=h915ae27_0\nprefix: /Users/galaz-garcia/opt/anaconda3/envs/my-env\nHere we can see the name of the environment, the channels used to install packages (in order of priority), and the package names, each with a version and build string. This ensures that the environment can be recreated exactly as it is on another system. This first method to export the environment specifications is the most detailed and it is both platform and package specific.\n\n\n\n\n\n\nDelete prefix line before sharing environment\n\n\n\nNotice the last line in the environment.yml file, this prefix indicates the absolute path to the environment‚Äôs installation on your local machine.\nYou should delete the prefix line when sharing the environment file. Leaving it there is unnecessary and could cause confusion since it points to specific directroy in your local machine.\n\n\n\n\n\nOur second method to export the environment specifications is to run\nconda env export --from-history &gt; environment_hist.yml\nHere I named the file environment_hist.yml to not overwrite the previous environmnet.yml, that way you can compare both of them. Open the environment_hist.yml file, it will look similar to this (much shorter than the previous one):\nname: my-env\nchannels:\n  - defaults\ndependencies:\n  - numpy=1.26.4\n  - rioxarray=0.17.0\nprefix: /Users/galaz-garcia/opt/anaconda3/envs/my-env\nAdding the --from-history option tells Conda to only include packages that we explicitely installed (in this case numpy=1.26.4 and rioxarray=0.17.0), omitting any dependencies that were automatically installed and channel information. This will create a minimal environment file that lists only the packages you explicitly installed, making it simpler and more portable for sharing with others. While this method ensures reproducibility of the core setup, Conda will resolve dependencies on its own and these might not be the same as your original environment.\n\n\n\n\n\n\nAdding channels to environment.yml file\n\n\n\nIn section 5 we saw that the rioxarray package is available on the conda-forge channel, but not the defaults channel. The output of conda env export with the --from-history option only includes the defaults channel, so we wouldn‚Äôt be able to recreate the environment with that file as is. Remember the .yml file can be manually modified and we can edit it to include conda-forge as the first priority channel, followed by the defaults channel. Rememember to also delete the prefix line before sharing. The modified version of the --from-history output that will allow us to recreate the environment will be:\nname: my-env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - numpy=1.26.4\n  - rioxarray=0.17.0\n\n\nBefore moving to the last exercise, delete the environment.yml and environment_hist.yml files.\n\n\n\n\n\nFor this last exercise, click here and download the environment.yml file in this repository. Move to a directory for a mock project and navigate to that directory in the terminal.\n\nTo create an environment from specifications in a .yml file we run\nconda env create --name test-env --file environment.yml\nThen --name option states how we want to name our environment (test-env in this case). If no --name option is given, Conda will name of the environment using the name field in the environment.yml file.\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify that the new environment was created, activate it, and take a look at the installed packages.\n\n\n\n\n\n\nThe following table includes the commands covered in this lesson‚Äôs exercises. Check out Conda‚Äôs cheatsheet for more commands and tips.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nconda env list\nlist available Conda environments\n\n\nconda create --name ENVNAME python\ncreate a new Python environment named ENVNAME\n\n\nconda remove --name ENVNAME  --all\ndelete ENVNAME environment\n\n\nconda activate ENVNAME\nactivate environment\n\n\nconda deactivate\ndeactivate active environment\n\n\npython -V\nprint Python version installed in active environment\n\n\nconda list\nlist installed packages in active environment\n\n\nconda list PKGNAME\ncheck if PKGNAME package is installed in active environment\n\n\nconda install PKGNAME\ninstall PKGNAME package in environment using environment channel priorities (generally just from defaults channel)\n\n\nconda install --channel CHANNELNAME PKGNAME\ninstall PKGNAME package in environment from from CHANNELNAME channel\n\n\nconda search PKGNAME\nsearch for PKGNAME package in environment‚Äôs channels\n\n\nconda search --channel CHANNELNAME PKGNAME\nsearch for PKGNAME package in CHANNELNAME channel\n\n\nconda env export &gt; ENV.yml\nexport environment specifications of currently active environmemt into ENV.yml file (includes all dependencies, buildstrings, and channels)\n\n\nconda env export --from-history &gt; ENV.yml\nexport environment specifications of currently active environmemt into ENV.yml file (only packages explicitely installed, no dependencies, buildstrings, or channels)\n\n\nconda env create --name ENVNAME --file ENV.yml\ncreate an environment named ENVNAME from specifications in ENV.yml file",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html#learning-objectives",
    "href": "book/appendices/A-python-environments.html#learning-objectives",
    "title": "Conda environments",
    "section": "",
    "text": "By the end of this lesson, students will be able to:\n\nDescribe what Conda environments are and their role in managing Python packages and dependencies\nUse standard Conda commands to list, activate, deactivate, create, and delete environments\nBuild Conda environments and install packages using the command line\nGenerate and edit a .yml file with environment specifications to enhance project reproducibility",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html#environments-what-and-why",
    "href": "book/appendices/A-python-environments.html#environments-what-and-why",
    "title": "Conda environments",
    "section": "",
    "text": "Environments are a way to keep the packages and Python versions you use for different projects organized.\n\n\n\nSome environments can have lots of packages and dependencies, while others can keep it simple.\n\n\nThe main reasons to create an environment for each of your projects are:\n\nTo not interfere with your computer‚Äôs pre-installed Python\nPackages usually depend on other packages to work properly, this is called a package dependency. Dependencies across different packages need to be carefully managed and may potentially be different across projects.\nReproducibility! Being able to share your code and what it needs to run it with others",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html#conda-environments-1",
    "href": "book/appendices/A-python-environments.html#conda-environments-1",
    "title": "Conda environments",
    "section": "",
    "text": "Conda is an environment and package management system: it can both create and administer the environments and install compatible versions of software and their dependencies within an environment.\nEnvironments created with Conda are usually called Conda environments. A Conda environment doesn‚Äôt need to be a Python environment, Conda can manage packages for any programming language.",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html#conda-channels",
    "href": "book/appendices/A-python-environments.html#conda-channels",
    "title": "Conda environments",
    "section": "",
    "text": "Conda channels are the remote locations where packages are stored. Think of them as shops for getting packages. By default, packages are downloaded to your computer and updated from the conda default channel. But there are others! Conda-forge and bioconda are two popular ones. We can choose which Conda channel to install a package from.",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html#pip",
    "href": "book/appendices/A-python-environments.html#pip",
    "title": "Conda environments",
    "section": "",
    "text": "pip is a package management system only for Python. We can use it to install packages from the Python Package Index (PyPI). We can use pip inside a Conda environment when a package is not available from a Conda channel.",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html#hands-on-environments",
    "href": "book/appendices/A-python-environments.html#hands-on-environments",
    "title": "Conda environments",
    "section": "",
    "text": "The following exercises will guide you through the basic commands to work with Conda environments. Unless otherwise specified, all the commands should be run in the command line. For a deeper dive after completing this introduction, check out the Conda documentation.\n\n\nTo list all the Conda environments available in your computer and their location we use:\nconda env list\nThe output should look something like this:\n# conda environments:\n#\nbase                  *  /Users/galaz-garcia/opt/anaconda3\neds220-env               /Users/galaz-garcia/opt/anaconda3/envs/eds220-env\nNotice the file path next to the environment name. This is the absolute path to the environment‚Äôs installation directory on my local machine. This is where Conda has created the environment and stored the packages!\nWhat is my currently active environment?\n\nWhen you run conda env list, the asterisk next to the environment path indicates which environment is active. In the previous example, I am using the base Python environment.\nThe currently active environment also appears in the terminal in parenthesis at the beginning of each line, something like this:\n\n(base) my-computer:MEDS-eds-220-course galaz-garcia$\n\n\n\nTo create a new environment called test-env and, within it, install the most recent version of Python we simply run:\nconda create --name test-env python\nWhen you run this command, it will print out information about the packages that will be installed and ask you whether to proceed or not. Type y and press enter to create the environment and install the packages.\n\n\n\n\n\n\nCheck-in\n\n\n\nCheck whether the new test-env environment is listed by Conda. Is it activated or not?\n\n\n\n\n\nTo activate the test-env environment we use the command\nconda activate test-env\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify that test-env is now your current environment.\n\n\n\n\n\nTo see which packages are installed within the currently active environment we run:\nconda list\nThe output will be a long list that looks something like this:\n# packages in environment at /Users/galaz-garcia/opt/anaconda3/envs/test-env:\n#\n# Name                    Version                   Build  Channel\nbzip2                     1.0.8                h6c40b1e_6  \npip                       24.2            py312hecd8cb5_0  \nsqlite                    3.45.3               h6c40b1e_0  \n[...]\nThe name and version of each installed package in the environment appear in their respective columns. The ‚ÄòBuild‚Äô column shows a build string for each package. This string encodes important information about how the package was compiled, including any updates or changes made without altering the package version. It also reflects any compatibility or performance optimizations that were integrated during the build process.\nFor example, the same package with the same version might have different builds (and therefore different build strings) depending on whether it was installed on macOS, Windows, or Linux. This difference arises because the package may need to be compiled differently to optimize performance and ensure compatibility with each operating system.\nThe channel column shows the source from which the package was downloaded and installed. In this example the entries on this column are blank, signaling these packages were downloaded from the defeault Conda channel. We‚Äôll see other channels appear in the next examples.\nWe can also request information about a specific package. For example, if we run\nconda list pip\nwe obtain only the information about the pip package:\n# packages in environment at /Users/galaz-garcia/opt/anaconda3/envs/test-env:\n#\n# Name                    Version                   Build  Channel\npip                       24.2            py312hecd8cb5_0  \nWe can use the same command to verify whether a package is installed. For example, if we run\nconda list numpy\nwe will get empty columns:\n# packages in environment at /Users/galaz-garcia/opt/anaconda3/envs/test-env:\n#\n# Name                    Version                   Build  Channel\nbecause the numpy package is not intalled in this environment.\n\n\n\nSuppose we want to install numpy in our currently active environment test-env. To do this we can simply use the command:\nconda install numpy\nSince we have not specified another channel, this command will install numpy from the default channel.\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify that numpy is now installed.\n\n\nNow, suppose we want to install rioxarray, this is a Python package for raster data analysis. This is a relatively new package, so it might not be available from the default Conda channels. A sensible measure would be to first look if rioxarray is available in the defaults channels, which we can do by running:\nconda search rioxarray\nThe output will look similar to\nLoading channels: done\nNo match found for: rioxarray. Search: *rioxarray*\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n  - rioxarray\n\nCurrent channels:\n\n  - https://repo.anaconda.com/pkgs/main/osx-64\n  - https://repo.anaconda.com/pkgs/main/noarch\n  - https://repo.anaconda.com/pkgs/r/osx-64\n  - https://repo.anaconda.com/pkgs/r/noarch\n\nTo search for alternate channels that may provide the Conda package you're\nlooking for, navigate to\n\n    https://anaconda.org\n\nand use the search bar at the top of the page.\nAs stated, rioxarray is not available on the default channels for my Conda installation. Notice a couple of the channels it is seraching on are specific to macOS (my current operating system). The last two channels are actually channels for R packages. If we had tried to install rioxarray using conda install rioxarray we would have obtained an error message with similar information (you can try it if you want!).\nWe may have better luck searching for rioxarray in the conda-forge channel, which we can do like this:\nconda search --channel conda-forge rioxarray\nThe result will be a list of available versions and builds of this package in the conda-forge channel:\nLoading channels: done\n# Name                       Version           Build  Channel             \nrioxarray                      0.0.3            py_0  conda-forge         \nrioxarray                      0.0.4            py_0  conda-forge         \nrioxarray                      0.0.5            py_0  conda-forge     \n[...]\nrioxarray                     0.16.0    pyhd8ed1ab_0  conda-forge         \nrioxarray                     0.17.0    pyhd8ed1ab_0  conda-forge   \n\n\n\n\n\n\nCheck-in\n\n\n\nDo you see any versions of rioxarray with multiple builds?\n\n\nSince we now know that we can find rioxarray in the conda-forge channel, we can go ahead and install it from this channel using the command\nconda install --channel conda-forge rioxarray\nYou‚Äôll see a moving bar that says Conda is solving (the) environment - this is Conda doing its package management job! It means Conda is working on:\n\nfinding the dependencies for the packages you are trying to install,\nfinding versions of these dependencies that are compatible with each other,\nmaking sure these new packages are compatible with the packages already present in the environment,\ndowngrade, upgrade, or remove packages as needed in the environment.\n\nOverall, Conda tries to install what we need with the least disruption. Solving an environment can often take time but it is crucial so that the environment remains stable and functional.\n\n\n\n\n\n\nCheck-in\n\n\n\n\nCheck whether pandas is installed in the environment.\nWhich channels were used to install dependencies for rioxarray?\n\n\n\n\n\n\n\nTo deactivate our current environment and return to our base environment we simply run\nconda deactivate\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify we are no longer in the test-env. What environment are we on?\n\n\n\n\n\nSince this was a test environment, we can go ahead and delete all of it using the command:\nconda remove --name test-env  --all\nIt will ask you whether you want to proceed. Type y and press enter to go ahead and delete the environment.\nWe can run conda env list to verify the test-env does not exist anymore.\n\n\n\nWe can also create a new environment by specifying the package versions and channel priorities. For example, to install the numpy and rioxarray versions we previously had in a new environment called my-env we run:\nconda create --name my-env rioxarray=0.17.0 numpy=1.26.4 --channel conda-forge\nNotice we are explicitely requiring certain versions of our packages, and the --channel conda-forge option is asking Conda to prioritize getting the packages from conda-forge.\n\n\n\n\n\n\nCheck-in\n\n\n\nTake a look at the dependencies that were installed with these two packages.\n\n\n\n\n\nOne of the main goals of using an environment to manage the packages used in your project is being able to share the environment so that other people can rerun your code and reproduce your results.\n‚ÄúSharing the environment‚Äù means sharing instructions that Conda can use to recreate the environment you have. These instructions are, basically, a list of the packages installed in the environment, their versions, which channels were used to install them, and, optionally, build specifications.\nThere are several ways of creating these instructions to reconstruct an environment depending on the level of cross-platform compatibility we are looking for. However, each of the following methods to export the environment specifications will generate a new YAML file. This file type with .yml extension is human-readable markup language that is commonly used for configuration in software applications.\n\n\n\n\n\n\nNavigate to project directory\n\n\n\nEach of the commands for exporting the environment specifications will create a .yml file in our terminal‚Äôs current working directory. Remember you can use pwd (Mac & Linux) or echo %cd% (Windows CMD) on the command line to check which is your current working directory. If you need to change directories you can use cd navigation in the terminal.\nThe best practice is to have the environment.yml file in the directory that holds your project. This way, you can use version control on it and push it to your remote repository.\n\n\n\n\nThe first method to export the environment specifications is running\nconda env export &gt; environment.yml\nThe environment.yml is the name of the YAML file that will have the environment specifications.\nOpen the environment.yml file that was generated, it will look similar to this excerpt:\nname: my-env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - affine=2.4.0=pyhd8ed1ab_0\n  - attrs=24.2.0=pyh71513ae_0\n  [...]\n  - zlib=1.3.1=h87427d6_1\n  - zstd=1.5.6=h915ae27_0\nprefix: /Users/galaz-garcia/opt/anaconda3/envs/my-env\nHere we can see the name of the environment, the channels used to install packages (in order of priority), and the package names, each with a version and build string. This ensures that the environment can be recreated exactly as it is on another system. This first method to export the environment specifications is the most detailed and it is both platform and package specific.\n\n\n\n\n\n\nDelete prefix line before sharing environment\n\n\n\nNotice the last line in the environment.yml file, this prefix indicates the absolute path to the environment‚Äôs installation on your local machine.\nYou should delete the prefix line when sharing the environment file. Leaving it there is unnecessary and could cause confusion since it points to specific directroy in your local machine.\n\n\n\n\n\nOur second method to export the environment specifications is to run\nconda env export --from-history &gt; environment_hist.yml\nHere I named the file environment_hist.yml to not overwrite the previous environmnet.yml, that way you can compare both of them. Open the environment_hist.yml file, it will look similar to this (much shorter than the previous one):\nname: my-env\nchannels:\n  - defaults\ndependencies:\n  - numpy=1.26.4\n  - rioxarray=0.17.0\nprefix: /Users/galaz-garcia/opt/anaconda3/envs/my-env\nAdding the --from-history option tells Conda to only include packages that we explicitely installed (in this case numpy=1.26.4 and rioxarray=0.17.0), omitting any dependencies that were automatically installed and channel information. This will create a minimal environment file that lists only the packages you explicitly installed, making it simpler and more portable for sharing with others. While this method ensures reproducibility of the core setup, Conda will resolve dependencies on its own and these might not be the same as your original environment.\n\n\n\n\n\n\nAdding channels to environment.yml file\n\n\n\nIn section 5 we saw that the rioxarray package is available on the conda-forge channel, but not the defaults channel. The output of conda env export with the --from-history option only includes the defaults channel, so we wouldn‚Äôt be able to recreate the environment with that file as is. Remember the .yml file can be manually modified and we can edit it to include conda-forge as the first priority channel, followed by the defaults channel. Rememember to also delete the prefix line before sharing. The modified version of the --from-history output that will allow us to recreate the environment will be:\nname: my-env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - numpy=1.26.4\n  - rioxarray=0.17.0\n\n\nBefore moving to the last exercise, delete the environment.yml and environment_hist.yml files.\n\n\n\n\n\nFor this last exercise, click here and download the environment.yml file in this repository. Move to a directory for a mock project and navigate to that directory in the terminal.\n\nTo create an environment from specifications in a .yml file we run\nconda env create --name test-env --file environment.yml\nThen --name option states how we want to name our environment (test-env in this case). If no --name option is given, Conda will name of the environment using the name field in the environment.yml file.\n\n\n\n\n\n\nCheck-in\n\n\n\nVerify that the new environment was created, activate it, and take a look at the installed packages.",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/appendices/A-python-environments.html#conda-commands",
    "href": "book/appendices/A-python-environments.html#conda-commands",
    "title": "Conda environments",
    "section": "",
    "text": "The following table includes the commands covered in this lesson‚Äôs exercises. Check out Conda‚Äôs cheatsheet for more commands and tips.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nconda env list\nlist available Conda environments\n\n\nconda create --name ENVNAME python\ncreate a new Python environment named ENVNAME\n\n\nconda remove --name ENVNAME  --all\ndelete ENVNAME environment\n\n\nconda activate ENVNAME\nactivate environment\n\n\nconda deactivate\ndeactivate active environment\n\n\npython -V\nprint Python version installed in active environment\n\n\nconda list\nlist installed packages in active environment\n\n\nconda list PKGNAME\ncheck if PKGNAME package is installed in active environment\n\n\nconda install PKGNAME\ninstall PKGNAME package in environment using environment channel priorities (generally just from defaults channel)\n\n\nconda install --channel CHANNELNAME PKGNAME\ninstall PKGNAME package in environment from from CHANNELNAME channel\n\n\nconda search PKGNAME\nsearch for PKGNAME package in environment‚Äôs channels\n\n\nconda search --channel CHANNELNAME PKGNAME\nsearch for PKGNAME package in CHANNELNAME channel\n\n\nconda env export &gt; ENV.yml\nexport environment specifications of currently active environmemt into ENV.yml file (includes all dependencies, buildstrings, and channels)\n\n\nconda env export --from-history &gt; ENV.yml\nexport environment specifications of currently active environmemt into ENV.yml file (only packages explicitely installed, no dependencies, buildstrings, or channels)\n\n\nconda env create --name ENVNAME --file ENV.yml\ncreate an environment named ENVNAME from specifications in ENV.yml file",
    "crumbs": [
      "notes",
      "Appendices",
      "Conda environments"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html",
    "href": "book/chapters/lesson-1-python-review.html",
    "title": "Python review",
    "section": "",
    "text": "This is a short review about some core concepts in Python exemplified by objects in the NumPy library. The goal is to recall basic Python vocabulary that will be used throughout the course, rather than to serve as an introduction to Python programming.\n\n\nBy the end of this lesson, students will be able to:\n\nDefine and provide examples for basic terms in Python programming like variable, object, function, class, attribute, and method.\nRecognize optional and non-optional arguments in a function.\nUnderstand some of the basic differences in R and Python syntax.\n\n\n\n\nA library is a collection of code that we can use to perform specific tasks in our programs. It can be a single file or multiple ones. NumPy [1] is one of the core libraries for numerical computing in Python. Many of the libraries we will use in this course use NumPy‚Äôs arrays as their building blocks. Additionally, NumPy objects have been optimized for processing, so computations on them are really fast and use less memory than doing the equivalent using the core Python data structures.\nIn this lesson we will use NumPy to review some fundamental concepts in Python you should be already familiar with.\n\n\n\n\n\n\nLibrary or package?\n\n\n\nA package in Python refers to a specific way of organizing multiple files of code into a directory hierarcy, often within a large code library. The words ‚Äúlibrary‚Äù and ‚Äúpackage‚Äù are often used interchangeably. NumPy, for example, is both a library and a package.\n\n\nLet‚Äôs start by importing the NumPy library by using the standard to abbreviation, np:\n\nimport numpy as np\n\nBy importing numpy, all the objects and functions in this library will be available for us to use in our notebook.\n\n\n\nWe can think of a variable as a name we assign to a particular object in Python. For example:\n\n# Assign a small array to variable a\na = np.array([[1,1,2],[3,5,8]])\n\nWhen we run the cell, we store the variables and their value. We can view a variable‚Äôs value in two ways from within our Jupyter notebook:\n\nrunning a cell with the variable name\nusing the print function to print the value\n\n\n# Show the value\na\n\narray([[1, 1, 2],\n       [3, 5, 8]])\n\n\n\n# Print the value \nprint(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\n\n\n\n\n\n\nR and Python: assigning values\n\n\n\nRemember that in Python we use the equal sign = to assign values to variables in the same way the left-arrow &lt;- is used in R:\n# R: assign value 10 to variable a\na &lt;- 10\n# Python: assign value 10 to variable a\na = 10\n\n\n\n\n\n\n\n\nConvention: Use snake_case for naming variables\n\n\n\nThere are many ways of constructing multi-word variable names. In this course we will name variables using snake_case, where words are all in small caps and separated by underscores (ex: raw_data, fires_2023). This is the naming convention suggested by the PEP 8 - Style Guide for Python Code [2]. Remember variable names should be both descriptive and concise!\n\n\n\n\n\nYou will often encounter the word object in Python documentation and tutorials. Informally speaking, an object is a bundle of properties and actions about something specific. For example, an object could represent a data frame with properties such as number of rows, names of columns, and date created, and actions suchs as selecting a specific row or adding a new column.\nA variable is the name we give a specific object, and the same object can be referenced by different variables. An analogy for this is the following: the Sun (object) is called ‚Äúsol‚Äù in Spanish and ‚Äúsoleil‚Äù in French, so two different names (variables) represent the same object. You can read more technical details about the difference between objects and variables in Python here [3].\nIn practice, we can often use the word variable and object interchangeably (for example, in the next subsection!). I want to bring up what objects are so you are not caught off-guard with vocabulary you will often encounter in the documentation, StackExchange, etc.\n\n\n\nEvery object in Python has a type, the type tells us what kind of object it is. We can also call the type of an object, the class of an object, so class and type both mean what kind of object we have.\nWe can see the type/class of a variable/object by using the type function:\n\nprint(a)\ntype(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\nnumpy.ndarray\n\n\nThe numpy.ndarray is the core object/data type in the NumPy package. We can check the type of an entry in the array by indexing:\n\nprint(a[0,0])\ntype(a[0,0])\n\n1\n\n\nnumpy.int64\n\n\nNotice the type of the value 1 in the array is numpy.int64 and not just the standard Python integer type int. The NumPy type numpy.int64 is telling us 1 is an integer stored as a 64-bit number. NumPy has its own data types to deal with numbers depending on memory storage and floating point precision, click here to know see all the types.\n\n\n\n\n\n\nR and Python: indexing\n\n\n\nRemember that in Python the indexing starts from 0, while in R it starts from 1. If you learned R first, this might seem odd but it‚Äôs easy to get used to it with some practice. A way to understand this 0-indexing is that, in Python, the index indicates the displacement from the start of the collection. So ‚Äò0 index in an array‚Äô means ‚Äòzero displacement from the start of the array‚Äô, in other words, the first element of the array.\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nHow would you access the value 5 in the array a?\n\n\nSince ‚Äúeverything in Python is an object‚Äù and every object belongs to a class, we will interact with SO MANY classes in this course. Often, knowing the type of an object is the first step to finding information to code what you want!\n\n\n\nprint was our first example of a Python function. Functions take in a set of arguments, separated by commas, and use those arguments to create an output. There are several built-in funcions in Python, most of them are for interacting with the Python basic data types such as int (integers), float (decimal numbers), str (strings), and bool (boolean values).\n\n\n\n\n\n\nArgument or parameter?\n\n\n\nIn this course we will use argument and parameter interchangeably. They do, however, have related but different meanings.\nA parameter is a variable in the function definition that accepts an argument passed to the function. It is a placeholder in the function that will receive the value of an argument when the function is called.\nFor example, in the function\ndef my_function(parameter1, parameter2):\n    return parameter1 + parameter2\nparameter1 and parameter2 are parameters. When we call the function\nresult = my_function(5, 10)\nwe have that 5 and 10 are arguments passed to the function my_function.\nYou will probably see parameter more often in a package‚Äôs documentation!\n\n\nWe can ask for information about what a function does function by executing ? followed by the function name:\n\n?print\n\n\nWhat we obtain is a docstring, a special type of comment that is used to document how a function (or class, or module) works. The first line in the docstring is telling us the function name followed by all of its arguments in parentheses. Then there is a short description of what the function does. And finally a list of the arguments and a brief explanation about each of them.\nYou can see there are different types of arguments inside the parenthesis. Roughly speaking, a function has two types of arguments:\n\nnon-optional arguments: arguments you need to specify for the function to do something, and\noptional arguments: arguments that are pre-filled with a default value by the function, but you can override them. Optional arguments appear inside the parenthesis () in the form optional_argument = default_value.\n\n\n\nend is a parameter in print with the default value a new line. We can pass the value ^_^ to this parameter so that finishes the line with ^_^ instead:\n\nprint('changing the default end argument of the print function', end=' ^_^')\n\nchanging the default end argument of the print function ^_^\n\n\nNotice that before we had always used print without specifying any value for the end parameter.\n\n\n\n\nAn object in Python has attributes and methods. An attribute is a property of the object, some piece of information about it. A method is a procedure associated with an object, so it is an action where the main ingredient is the object.\nFor example, these could be some attributes and methods for class cat:\n\n\n\n.\n\n\nMore formally, a method is a function that acts on the object it is part of.\nWe can access a variable‚Äôs attributes and methods by adding a period . at the end of the variable‚Äôs name. So we would write variable.variable_method() or variable.variable_attribute.\n\n\n\n\n\n\nCheck-in\n\n\n\nSuppose we have a class fish, make a diagram similar to the cat class diagram showing 3 attributes for the class and 3 methods.\n\n\n\n\nNumPy arrays have many methods and attributes. Let‚Äôs see some concrete examples.\n\n# A 3x3 array\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nT is an example of attribute, it returns the transpose of var:\n\nprint(var.T)\nprint(type(var.T))\n\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n&lt;class 'numpy.ndarray'&gt;\n\n\nshape, another attribute, tells us the shape of the array:\n\nprint(var.shape)\nprint(type(var.shape))\n\n(3, 3)\n&lt;class 'tuple'&gt;\n\n\nndim is an attribute holding the number of array dimensions\n\nprint(var.ndim)\nprint(type(var.ndim))\n\n2\n&lt;class 'int'&gt;\n\n\nNotice these attributes can have many different data types. Here we saw a tuple and an int (two of the basic Python classes) and also a NumPy array as attributes of var.\nNow some examples of methods.\nThe tolist method returns the array as a nested list of scalars:\n\nvar.tolist()\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\nThe min method returns the minimum value in the array along a specified axis:\n\nvar.min(axis=0)\n\narray([1, 2, 3])\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWe can also call the min method without any parameters:\n\nvar.min()\n\n1\n\n\nWhat kind of parameter is axis in our previous call of the var method?\n\n\nRemember, methods are functions associated to an object. We can confirm this!\n\ntype(var.tolist)\n\nbuiltin_function_or_method\n\n\n\ntype(var.min)\n\nbuiltin_function_or_method\n\n\nYou can see a complete list of NumPy array‚Äôs methods and attributes in the documentation.\n\n\n\n\n\n\nR and Python: are there methods in R?\n\n\n\nIt is uncommon to use methods within an object in R. Rather, functions are extrinsic to the objects they are acting on. In R, for example, there would usually be two separate items: the variable var and a separate function min that gets var as a parameter:\n# This is R code\nvar &lt;- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nmin(var)\nUsing the pipe operator %&gt;% in R‚Äôs tidyverse is closer to the dot . in Python:\n# This is R code\nvar &lt;- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nvar %&gt;% min()\nWhat happens here is that the pipe %&gt;% is passing var to the min() function as its first argument. This is similar to what happens in Python when a function is a method of a class:\n# This is Python code\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar.min()\nWhen working in Python, remember that methods are functions that are part of an object and a method uses the object it is part of to produce some information.",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html#learning-objectives",
    "href": "book/chapters/lesson-1-python-review.html#learning-objectives",
    "title": "Python review",
    "section": "",
    "text": "By the end of this lesson, students will be able to:\n\nDefine and provide examples for basic terms in Python programming like variable, object, function, class, attribute, and method.\nRecognize optional and non-optional arguments in a function.\nUnderstand some of the basic differences in R and Python syntax.",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html#libraries-and-packages",
    "href": "book/chapters/lesson-1-python-review.html#libraries-and-packages",
    "title": "Python review",
    "section": "",
    "text": "A library is a collection of code that we can use to perform specific tasks in our programs. It can be a single file or multiple ones. NumPy [1] is one of the core libraries for numerical computing in Python. Many of the libraries we will use in this course use NumPy‚Äôs arrays as their building blocks. Additionally, NumPy objects have been optimized for processing, so computations on them are really fast and use less memory than doing the equivalent using the core Python data structures.\nIn this lesson we will use NumPy to review some fundamental concepts in Python you should be already familiar with.\n\n\n\n\n\n\nLibrary or package?\n\n\n\nA package in Python refers to a specific way of organizing multiple files of code into a directory hierarcy, often within a large code library. The words ‚Äúlibrary‚Äù and ‚Äúpackage‚Äù are often used interchangeably. NumPy, for example, is both a library and a package.\n\n\nLet‚Äôs start by importing the NumPy library by using the standard to abbreviation, np:\n\nimport numpy as np\n\nBy importing numpy, all the objects and functions in this library will be available for us to use in our notebook.",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html#variables",
    "href": "book/chapters/lesson-1-python-review.html#variables",
    "title": "Python review",
    "section": "",
    "text": "We can think of a variable as a name we assign to a particular object in Python. For example:\n\n# Assign a small array to variable a\na = np.array([[1,1,2],[3,5,8]])\n\nWhen we run the cell, we store the variables and their value. We can view a variable‚Äôs value in two ways from within our Jupyter notebook:\n\nrunning a cell with the variable name\nusing the print function to print the value\n\n\n# Show the value\na\n\narray([[1, 1, 2],\n       [3, 5, 8]])\n\n\n\n# Print the value \nprint(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\n\n\n\n\n\n\nR and Python: assigning values\n\n\n\nRemember that in Python we use the equal sign = to assign values to variables in the same way the left-arrow &lt;- is used in R:\n# R: assign value 10 to variable a\na &lt;- 10\n# Python: assign value 10 to variable a\na = 10\n\n\n\n\n\n\n\n\nConvention: Use snake_case for naming variables\n\n\n\nThere are many ways of constructing multi-word variable names. In this course we will name variables using snake_case, where words are all in small caps and separated by underscores (ex: raw_data, fires_2023). This is the naming convention suggested by the PEP 8 - Style Guide for Python Code [2]. Remember variable names should be both descriptive and concise!",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html#objects",
    "href": "book/chapters/lesson-1-python-review.html#objects",
    "title": "Python review",
    "section": "",
    "text": "You will often encounter the word object in Python documentation and tutorials. Informally speaking, an object is a bundle of properties and actions about something specific. For example, an object could represent a data frame with properties such as number of rows, names of columns, and date created, and actions suchs as selecting a specific row or adding a new column.\nA variable is the name we give a specific object, and the same object can be referenced by different variables. An analogy for this is the following: the Sun (object) is called ‚Äúsol‚Äù in Spanish and ‚Äúsoleil‚Äù in French, so two different names (variables) represent the same object. You can read more technical details about the difference between objects and variables in Python here [3].\nIn practice, we can often use the word variable and object interchangeably (for example, in the next subsection!). I want to bring up what objects are so you are not caught off-guard with vocabulary you will often encounter in the documentation, StackExchange, etc.",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html#types",
    "href": "book/chapters/lesson-1-python-review.html#types",
    "title": "Python review",
    "section": "",
    "text": "Every object in Python has a type, the type tells us what kind of object it is. We can also call the type of an object, the class of an object, so class and type both mean what kind of object we have.\nWe can see the type/class of a variable/object by using the type function:\n\nprint(a)\ntype(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\nnumpy.ndarray\n\n\nThe numpy.ndarray is the core object/data type in the NumPy package. We can check the type of an entry in the array by indexing:\n\nprint(a[0,0])\ntype(a[0,0])\n\n1\n\n\nnumpy.int64\n\n\nNotice the type of the value 1 in the array is numpy.int64 and not just the standard Python integer type int. The NumPy type numpy.int64 is telling us 1 is an integer stored as a 64-bit number. NumPy has its own data types to deal with numbers depending on memory storage and floating point precision, click here to know see all the types.\n\n\n\n\n\n\nR and Python: indexing\n\n\n\nRemember that in Python the indexing starts from 0, while in R it starts from 1. If you learned R first, this might seem odd but it‚Äôs easy to get used to it with some practice. A way to understand this 0-indexing is that, in Python, the index indicates the displacement from the start of the collection. So ‚Äò0 index in an array‚Äô means ‚Äòzero displacement from the start of the array‚Äô, in other words, the first element of the array.\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nHow would you access the value 5 in the array a?\n\n\nSince ‚Äúeverything in Python is an object‚Äù and every object belongs to a class, we will interact with SO MANY classes in this course. Often, knowing the type of an object is the first step to finding information to code what you want!",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html#functions",
    "href": "book/chapters/lesson-1-python-review.html#functions",
    "title": "Python review",
    "section": "",
    "text": "print was our first example of a Python function. Functions take in a set of arguments, separated by commas, and use those arguments to create an output. There are several built-in funcions in Python, most of them are for interacting with the Python basic data types such as int (integers), float (decimal numbers), str (strings), and bool (boolean values).\n\n\n\n\n\n\nArgument or parameter?\n\n\n\nIn this course we will use argument and parameter interchangeably. They do, however, have related but different meanings.\nA parameter is a variable in the function definition that accepts an argument passed to the function. It is a placeholder in the function that will receive the value of an argument when the function is called.\nFor example, in the function\ndef my_function(parameter1, parameter2):\n    return parameter1 + parameter2\nparameter1 and parameter2 are parameters. When we call the function\nresult = my_function(5, 10)\nwe have that 5 and 10 are arguments passed to the function my_function.\nYou will probably see parameter more often in a package‚Äôs documentation!\n\n\nWe can ask for information about what a function does function by executing ? followed by the function name:\n\n?print\n\n\nWhat we obtain is a docstring, a special type of comment that is used to document how a function (or class, or module) works. The first line in the docstring is telling us the function name followed by all of its arguments in parentheses. Then there is a short description of what the function does. And finally a list of the arguments and a brief explanation about each of them.\nYou can see there are different types of arguments inside the parenthesis. Roughly speaking, a function has two types of arguments:\n\nnon-optional arguments: arguments you need to specify for the function to do something, and\noptional arguments: arguments that are pre-filled with a default value by the function, but you can override them. Optional arguments appear inside the parenthesis () in the form optional_argument = default_value.\n\n\n\nend is a parameter in print with the default value a new line. We can pass the value ^_^ to this parameter so that finishes the line with ^_^ instead:\n\nprint('changing the default end argument of the print function', end=' ^_^')\n\nchanging the default end argument of the print function ^_^\n\n\nNotice that before we had always used print without specifying any value for the end parameter.",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-1-python-review.html#attributes-methods",
    "href": "book/chapters/lesson-1-python-review.html#attributes-methods",
    "title": "Python review",
    "section": "",
    "text": "An object in Python has attributes and methods. An attribute is a property of the object, some piece of information about it. A method is a procedure associated with an object, so it is an action where the main ingredient is the object.\nFor example, these could be some attributes and methods for class cat:\n\n\n\n.\n\n\nMore formally, a method is a function that acts on the object it is part of.\nWe can access a variable‚Äôs attributes and methods by adding a period . at the end of the variable‚Äôs name. So we would write variable.variable_method() or variable.variable_attribute.\n\n\n\n\n\n\nCheck-in\n\n\n\nSuppose we have a class fish, make a diagram similar to the cat class diagram showing 3 attributes for the class and 3 methods.\n\n\n\n\nNumPy arrays have many methods and attributes. Let‚Äôs see some concrete examples.\n\n# A 3x3 array\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nT is an example of attribute, it returns the transpose of var:\n\nprint(var.T)\nprint(type(var.T))\n\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n&lt;class 'numpy.ndarray'&gt;\n\n\nshape, another attribute, tells us the shape of the array:\n\nprint(var.shape)\nprint(type(var.shape))\n\n(3, 3)\n&lt;class 'tuple'&gt;\n\n\nndim is an attribute holding the number of array dimensions\n\nprint(var.ndim)\nprint(type(var.ndim))\n\n2\n&lt;class 'int'&gt;\n\n\nNotice these attributes can have many different data types. Here we saw a tuple and an int (two of the basic Python classes) and also a NumPy array as attributes of var.\nNow some examples of methods.\nThe tolist method returns the array as a nested list of scalars:\n\nvar.tolist()\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\nThe min method returns the minimum value in the array along a specified axis:\n\nvar.min(axis=0)\n\narray([1, 2, 3])\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWe can also call the min method without any parameters:\n\nvar.min()\n\n1\n\n\nWhat kind of parameter is axis in our previous call of the var method?\n\n\nRemember, methods are functions associated to an object. We can confirm this!\n\ntype(var.tolist)\n\nbuiltin_function_or_method\n\n\n\ntype(var.min)\n\nbuiltin_function_or_method\n\n\nYou can see a complete list of NumPy array‚Äôs methods and attributes in the documentation.\n\n\n\n\n\n\nR and Python: are there methods in R?\n\n\n\nIt is uncommon to use methods within an object in R. Rather, functions are extrinsic to the objects they are acting on. In R, for example, there would usually be two separate items: the variable var and a separate function min that gets var as a parameter:\n# This is R code\nvar &lt;- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nmin(var)\nUsing the pipe operator %&gt;% in R‚Äôs tidyverse is closer to the dot . in Python:\n# This is R code\nvar &lt;- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nvar %&gt;% min()\nWhat happens here is that the pipe %&gt;% is passing var to the min() function as its first argument. This is similar to what happens in Python when a function is a method of a class:\n# This is Python code\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar.min()\nWhen working in Python, remember that methods are functions that are part of an object and a method uses the object it is part of to produce some information.",
    "crumbs": [
      "notes",
      "Python review"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html",
    "title": "2 Subsetting",
    "section": "",
    "text": "In this lesson we will learn different methods to select data from a pandas.DataFrame. Like it‚Äôs often the case when working with the pandas package, there are many ways in which we can subset a data frame. Here we will review the core methods to do this.\nA summary of the methods covered in this lesson can be found in Figure¬†1.\n\n\nBy the end of this lesson, students will be able to:\n\nChoose appropriate methods for selecting rows and columns from a pandas.DataFrame\nConstruct conditions to subset rows\nDescribe the difference between label-based subsetting and position-based subsetting\nApply best practies when using iloc and loc selection\n\n\n\n\nIn this lesson we will use annual estimates of bird species abundance in four coastal wetlands along the California coast. This dataset was derived for education purposes for this course from the UCSB SONGS Mitigation Monitoring: Wetland Performance Standard - Bird Abundance and Species Richness dataset [1]. The SONGS dataset was collected as part of the San Onofre Nuclear Generating Station (SONGS) San Dieguito Wetland Restoration monitoring program.\n\n\n\nSan Onofre Nuclear Generating Station in San Diego County, California. Source: Southern California Edison\n\n\nThe annual bird species abundance estimates is a CSV file with 13 columns and 14 rows. You can see the first three rows below.\n\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n\n\n\n\n\nThe four wetlands where the bird surveys occured are Carpinteria Salt Marsh (CSM), Mugu Lagoon (MUL), the San Dieguito Wetland (SDW), and the Tijuana Estuary (TJE). The values from the second column to the last column correspond to the number of different bird species recorded across the survey sites in each wetland during winter, spring, and fall of a given year. For example, the CSM_fall column has the number of species recorded in fall at Carpinteria Salt Marsh across years. The year column corresponds to the calendar year on which the data was collected. Surveys have happened yearly from 2010 to 2023.\n\n\n\nMugu Lagoon in Ventura County, California, seen from the Mugu Peak Trail. Source: USA National Park Service\n\n\n\n\n\nA CSV (Comma-Separated Values) file is an open, simple text format for storing tabular data, with rows separated by line breaks and columns by commas. It‚Äôs widely used in environmental science for sharing datasets like species counts and environmental monitoring data because it‚Äôs easy to create, read, and process in different platforms, without the need of proprietary software.\nTo read in a CSV file into our Python workspace as pandas.DataFrame we use the pandas.read_csv function:\n\nimport pandas as pd\n\n# Read in file, argument is the file path\ndf = pd.read_csv('data/wetlands_seasonal_bird_diversity.csv')\n\nNext, we obtain some high-level information about this data frame:\n\n# Print data frame's first five rows \ndf.head()\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\n\n# Print data frame's last five rows \ndf.tail()\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n9\n2019\n39.0\n39.0\n40.0\n57.0\n52.0\n53.0\n54.0\n55.0\n53.0\n63.0\n54.0\n50.0\n\n\n10\n2020\n46.0\nNaN\n47.0\n56.0\nNaN\n66.0\n57.0\nNaN\n58.0\n54.0\n40.0\n54.0\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n12\n2022\n40.0\n46.0\n49.0\n60.0\n55.0\n65.0\n57.0\n60.0\n57.0\n60.0\n61.0\n60.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\n\n# Print data frame's column names\ndf.columns\n\nIndex(['year', 'CSM_winter', 'CSM_spring', 'CSM_fall', 'MUL_winter',\n       'MUL_spring', 'MUL_fall', 'SDW_winter', 'SDW_spring', 'SDW_fall',\n       'TJE_winter', 'TJE_spring', 'TJE_fall'],\n      dtype='object')\n\n\n\n# List the data types of each column\ndf.dtypes\n\nyear            int64\nCSM_winter    float64\nCSM_spring    float64\nCSM_fall      float64\nMUL_winter    float64\nMUL_spring    float64\nMUL_fall      float64\nSDW_winter    float64\nSDW_spring    float64\nSDW_fall      float64\nTJE_winter    float64\nTJE_spring    float64\nTJE_fall      float64\ndtype: object\n\n\n\n# Print data frame's shape: output is a tuple (# rows, # columns)\ndf.shape\n\n(14, 13)\n\n\n\n\n\nSelecting a single column by column name is the simplest case for selecting data in a data frame. The genereal syntax to do this is:\ndf['column_name']\nNotice the column name is given as string inside the square brackets. This is an example of label-based subsetting, which means we want to select data from our data frame using the names of the columns, not their position. When we select rows or column using their position, we are doing position-based subsetting. We‚Äôll see some methods to do this when we move into selecting rows.\n\n\nSuppose we are interested in the number of bird species observed at the Mugu Lagoon in spring. We can access that single column in this way:\n\n# Select a single column by using square brackets []\nmul_spring = df['MUL_spring']\n\n# Print first five elements in this column\nmul_spring.head()\n\n0     NaN\n1    52.0\n2    58.0\n3    58.0\n4    52.0\nName: MUL_spring, dtype: float64\n\n\nSince we only selected a single column, mul_spring is a pandas.Series:\n\n# Check the type of the ouput\nprint(type(mul_spring))\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\npd.DataFrame = dictionary of columns\n\n\n\nRemember we can think of a pandas.DataFrame as a dictionary of its columns? Then we can access a single column using the column name as the key, just like we would do in a dictionary. That is the we just used: df['column_name'].\n\n\nWe can also do label-based subsetting of a single column using attribute syntax:\ndf.column_name\nFor example, to see the head of the MUL_spring column we would do:\n\ndf.MUL_spring.head()\n\n0     NaN\n1    52.0\n2    58.0\n3    58.0\n4    52.0\nName: MUL_spring, dtype: float64\n\n\n\n\n\n\n\n\nFavor df['column_name'] instead of df.column_name\n\n\n\nIn general, it is better to use the df['column_name'] syntax. A couple reasons why are:\n\ndf['column_name'] can take in any column name, while df.column_name only works if the column name has no spaces or special characters\ndf['column_name'] avoids conflicts with pd.DataFrame methods and attributes. For example, if df has a column named count, it‚Äôs ambiguous whehter pd.count is referring to the count() method or the count column.\n\n\n\n\n\n\n\n\n\nWe can select multiple columns in a single call by passing a list with the column names to the square brackets []:\ndf[['column_1', 'column_10', 'column_245']]\nNotice there are double square brackets. This is because we are passing the list of names ['column_1', 'column_10', 'column_245'] to the selection brakcets [].\n\n\n\n\n\n\nCheck-in\n\n\n\nIs this an example of label-based selection or location-based selection?\n\n\n\n\nIf we want to look at the species in the Tijuana Estuary during winter and fall, then we can select these columns like this:\n\n# Select columns with names \"TJE_winter\" and \"TJE_fall\"\ntje_wf = df[['TJE_winter','TJE_fall']]\n\nNotice there are double square brackets. This is because we are passing the list of names ['TJE_winter','TJE_fall'] to the selection brakcets [].\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat is the type and shape of the tje_wf output? Verify your answer.\n\n\n\n\n\n\nTo select a slice of the columns we will use a special case of loc selection (we‚Äôll cover the general one by the end of the lesson). The syntax will be\ndf.loc[ : , 'column_start':'column_end']\nwhere column_start and column_end are, respectively, the starting point and endpoint of the column slice we want to subset from the data frame.\nNotice two things:\n\nthe first value passed to loc is used for selecting rows, using a colon : as the row-selection parameter means ‚Äúselect all the rows‚Äù\nthe slice of the data frame we‚Äôll obtain includes both endpoints of the slice 'column_start':'column_end'. In other words, we‚Äôll get the column_start column and the column_end column. This is different from how slicing works in base Python and NumPy, where the endpoint is not included.\n\n\n\nLet‚Äôs select the slice of columns that covers all data from Carpinteria Salt Marsh and Mugu Lagoon. This corresponds to all columns between CSM_winter and MUL_fall.\n\n# Select columns between 'CSM_winter' and 'MUL_fall'\ncsm_mul = df.loc[:,'CSM_winter':'MUL_fall']\ncsm_mul.head()\n\n\n\n\n\n\n\n\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\n\n\n\n\n0\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\n\n\n1\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n\n\n2\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n\n\n3\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n\n\n4\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we are familiar with some methods for selecting columns, let‚Äôs move on to selecting rows.\n\n\nSelecting rows that satisfy a particular condition is one of the most usual kinds of row subsetting. The general syntax for this type of selection is\ndf[condition_on_rows]\nThat condition_of_rows can be a myriad things, let‚Äôs see some usual scenarios.\n\n\nSuppose we are intersted in all data after 2020. We can select these rows in this way:\n\n# Select all rows with year &gt; 2020\npost_2020 = df[df['year']&gt;2020]\npost_2020\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n12\n2022\n40.0\n46.0\n49.0\n60.0\n55.0\n65.0\n57.0\n60.0\n57.0\n60.0\n61.0\n60.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\nLet‚Äôs break down what is happening here. The condition for our rows is df['year']&gt;2020, this is a pandas.Series with boolean values (True or False) indicating which rows satisfy the condition year&gt;2020:\n\n# Check the type of df['year']&gt;1996\nprint(type(df['year']&gt;2020))\n\n# Print the boolean series\ndf['year']&gt;2020\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11     True\n12     True\n13     True\nName: year, dtype: bool\n\n\nWhen we pass such a series of boolean values to the selection brackets [] we keep only the rows that correspond to a True value.\n\n\n\n\n\n\n\nCheck-in\n\n\n\nGet the subset of our data frame on which the San Dieguito Wetland has at least 75 species recorded during spring.\n\n\n\n\n\nSuppose we want to look at data from years 2012 to 2015 (including both years). One way of doing this is to use the between operator in our condition:\n\nsubset = df[df['year'].between(2012, 2015)]\nsubset\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n5\n2015\n44.0\n42.0\n45.0\n58.0\n50.0\n51.0\n71.0\n61.0\n65.0\n73.0\n76.0\n64.0\n\n\n\n\n\n\n\nLet‚Äôs break down this code:\n\ndf['year'] is the column with the year values, a pandas.Series\nin df['year'].between(), we have that between is a method for the pandas.Series and we are calling it using the dot .\n(2012, 2015) are the parameters for the between() method, from the pandas documentation we can see this method will subset including both endpoints\ndf['year'].between(2012, 2015) is then a pandas.Series of boolean values indicating which rows have year equal to 2012, 2013, 2014, or 2015.\nwhen we put df['year'].between(2012, 2015) inside the selection brackets [] we obtain the rows of the data frame with year equal to 2012, ‚Ä¶, 2015.\n\n\n\n\n\n\n\nAvoid using loc for selecting only rows\n\n\n\nIt is equivalent to write\n# Select rows with year&lt;2015\ndf[df['year']&lt;2015]\nand\n# Select rows with year&lt;2015 using loc\ndf.loc[ df['year']&lt;2015 , :]\nIn the second one:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']&lt;2015\nthe column-selection parameter is a colon :, which indicates we want all columns for the rows we are selecting.\n\nWe prefer the first syntax when we are selecting rows and not columns since it is simpler.\n\n\n\n\n\n\nWe can combine multipe conditions to select rows by surrounding each one in parenthesis () and using the or operator | and the and operator &.\n\n\nLet‚Äôs select rows in which the Carpinteria Salt Marsh has more than 50 registered in winter or fall:\n\ndf[ (df['CSM_winter']&gt;50) | (df['CSM_fall']&gt;50)]\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs select rows with in which both the Carpinteria Salt Marsh and the San Dieguito Wetland have more than 60 reported bird species during spring:\n\ndf[ (df['CSM_spring']&gt;60) & (df['SDW_spring']&gt;60)]\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n\n\n\n\n\nAn empty data frame! That‚Äôs ok, it just means there are no rows that satisfy the given condition.\n\n\n\n\nAll the selections we have done so far have been using labels. Sometimes we may want to select certain rows depending on their actual position in the data frame in other words, using position-based subsetting. To do this, we use iloc selection with the syntax\n df.iloc[row-indices]\niloc stands for integer-location based indexing.\n\n\n\n# Select the fifth row (index=4)\ndf.iloc[4]\n\nyear          2014.0\nCSM_winter      38.0\nCSM_spring      43.0\nCSM_fall        45.0\nMUL_winter      49.0\nMUL_spring      52.0\nMUL_fall        57.0\nSDW_winter      61.0\nSDW_spring      78.0\nSDW_fall        71.0\nTJE_winter      60.0\nTJE_spring      81.0\nTJE_fall        62.0\nName: 4, dtype: float64\n\n\n\n# Select rows 9 through 13, inclduing 13\ndf.iloc[9:14]\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n9\n2019\n39.0\n39.0\n40.0\n57.0\n52.0\n53.0\n54.0\n55.0\n53.0\n63.0\n54.0\n50.0\n\n\n10\n2020\n46.0\nNaN\n47.0\n56.0\nNaN\n66.0\n57.0\nNaN\n58.0\n54.0\n40.0\n54.0\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n12\n2022\n40.0\n46.0\n49.0\n60.0\n55.0\n65.0\n57.0\n60.0\n57.0\n60.0\n61.0\n60.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\nNotice that, since we are back to indexing by position, the right endpoint of the slice is not included in the ouput.\n\n\n\n\n\nSelecting rows and columns simultaneously can be done using loc (labels) or iloc (positions).\n\n\nWhen we want to select rows and columns simultaneously by labels (including using conditions) we can use loc selection with the syntax\ndf.loc[ row-selection , column-selection]\nspecifying both paratmers: row-selection and column-selection. These parameters can be a condition or a subset of labels from the index or the column names.\n\n\nLet‚Äôs select the winter surveys for Mugu Lagoon and the Tijuana Estuary after 2020:\n\ndf.loc[df['year']&gt;2020, ['MUL_winter','TJE_winter']]\n\n\n\n\n\n\n\n\nMUL_winter\nTJE_winter\n\n\n\n\n11\n54.0\n53.0\n\n\n12\n60.0\n60.0\n\n\n13\n72.0\n60.0\n\n\n\n\n\n\n\nLet‚Äôs break down this code:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']&gt;2020, which is a boolean array saying which years are greater than 2020\nthe column-selection parameter is ['MUL_winter','TJE_winter'] which is a list with the names of the two columns we are interested in.\n\n\n\n\n\nWhen we want to select rows and columns simultaneously by position we use iloc selection with the syntax:\ndf.iloc[ row-indices , column-indices]\n\n\nSuppose we want to select rows 3-7 (including 7) and columns 3 and 4:\n\ndf.iloc[3:8, [3,4]]\n\n\n\n\n\n\n\n\nCSM_fall\nMUL_winter\n\n\n\n\n3\n38.0\n60.0\n\n\n4\n45.0\n49.0\n\n\n5\n45.0\n58.0\n\n\n6\n47.0\n63.0\n\n\n7\n43.0\n57.0\n\n\n\n\n\n\n\nLet‚Äôs break it down:\n\nwe are using the df.iloc[ row-indices , column-indices] syntax to select by position\nthe row-indices parameter is the slice of integer indices 3:8. Remember the right endpoint (8) won‚Äôt be included.\nthe column-indices parameter is the list of integer indices 3 and 4. This means we are selecting the fourth and fifth column.\n\n\n\n\n\n\n\n\n\n\n\n\niloc vs.¬†loc: which one does what?\n\n\n\nAt the beginning, the difference between iloc and loc can be confusing. Remember the i in iloc stands for integer location, this reminds us iloc only uses integer indexing to retrieve information from the data frames in the same way as indexing for Python lists.\nIf you want to dive deeper, this is a great discussion about the difference between iloc and loc: Stackoverflow - How are iloc and loc different?\nAnd, as always, the documentation will provide you with more information: pandas.DataFrame.loc and pandas.DataFrame.iloc.\n\n\n\n\n\n\n\n\niloc for column selection? Avoid it!\n\n\n\nWe can also access columns by position using iloc - but it is best not to if possible.\nSuppose we want to access the 10th column in the data frame - then we want to select a column by position. In this case the 10th column is the annual sea level rise data and the 10th position corresponds to the index 9. We can select this column by position using the iloc selection:\n\n# Select column by position using iloc\n# The syntax is iloc[row-indices, column-indices]\n# [:,9] means \"select all rows from the 10th column\"\nannual_rise_3 = df.iloc[:,9]\nannual_rise_3.head()\n\n0    85.0\n1     NaN\n2    73.0\n3    70.0\n4    71.0\nName: SDW_fall, dtype: float64\n\n\nUnless you are really looking for information about the 10th column, do not access a column by position. This is bound to break in many ways:\n\nit relies on a person correctly counting the position of a column. Even with a small dataset this can be prone to error.\nit is not explicit: if we want information about sea level rise df.annual_sea_level_rise or df['annual_sea_level_rise'] are explicitely telling us we are accessing that information. df.iloc[:,9] is obscure and uninformative.\ndatastets can get updated. Maybe a new column was added before annual_sea_level_rise, this would change the position of the column, which would make any code depending on df.iloc[:,9] invalid.\n\nAccessing columns by labels helps reproducibility!\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Flow diagram for selecting core methods to subset a pandas.DataFrame.\n\n\n\n\n\n\n\nWhat is presented in this section is a comprehensive, but not exhaustive list of methods to select data in pandas.DataFrames. There are so many ways to subset data to get the same result. Some of the content from this lesson is adapted from the following resources and I encourage you to read them to learn more!\nüìñ Pandas getting started tutorials - How to I select a subset of a DataFrame\nüìñ Pandas documentation - User Guide - Indexing and Selecting Data\nüìñ Python for Data Analysis, 3E - Getting started with pandas",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#learning-objectives",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#learning-objectives",
    "title": "2 Subsetting",
    "section": "",
    "text": "By the end of this lesson, students will be able to:\n\nChoose appropriate methods for selecting rows and columns from a pandas.DataFrame\nConstruct conditions to subset rows\nDescribe the difference between label-based subsetting and position-based subsetting\nApply best practies when using iloc and loc selection",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#about-the-data",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#about-the-data",
    "title": "2 Subsetting",
    "section": "",
    "text": "In this lesson we will use annual estimates of bird species abundance in four coastal wetlands along the California coast. This dataset was derived for education purposes for this course from the UCSB SONGS Mitigation Monitoring: Wetland Performance Standard - Bird Abundance and Species Richness dataset [1]. The SONGS dataset was collected as part of the San Onofre Nuclear Generating Station (SONGS) San Dieguito Wetland Restoration monitoring program.\n\n\n\nSan Onofre Nuclear Generating Station in San Diego County, California. Source: Southern California Edison\n\n\nThe annual bird species abundance estimates is a CSV file with 13 columns and 14 rows. You can see the first three rows below.\n\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n\n\n\n\n\nThe four wetlands where the bird surveys occured are Carpinteria Salt Marsh (CSM), Mugu Lagoon (MUL), the San Dieguito Wetland (SDW), and the Tijuana Estuary (TJE). The values from the second column to the last column correspond to the number of different bird species recorded across the survey sites in each wetland during winter, spring, and fall of a given year. For example, the CSM_fall column has the number of species recorded in fall at Carpinteria Salt Marsh across years. The year column corresponds to the calendar year on which the data was collected. Surveys have happened yearly from 2010 to 2023.\n\n\n\nMugu Lagoon in Ventura County, California, seen from the Mugu Peak Trail. Source: USA National Park Service",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#csv-files",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#csv-files",
    "title": "2 Subsetting",
    "section": "",
    "text": "A CSV (Comma-Separated Values) file is an open, simple text format for storing tabular data, with rows separated by line breaks and columns by commas. It‚Äôs widely used in environmental science for sharing datasets like species counts and environmental monitoring data because it‚Äôs easy to create, read, and process in different platforms, without the need of proprietary software.\nTo read in a CSV file into our Python workspace as pandas.DataFrame we use the pandas.read_csv function:\n\nimport pandas as pd\n\n# Read in file, argument is the file path\ndf = pd.read_csv('data/wetlands_seasonal_bird_diversity.csv')\n\nNext, we obtain some high-level information about this data frame:\n\n# Print data frame's first five rows \ndf.head()\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\n\n# Print data frame's last five rows \ndf.tail()\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n9\n2019\n39.0\n39.0\n40.0\n57.0\n52.0\n53.0\n54.0\n55.0\n53.0\n63.0\n54.0\n50.0\n\n\n10\n2020\n46.0\nNaN\n47.0\n56.0\nNaN\n66.0\n57.0\nNaN\n58.0\n54.0\n40.0\n54.0\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n12\n2022\n40.0\n46.0\n49.0\n60.0\n55.0\n65.0\n57.0\n60.0\n57.0\n60.0\n61.0\n60.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\n\n# Print data frame's column names\ndf.columns\n\nIndex(['year', 'CSM_winter', 'CSM_spring', 'CSM_fall', 'MUL_winter',\n       'MUL_spring', 'MUL_fall', 'SDW_winter', 'SDW_spring', 'SDW_fall',\n       'TJE_winter', 'TJE_spring', 'TJE_fall'],\n      dtype='object')\n\n\n\n# List the data types of each column\ndf.dtypes\n\nyear            int64\nCSM_winter    float64\nCSM_spring    float64\nCSM_fall      float64\nMUL_winter    float64\nMUL_spring    float64\nMUL_fall      float64\nSDW_winter    float64\nSDW_spring    float64\nSDW_fall      float64\nTJE_winter    float64\nTJE_spring    float64\nTJE_fall      float64\ndtype: object\n\n\n\n# Print data frame's shape: output is a tuple (# rows, # columns)\ndf.shape\n\n(14, 13)",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-a-single-column",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-a-single-column",
    "title": "2 Subsetting",
    "section": "",
    "text": "Selecting a single column by column name is the simplest case for selecting data in a data frame. The genereal syntax to do this is:\ndf['column_name']\nNotice the column name is given as string inside the square brackets. This is an example of label-based subsetting, which means we want to select data from our data frame using the names of the columns, not their position. When we select rows or column using their position, we are doing position-based subsetting. We‚Äôll see some methods to do this when we move into selecting rows.\n\n\nSuppose we are interested in the number of bird species observed at the Mugu Lagoon in spring. We can access that single column in this way:\n\n# Select a single column by using square brackets []\nmul_spring = df['MUL_spring']\n\n# Print first five elements in this column\nmul_spring.head()\n\n0     NaN\n1    52.0\n2    58.0\n3    58.0\n4    52.0\nName: MUL_spring, dtype: float64\n\n\nSince we only selected a single column, mul_spring is a pandas.Series:\n\n# Check the type of the ouput\nprint(type(mul_spring))\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\npd.DataFrame = dictionary of columns\n\n\n\nRemember we can think of a pandas.DataFrame as a dictionary of its columns? Then we can access a single column using the column name as the key, just like we would do in a dictionary. That is the we just used: df['column_name'].\n\n\nWe can also do label-based subsetting of a single column using attribute syntax:\ndf.column_name\nFor example, to see the head of the MUL_spring column we would do:\n\ndf.MUL_spring.head()\n\n0     NaN\n1    52.0\n2    58.0\n3    58.0\n4    52.0\nName: MUL_spring, dtype: float64\n\n\n\n\n\n\n\n\nFavor df['column_name'] instead of df.column_name\n\n\n\nIn general, it is better to use the df['column_name'] syntax. A couple reasons why are:\n\ndf['column_name'] can take in any column name, while df.column_name only works if the column name has no spaces or special characters\ndf['column_name'] avoids conflicts with pd.DataFrame methods and attributes. For example, if df has a column named count, it‚Äôs ambiguous whehter pd.count is referring to the count() method or the count column.",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-multiple-columns",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-multiple-columns",
    "title": "2 Subsetting",
    "section": "",
    "text": "We can select multiple columns in a single call by passing a list with the column names to the square brackets []:\ndf[['column_1', 'column_10', 'column_245']]\nNotice there are double square brackets. This is because we are passing the list of names ['column_1', 'column_10', 'column_245'] to the selection brakcets [].\n\n\n\n\n\n\nCheck-in\n\n\n\nIs this an example of label-based selection or location-based selection?\n\n\n\n\nIf we want to look at the species in the Tijuana Estuary during winter and fall, then we can select these columns like this:\n\n# Select columns with names \"TJE_winter\" and \"TJE_fall\"\ntje_wf = df[['TJE_winter','TJE_fall']]\n\nNotice there are double square brackets. This is because we are passing the list of names ['TJE_winter','TJE_fall'] to the selection brakcets [].\n\n\n\n\n\n\nCheck-in\n\n\n\nWhat is the type and shape of the tje_wf output? Verify your answer.\n\n\n\n\n\n\nTo select a slice of the columns we will use a special case of loc selection (we‚Äôll cover the general one by the end of the lesson). The syntax will be\ndf.loc[ : , 'column_start':'column_end']\nwhere column_start and column_end are, respectively, the starting point and endpoint of the column slice we want to subset from the data frame.\nNotice two things:\n\nthe first value passed to loc is used for selecting rows, using a colon : as the row-selection parameter means ‚Äúselect all the rows‚Äù\nthe slice of the data frame we‚Äôll obtain includes both endpoints of the slice 'column_start':'column_end'. In other words, we‚Äôll get the column_start column and the column_end column. This is different from how slicing works in base Python and NumPy, where the endpoint is not included.\n\n\n\nLet‚Äôs select the slice of columns that covers all data from Carpinteria Salt Marsh and Mugu Lagoon. This corresponds to all columns between CSM_winter and MUL_fall.\n\n# Select columns between 'CSM_winter' and 'MUL_fall'\ncsm_mul = df.loc[:,'CSM_winter':'MUL_fall']\ncsm_mul.head()\n\n\n\n\n\n\n\n\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\n\n\n\n\n0\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\n\n\n1\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n\n\n2\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n\n\n3\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n\n\n4\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-rows",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-rows",
    "title": "2 Subsetting",
    "section": "",
    "text": "Now that we are familiar with some methods for selecting columns, let‚Äôs move on to selecting rows.\n\n\nSelecting rows that satisfy a particular condition is one of the most usual kinds of row subsetting. The general syntax for this type of selection is\ndf[condition_on_rows]\nThat condition_of_rows can be a myriad things, let‚Äôs see some usual scenarios.\n\n\nSuppose we are intersted in all data after 2020. We can select these rows in this way:\n\n# Select all rows with year &gt; 2020\npost_2020 = df[df['year']&gt;2020]\npost_2020\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n12\n2022\n40.0\n46.0\n49.0\n60.0\n55.0\n65.0\n57.0\n60.0\n57.0\n60.0\n61.0\n60.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\nLet‚Äôs break down what is happening here. The condition for our rows is df['year']&gt;2020, this is a pandas.Series with boolean values (True or False) indicating which rows satisfy the condition year&gt;2020:\n\n# Check the type of df['year']&gt;1996\nprint(type(df['year']&gt;2020))\n\n# Print the boolean series\ndf['year']&gt;2020\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11     True\n12     True\n13     True\nName: year, dtype: bool\n\n\nWhen we pass such a series of boolean values to the selection brackets [] we keep only the rows that correspond to a True value.\n\n\n\n\n\n\n\nCheck-in\n\n\n\nGet the subset of our data frame on which the San Dieguito Wetland has at least 75 species recorded during spring.\n\n\n\n\n\nSuppose we want to look at data from years 2012 to 2015 (including both years). One way of doing this is to use the between operator in our condition:\n\nsubset = df[df['year'].between(2012, 2015)]\nsubset\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n5\n2015\n44.0\n42.0\n45.0\n58.0\n50.0\n51.0\n71.0\n61.0\n65.0\n73.0\n76.0\n64.0\n\n\n\n\n\n\n\nLet‚Äôs break down this code:\n\ndf['year'] is the column with the year values, a pandas.Series\nin df['year'].between(), we have that between is a method for the pandas.Series and we are calling it using the dot .\n(2012, 2015) are the parameters for the between() method, from the pandas documentation we can see this method will subset including both endpoints\ndf['year'].between(2012, 2015) is then a pandas.Series of boolean values indicating which rows have year equal to 2012, 2013, 2014, or 2015.\nwhen we put df['year'].between(2012, 2015) inside the selection brackets [] we obtain the rows of the data frame with year equal to 2012, ‚Ä¶, 2015.\n\n\n\n\n\n\n\nAvoid using loc for selecting only rows\n\n\n\nIt is equivalent to write\n# Select rows with year&lt;2015\ndf[df['year']&lt;2015]\nand\n# Select rows with year&lt;2015 using loc\ndf.loc[ df['year']&lt;2015 , :]\nIn the second one:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']&lt;2015\nthe column-selection parameter is a colon :, which indicates we want all columns for the rows we are selecting.\n\nWe prefer the first syntax when we are selecting rows and not columns since it is simpler.\n\n\n\n\n\n\nWe can combine multipe conditions to select rows by surrounding each one in parenthesis () and using the or operator | and the and operator &.\n\n\nLet‚Äôs select rows in which the Carpinteria Salt Marsh has more than 50 registered in winter or fall:\n\ndf[ (df['CSM_winter']&gt;50) | (df['CSM_fall']&gt;50)]\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs select rows with in which both the Carpinteria Salt Marsh and the San Dieguito Wetland have more than 60 reported bird species during spring:\n\ndf[ (df['CSM_spring']&gt;60) & (df['SDW_spring']&gt;60)]\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n\n\n\n\n\nAn empty data frame! That‚Äôs ok, it just means there are no rows that satisfy the given condition.\n\n\n\n\nAll the selections we have done so far have been using labels. Sometimes we may want to select certain rows depending on their actual position in the data frame in other words, using position-based subsetting. To do this, we use iloc selection with the syntax\n df.iloc[row-indices]\niloc stands for integer-location based indexing.\n\n\n\n# Select the fifth row (index=4)\ndf.iloc[4]\n\nyear          2014.0\nCSM_winter      38.0\nCSM_spring      43.0\nCSM_fall        45.0\nMUL_winter      49.0\nMUL_spring      52.0\nMUL_fall        57.0\nSDW_winter      61.0\nSDW_spring      78.0\nSDW_fall        71.0\nTJE_winter      60.0\nTJE_spring      81.0\nTJE_fall        62.0\nName: 4, dtype: float64\n\n\n\n# Select rows 9 through 13, inclduing 13\ndf.iloc[9:14]\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n9\n2019\n39.0\n39.0\n40.0\n57.0\n52.0\n53.0\n54.0\n55.0\n53.0\n63.0\n54.0\n50.0\n\n\n10\n2020\n46.0\nNaN\n47.0\n56.0\nNaN\n66.0\n57.0\nNaN\n58.0\n54.0\n40.0\n54.0\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n12\n2022\n40.0\n46.0\n49.0\n60.0\n55.0\n65.0\n57.0\n60.0\n57.0\n60.0\n61.0\n60.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\nNotice that, since we are back to indexing by position, the right endpoint of the slice is not included in the ouput.",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-rows-and-columns-simultaneously",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#selecting-rows-and-columns-simultaneously",
    "title": "2 Subsetting",
    "section": "",
    "text": "Selecting rows and columns simultaneously can be done using loc (labels) or iloc (positions).\n\n\nWhen we want to select rows and columns simultaneously by labels (including using conditions) we can use loc selection with the syntax\ndf.loc[ row-selection , column-selection]\nspecifying both paratmers: row-selection and column-selection. These parameters can be a condition or a subset of labels from the index or the column names.\n\n\nLet‚Äôs select the winter surveys for Mugu Lagoon and the Tijuana Estuary after 2020:\n\ndf.loc[df['year']&gt;2020, ['MUL_winter','TJE_winter']]\n\n\n\n\n\n\n\n\nMUL_winter\nTJE_winter\n\n\n\n\n11\n54.0\n53.0\n\n\n12\n60.0\n60.0\n\n\n13\n72.0\n60.0\n\n\n\n\n\n\n\nLet‚Äôs break down this code:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']&gt;2020, which is a boolean array saying which years are greater than 2020\nthe column-selection parameter is ['MUL_winter','TJE_winter'] which is a list with the names of the two columns we are interested in.\n\n\n\n\n\nWhen we want to select rows and columns simultaneously by position we use iloc selection with the syntax:\ndf.iloc[ row-indices , column-indices]\n\n\nSuppose we want to select rows 3-7 (including 7) and columns 3 and 4:\n\ndf.iloc[3:8, [3,4]]\n\n\n\n\n\n\n\n\nCSM_fall\nMUL_winter\n\n\n\n\n3\n38.0\n60.0\n\n\n4\n45.0\n49.0\n\n\n5\n45.0\n58.0\n\n\n6\n47.0\n63.0\n\n\n7\n43.0\n57.0\n\n\n\n\n\n\n\nLet‚Äôs break it down:\n\nwe are using the df.iloc[ row-indices , column-indices] syntax to select by position\nthe row-indices parameter is the slice of integer indices 3:8. Remember the right endpoint (8) won‚Äôt be included.\nthe column-indices parameter is the list of integer indices 3 and 4. This means we are selecting the fourth and fifth column.",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#notes-about-loc-and-iloc",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#notes-about-loc-and-iloc",
    "title": "2 Subsetting",
    "section": "",
    "text": "iloc vs.¬†loc: which one does what?\n\n\n\nAt the beginning, the difference between iloc and loc can be confusing. Remember the i in iloc stands for integer location, this reminds us iloc only uses integer indexing to retrieve information from the data frames in the same way as indexing for Python lists.\nIf you want to dive deeper, this is a great discussion about the difference between iloc and loc: Stackoverflow - How are iloc and loc different?\nAnd, as always, the documentation will provide you with more information: pandas.DataFrame.loc and pandas.DataFrame.iloc.\n\n\n\n\n\n\n\n\niloc for column selection? Avoid it!\n\n\n\nWe can also access columns by position using iloc - but it is best not to if possible.\nSuppose we want to access the 10th column in the data frame - then we want to select a column by position. In this case the 10th column is the annual sea level rise data and the 10th position corresponds to the index 9. We can select this column by position using the iloc selection:\n\n# Select column by position using iloc\n# The syntax is iloc[row-indices, column-indices]\n# [:,9] means \"select all rows from the 10th column\"\nannual_rise_3 = df.iloc[:,9]\nannual_rise_3.head()\n\n0    85.0\n1     NaN\n2    73.0\n3    70.0\n4    71.0\nName: SDW_fall, dtype: float64\n\n\nUnless you are really looking for information about the 10th column, do not access a column by position. This is bound to break in many ways:\n\nit relies on a person correctly counting the position of a column. Even with a small dataset this can be prone to error.\nit is not explicit: if we want information about sea level rise df.annual_sea_level_rise or df['annual_sea_level_rise'] are explicitely telling us we are accessing that information. df.iloc[:,9] is obscure and uninformative.\ndatastets can get updated. Maybe a new column was added before annual_sea_level_rise, this would change the position of the column, which would make any code depending on df.iloc[:,9] invalid.\n\nAccessing columns by labels helps reproducibility!",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#summary",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#summary",
    "title": "2 Subsetting",
    "section": "",
    "text": "Figure¬†1: Flow diagram for selecting core methods to subset a pandas.DataFrame.",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#resources",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-pandas-subsetting.html#resources",
    "title": "2 Subsetting",
    "section": "",
    "text": "What is presented in this section is a comprehensive, but not exhaustive list of methods to select data in pandas.DataFrames. There are so many ways to subset data to get the same result. Some of the content from this lesson is adapted from the following resources and I encourage you to read them to learn more!\nüìñ Pandas getting started tutorials - How to I select a subset of a DataFrame\nüìñ Pandas documentation - User Guide - Indexing and Selecting Data\nüìñ Python for Data Analysis, 3E - Getting started with pandas",
    "crumbs": [
      "notes",
      "Tabular data",
      "2 Subsetting"
    ]
  },
  {
    "objectID": "conventions.html",
    "href": "conventions.html",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Any website, package, scientific article, dataset or other source should be cited. The IEEE citation style will be used. This is set up via de iee-with-url.csl file and the bibliography and csl parameters in the _quarto.yml file.\nReferences are add into one file for the whole course: references.bib."
  },
  {
    "objectID": "conventions.html#citations",
    "href": "conventions.html#citations",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Any website, package, scientific article, dataset or other source should be cited. The IEEE citation style will be used. This is set up via de iee-with-url.csl file and the bibliography and csl parameters in the _quarto.yml file.\nReferences are add into one file for the whole course: references.bib."
  },
  {
    "objectID": "conventions.html#subsection-names",
    "href": "conventions.html#subsection-names",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Subsection names",
    "text": "Subsection names\n\nOnly the first letter in subsection names should be capitalized.\nExamples should follow the format ‚ÄúExample: what this example is about‚Äù and should be unlisted from the table of contents using {.unlisted}."
  },
  {
    "objectID": "conventions.html#file-naming",
    "href": "conventions.html#file-naming",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "File naming",
    "text": "File naming\nAll files (except README) should be named in small caps and each work separated by -."
  },
  {
    "objectID": "conventions.html#discussion-sections",
    "href": "conventions.html#discussion-sections",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Discussion sections",
    "text": "Discussion sections\nAll discussion sections should have the following YAML heading:\n---\ntitle: Topic of discussion section\nsubtitle: Week n - Discussion section \ndate: YYYY-MM-DD\nweek: week n\nimage: images/ds-weekn.png\nsidebar: false\n---\nStructure for sections files should be:\n\nBrief intro about what the discussion section is about, followed by In ‚Äòthis discussion section, you will:‚Äô then a list of what will happen in the section.\nSetup in a :::{.callout-tip appearance=\"minimal\"}} ::: block\nGeneral directions in a :::{.callout-tip appearance=\"minimal\"}} ::: block\nAbout the data ina a :::{.callout-note appearance=\"minimal\"}} :::\nExercises numbered"
  },
  {
    "objectID": "conventions.html#lessons",
    "href": "conventions.html#lessons",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Lessons",
    "text": "Lessons\n\nYAML\n---\ntoc-title: In this lesson\n---\n\n\nLearning objectives\nState them using ‚ÄúBy the end of this lesson, students will be able to:‚Äù"
  },
  {
    "objectID": "discussion-sections/ds0-sections-setup.html",
    "href": "discussion-sections/ds0-sections-setup.html",
    "title": "Repository setup",
    "section": "",
    "text": "This discussion section will guide you through setting up a repository where you will collect your notebooks for the course‚Äôs discussion sections. In this discussion section, you will:"
  },
  {
    "objectID": "discussion-sections/ds0-sections-setup.html#setup",
    "href": "discussion-sections/ds0-sections-setup.html#setup",
    "title": "Repository setup",
    "section": "Setup",
    "text": "Setup\n\n\n\n\n\n\nOur TA will guide the activities for this discussion section."
  },
  {
    "objectID": "discussion-sections/ds0-sections-setup.html#create-repository",
    "href": "discussion-sections/ds0-sections-setup.html#create-repository",
    "title": "Repository setup",
    "section": "1. Create repository",
    "text": "1. Create repository\n\nCreate a new repository on GitHub. Use the following settings:\n\nRepository name: eds220-2024-sections.\nDescription: Work during discussion sections for the EDS 220 MEDS course.\nKeep the repository public.\nAdd a README file.\nAdd a Python .gitignore template.\n\nIn GitHub, update your repository‚Äôs README by:\n\nDeleting the text that was automatically generated when you created the repo.\nCopy-pasting the markdown text below, do not update [YOUR NAME HERE] yet.\n\n\n# EDS 220 Discussion Sections Repository\n\nThis repository hosts all the work completed by [YOUR NAME HERE] during the discussion sections of EDS 220 - *Working with Environmental Datasets*.\n\n## Course Information\n\n- **Course Title:** EDS 220 - Working with Environmental Datasets\n- **Term:** Fall 2024\n- **Program:** [UCSB Masters in Environmental Data Science](https://bren.ucsb.edu/masters-programs/master-environmental-data-science)\n- **Instructor:** Carmen Galaz Garc√≠a\n- **TA:** Annie Adams \n\nComplete materials for the discussion sections and additional resources can be found on the [course website](https://meds-eds-220.github.io/MEDS-eds-220-course/discussion-sections/discussion-sections-listing.html).\n\nAdd the URL to your GitHub repository to this spreadhseet.\n\n\nMAKE SURE YOU‚ÄôVE SUCCESSFULLY SET UP YOUR REPOSITORY ON GITHUB BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections/ds0-sections-setup.html#clone-the-repository",
    "href": "discussion-sections/ds0-sections-setup.html#clone-the-repository",
    "title": "Repository setup",
    "section": "2. Clone the repository",
    "text": "2. Clone the repository\n\nAt Bren‚Äôs workbench 1 server, start a new JupyterLab session or access an active one.\nInside your MEDS/ directory, create a new directory called EDS-220. We‚Äôll use this directory to collect all the repositories used in the course.\nIn the terminal, navigate to the EDS-220 directory by using cd NAME-OF-DIRECTORY.\nUsing the terminal, clone the repository into your EDS-220 directory. To do this:\n\nIn your repository‚Äôs landing page on GitHub, click on the green button that says &lt; &gt; Code. In the HTTPS tab option, copy the URL that appears there.\nGo back to the server. In the terminal, run pwd (print working directory) to make sure we‚Äôll be cloning into the desired location (inside the EDS-220 directory). The ouput should look like:\n\n/Users/c_galazgarcia/MEDS/EDS-220/\n\nRun git clone URL. You should see the directory appear in the folder navigation bar to the left\nGo into the cloned repo in the terminal: cd eds220-2024-sections."
  },
  {
    "objectID": "discussion-sections/ds0-sections-setup.html#update-your-name-and-push-changes",
    "href": "discussion-sections/ds0-sections-setup.html#update-your-name-and-push-changes",
    "title": "Repository setup",
    "section": "3. Update your name and push changes",
    "text": "3. Update your name and push changes\n\nOpen the README.md file and update the [YOUR NAME HERE] with your name. Save your changes.\nPush your changes to your remote repository (remember, in this case, this is how we call the copy of your repo that lives in GitHub). To do this, remember the basic git routine:\n\ngit status : check git status, shows which files are staged, modified, or untracked\ngit add FILE-NAME : stage files that have been updated, these files will be added to your next commit\ngit status : check git status again to confirm you have staged the right files\ngit commit -m \"Commit message\" : create a commit with message\ngit pull : pull latest changes before pushing to ensure your local repo is up-to-date\ngit push : push your changes to upstream repo\n\nYou should have previously configured your personal access token (PAT) during summer session, but it might be possible that it is not cached anymore and you are prompted to create a new one when doing git push. If this is the case:\n\nFollow the instructions in the MEDS setup guide to create a new PAT.\nOnce you have created your PAT, do git push again to push your changes.\n\n\n\nMAKE SURE YOU‚ÄôVE SUCCESSFULLY PUSHED YOUR UPDATES BEFORE ENDING"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to the course materials for EDS 220 - Working with Environmental Datasets! In this website you will find all the materials for the Fall 2024 term. This course is part of the UCSB Masters in Environmental Data Science.\n\n\nThis hands-on course explores widely used environmental data formats and Python libraries for analyzing diverse environmental data. Students will gain experience working with popular open data repositories and cloud platforms to source and analyze real-world environmental datasets. The course will also serve as an introduction to Python programming and provide opportunities to practice effective communication of the strengths and weaknesses of students‚Äô data products and analyses.\n\n\n\n\n\n\nCarmen Galaz Garc√≠a (she/her/hers)\n\nE-mail: galazgarcia@bren.ucsb.edu\nStudent hours: Wednesday 4-5 @ Bren Hall 4424\n\n\n\n\nAnnie Adams (she/her/hers)\n\nE-mail: aradams@ucsb.edu\nStudent hours: Thursday 12 pm - 1 pm @ Bren Hall 3022\n\n\n\n\n\n\nClick here to access the syllabus.\n\n\n\nThe following is our tentative calendar. The course content and calendar may be subject to change as the course progresses.\n\n\n\n\nüìù If you have suggestions on how to correct, improve, or expand these course materials, please feel free to email the course instructor at galazgarcia@bren.ucsb.edu or file a GitHub issue.\nüåü If these materials have been useful to you, consider adding a star to the project‚Äôs repository!"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Welcome!",
    "section": "",
    "text": "This hands-on course explores widely used environmental data formats and Python libraries for analyzing diverse environmental data. Students will gain experience working with popular open data repositories and cloud platforms to source and analyze real-world environmental datasets. The course will also serve as an introduction to Python programming and provide opportunities to practice effective communication of the strengths and weaknesses of students‚Äô data products and analyses."
  },
  {
    "objectID": "index.html#instruction-team",
    "href": "index.html#instruction-team",
    "title": "Welcome!",
    "section": "",
    "text": "Carmen Galaz Garc√≠a (she/her/hers)\n\nE-mail: galazgarcia@bren.ucsb.edu\nStudent hours: Wednesday 4-5 @ Bren Hall 4424\n\n\n\n\nAnnie Adams (she/her/hers)\n\nE-mail: aradams@ucsb.edu\nStudent hours: Thursday 12 pm - 1 pm @ Bren Hall 3022"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Welcome!",
    "section": "",
    "text": "Click here to access the syllabus."
  },
  {
    "objectID": "index.html#calendar",
    "href": "index.html#calendar",
    "title": "Welcome!",
    "section": "",
    "text": "The following is our tentative calendar. The course content and calendar may be subject to change as the course progresses."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "Welcome!",
    "section": "",
    "text": "üìù If you have suggestions on how to correct, improve, or expand these course materials, please feel free to email the course instructor at galazgarcia@bren.ucsb.edu or file a GitHub issue.\nüåü If these materials have been useful to you, consider adding a star to the project‚Äôs repository!"
  },
  {
    "objectID": "assignments/assignment1.html",
    "href": "assignments/assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This assignment covers topics in the notes from the Python review to the plotting with pandas lesson. Task 1 will contribute 20% to the total grade of the assignment and tasks 2 and 3 will contribute 40% each.\n\n\n\n\n\n\n\n\n\nThis assignment is due by 11:59 pm on Saturday, September 12. All tasks for this assignment should be submitted via Gradescope. Make sure you double-check your submission to ensure it satisfies all the items in this checklist:\n\nAnswer for task 1 must be a PDF file.\nAnswers for tasks 2 and 3 must be submitted as .ipynb files (Jupyter Notebooks) to Gradescope, not a PDF or other format.\nEnsure your notebooks include a link to your assignment‚Äôs GitHub repository in the designated section.\nThe notebooks you submit must have your solutions to the exercises, they should NOT be the blank template notebooks.\nDouble-check that each notebook or PDF is uploaded to the correct task on Gradescope.\n\nResubmissions after the due date due to not satisfying one of the checks above will be strictly held to the course‚Äôs 50%-regrade resubmission policy (see syllabus).\nIf you have any questions, please reach out to the TA or instructor by 5 pm Friday, September 11. \n\n\n\n\n\n\nSo much goes into creating a dataset, and data is more than numbers and words in a file. Without a proper understanding of the whole context where data was created, biases, omissions, and inacuracies can go undetected. The Datasheets for Datasets [1] advocates for transparency about the purpose and contents of datasets.\nCheck out this short interview with lead author Dr.¬†Timnit Gebru, the executive director of the Distributed Artificial Intelligence Research Institute (DAIR), on the motivation to write this article:\n\nRead the paper and write a one-paragraph (between 100 and 150 words) open reflection about it. Review the rubric for this assignment here. Answer at least one of the following questions for your reflection:\n\nCan you think of a dataset you have worked with or encountered in your studies that would have benefited from a datasheet? Explain why or why not, using specific details about the dataset‚Äôs context, collection methods, or biases.\nWhat do you think are the limitations of the datasheets framework? Are there any challenges or risks associated with this approach, and how might they be addressed in practical settings?\nHow does the topic of transparency in datasets relate to your understanding of ethical data science practices? Provide an example where increased transparency could have changed the outcome of a dataset you have used or read about.\nBased on your previous professional experience, if you were tasked with creating a dataset for a project, what challenges or decisions would you face when creating its datasheet? Reflect on one or two aspects of data collection or transparency that you feel are particularly important.\n\n\n\n\n\n\n\n\nReady to submit your answer? Make sure your submission follows the checklist at the top of the assginment!\n\n\n\n\n\n\n\n\n\n\n\n\n\nFork this repository: https://github.com/MEDS-eds-220/eds220-hwk-1\nIn the Taylor server, start a new JupyterLab session or access an active one.\nUsing the terminal, clone your eds220-hwk-1 repository to a new directory under your eds-220 directory.\nIn the terminal, use cd to navigate into the eds-220-hwk-1 directory. Use pwd to verify eds-220-hwk-1 is your current working directory.\n\n\n\n\n\n\n\nFor this task we are going to use data about Western Indian Ocean Coral Diversity [2] stored in the the Knowledge Network for Biocomplexity (KNB) data repository. The author for this dataset is Dr.¬†Tim McClanahan, senior conservation zoologist at Wildlife Conservation Society.\n\n\n\nDr.¬†Tim McClanahan underwater surveying coral reefs in coastal Tanzania. Photo credit: ¬©Michael Markovina. From the online article How Mount Kilimanjaro and We Can Save Corals\n\n\nFollow the instructions in the notebook hwk1-task2-corals.ipynb to complete this task. Review the rubric for this assignment here. In this task you will practice:\n\npreliminary data exploration\naccessing data using a URL from a data archive\nselecting data from a data frame\nbasic git workflow\ncommenting your code\n\n\n\n\n\n\n\nReady to submit your answers? Make sure your submission follows the checklist at the top of the assginment!\n\n\n\n\n\n\nThis task is adapted from the Pandas Fundamentals with Earthquake Data assignment from the e-book Earth and Environmental Data Science [3].\nYou will use simplified data from the USGS Earthquakes Database.\n\nFollow the instructions in the notebook hwk1-task3-earthquakes.ipynb to complete this task.Review the rubric for this assignment here. Here you will practice:\n\naccessing data from your directory\nselecting data from a data frame\ncreating exploratory graphs\nbasic git workflow\ncommenting your code\n\n\n\n\n\n\n\nReady to submit your answers? Make sure your submission follows the checklist at the top of the assginment!"
  },
  {
    "objectID": "assignments/assignment1.html#submission-instructions",
    "href": "assignments/assignment1.html#submission-instructions",
    "title": "Assignment 1",
    "section": "",
    "text": "This assignment is due by 11:59 pm on Saturday, September 12. All tasks for this assignment should be submitted via Gradescope. Make sure you double-check your submission to ensure it satisfies all the items in this checklist:\n\nAnswer for task 1 must be a PDF file.\nAnswers for tasks 2 and 3 must be submitted as .ipynb files (Jupyter Notebooks) to Gradescope, not a PDF or other format.\nEnsure your notebooks include a link to your assignment‚Äôs GitHub repository in the designated section.\nThe notebooks you submit must have your solutions to the exercises, they should NOT be the blank template notebooks.\nDouble-check that each notebook or PDF is uploaded to the correct task on Gradescope.\n\nResubmissions after the due date due to not satisfying one of the checks above will be strictly held to the course‚Äôs 50%-regrade resubmission policy (see syllabus).\nIf you have any questions, please reach out to the TA or instructor by 5 pm Friday, September 11."
  },
  {
    "objectID": "assignments/assignment1.html#task-1-datasheets-for-datasets-reading",
    "href": "assignments/assignment1.html#task-1-datasheets-for-datasets-reading",
    "title": "Assignment 1",
    "section": "",
    "text": "So much goes into creating a dataset, and data is more than numbers and words in a file. Without a proper understanding of the whole context where data was created, biases, omissions, and inacuracies can go undetected. The Datasheets for Datasets [1] advocates for transparency about the purpose and contents of datasets.\nCheck out this short interview with lead author Dr.¬†Timnit Gebru, the executive director of the Distributed Artificial Intelligence Research Institute (DAIR), on the motivation to write this article:\n\nRead the paper and write a one-paragraph (between 100 and 150 words) open reflection about it. Review the rubric for this assignment here. Answer at least one of the following questions for your reflection:\n\nCan you think of a dataset you have worked with or encountered in your studies that would have benefited from a datasheet? Explain why or why not, using specific details about the dataset‚Äôs context, collection methods, or biases.\nWhat do you think are the limitations of the datasheets framework? Are there any challenges or risks associated with this approach, and how might they be addressed in practical settings?\nHow does the topic of transparency in datasets relate to your understanding of ethical data science practices? Provide an example where increased transparency could have changed the outcome of a dataset you have used or read about.\nBased on your previous professional experience, if you were tasked with creating a dataset for a project, what challenges or decisions would you face when creating its datasheet? Reflect on one or two aspects of data collection or transparency that you feel are particularly important.\n\n\n\n\n\n\n\n\nReady to submit your answer? Make sure your submission follows the checklist at the top of the assginment!"
  },
  {
    "objectID": "assignments/assignment1.html#setup-for-tasks-2-and-3",
    "href": "assignments/assignment1.html#setup-for-tasks-2-and-3",
    "title": "Assignment 1",
    "section": "",
    "text": "Fork this repository: https://github.com/MEDS-eds-220/eds220-hwk-1\nIn the Taylor server, start a new JupyterLab session or access an active one.\nUsing the terminal, clone your eds220-hwk-1 repository to a new directory under your eds-220 directory.\nIn the terminal, use cd to navigate into the eds-220-hwk-1 directory. Use pwd to verify eds-220-hwk-1 is your current working directory."
  },
  {
    "objectID": "assignments/assignment1.html#task-2-exploring-coral-diversity-data",
    "href": "assignments/assignment1.html#task-2-exploring-coral-diversity-data",
    "title": "Assignment 1",
    "section": "",
    "text": "For this task we are going to use data about Western Indian Ocean Coral Diversity [2] stored in the the Knowledge Network for Biocomplexity (KNB) data repository. The author for this dataset is Dr.¬†Tim McClanahan, senior conservation zoologist at Wildlife Conservation Society.\n\n\n\nDr.¬†Tim McClanahan underwater surveying coral reefs in coastal Tanzania. Photo credit: ¬©Michael Markovina. From the online article How Mount Kilimanjaro and We Can Save Corals\n\n\nFollow the instructions in the notebook hwk1-task2-corals.ipynb to complete this task. Review the rubric for this assignment here. In this task you will practice:\n\npreliminary data exploration\naccessing data using a URL from a data archive\nselecting data from a data frame\nbasic git workflow\ncommenting your code\n\n\n\n\n\n\n\nReady to submit your answers? Make sure your submission follows the checklist at the top of the assginment!"
  },
  {
    "objectID": "assignments/assignment1.html#task-3-pandas-fundamentals-with-earthquake-data",
    "href": "assignments/assignment1.html#task-3-pandas-fundamentals-with-earthquake-data",
    "title": "Assignment 1",
    "section": "",
    "text": "This task is adapted from the Pandas Fundamentals with Earthquake Data assignment from the e-book Earth and Environmental Data Science [3].\nYou will use simplified data from the USGS Earthquakes Database.\n\nFollow the instructions in the notebook hwk1-task3-earthquakes.ipynb to complete this task.Review the rubric for this assignment here. Here you will practice:\n\naccessing data from your directory\nselecting data from a data frame\ncreating exploratory graphs\nbasic git workflow\ncommenting your code\n\n\n\n\n\n\n\nReady to submit your answers? Make sure your submission follows the checklist at the top of the assginment!"
  },
  {
    "objectID": "week-by-week/week-by-week.html",
    "href": "week-by-week/week-by-week.html",
    "title": "Week by week",
    "section": "",
    "text": "You will find the course announcements and daily activities here.\n\n\n\n\n\nCourse introduction slides\nSet up of GitHub repository for in-class coding sessions. \nPython review up to the end of the variables section.\n\n\n\n\n\n\n\nPreparation for Wednesday class (October 2)\n\n\n\n\nIn your EDS-220/eds220-2024-in-class directory, create a new Python notebook called week1-pandas-series.ipynb.\nRead the notes chapter on pandas series data frames and follow along with the code.\nSolve the check-in exercises. We‚Äôll present these during class.\nMake a summary of the lesson. What are the most important concepts or ideas?\n\n\n\n\n\n\n\nFinished Python review.\nStudent presentations of pandas.Series and pandas.DataFrames exercises\nCompleted the following activity to add data/ directory the .gitignore file of the EDS-220/in-class-notebooks/ directory.\n\n\n\n\nThere‚Äôs no setup for this week‚Äôs discussion section. We‚Äôll follow the materials in"
  },
  {
    "objectID": "week-by-week/week-by-week.html#week-1-sept-30---oct-6",
    "href": "week-by-week/week-by-week.html#week-1-sept-30---oct-6",
    "title": "Week by week",
    "section": "",
    "text": "Course introduction slides\nSet up of GitHub repository for in-class coding sessions. \nPython review up to the end of the variables section.\n\n\n\n\n\n\n\nPreparation for Wednesday class (October 2)\n\n\n\n\nIn your EDS-220/eds220-2024-in-class directory, create a new Python notebook called week1-pandas-series.ipynb.\nRead the notes chapter on pandas series data frames and follow along with the code.\nSolve the check-in exercises. We‚Äôll present these during class.\nMake a summary of the lesson. What are the most important concepts or ideas?\n\n\n\n\n\n\n\nFinished Python review.\nStudent presentations of pandas.Series and pandas.DataFrames exercises\nCompleted the following activity to add data/ directory the .gitignore file of the EDS-220/in-class-notebooks/ directory.\n\n\n\n\nThere‚Äôs no setup for this week‚Äôs discussion section. We‚Äôll follow the materials in"
  },
  {
    "objectID": "slides/syllabus-slides.html#title-slide",
    "href": "slides/syllabus-slides.html#title-slide",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "EDS 220: Working with Environmental Data\nCourse logistics & syllabus\n\nWeek 1"
  },
  {
    "objectID": "slides/syllabus-slides.html#welcome",
    "href": "slides/syllabus-slides.html#welcome",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Welcome to EDS 220!\n\nThis course focuses on hands-on exploration of widely-used environmental data formats and Python libraries. Together, we‚Äôll work with real-world datasets, giving you the skills to analyze and understand the environment around us.\n\n\nBanner by NASA‚Äôs Your Name in Landsat"
  },
  {
    "objectID": "slides/syllabus-slides.html#basics",
    "href": "slides/syllabus-slides.html#basics",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "The basics\n\nInstructor\n\nCarmen Galaz Garc√≠a (she/her/hers)\n\nE-mail: galazgarcia@bren.ucsb.edu\nStudent hours: Wednesday 4-5 @ Bren Hall 4424\n\nTA\n\nAnnie Adams (she/her/hers)\n\nE-mail: aradams@ucsb.edu\nStudent hours: Thursday 12 pm - 1 pm @ Bren Hall 3022\n\n\nClass Schedule: Monday and Wednesday, 9:30 - 10:45 AM, Bren Hall 1424\nDiscussion Sections: Thursday, 1:00 - 1:50 PM, Bren Hall 3022"
  },
  {
    "objectID": "slides/syllabus-slides.html#about-me",
    "href": "slides/syllabus-slides.html#about-me",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "About me\n\n\n\n\nAssistant Teaching Professor @ Bren\n\n\nBefore that:\n\nData Scientist @ NCEAS\nPh.D.¬†in Mathematics @ UCSB\n\n\n\nResearch:\n\nImage analysis for invasive plant species detection\n\n\n\nTeaching:\n\nDeveloping our MEDS Python curriculum!"
  },
  {
    "objectID": "slides/syllabus-slides.html#section",
    "href": "slides/syllabus-slides.html#section",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Introductions\n\n\nIn the next few minutes, talk with a person next to you and ask them what parts of Santa Barbara have you enjoyed exploring.\n\n\nYou‚Äôll get to introduce your partner at the end."
  },
  {
    "objectID": "slides/syllabus-slides.html#learning-objectives",
    "href": "slides/syllabus-slides.html#learning-objectives",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Learning Objectives\n\nBy the end of this course, you will be able to:\n\nWrite Python code from scratch following best practices and adapt code others write.\n\n\n\nManipulate various types of environmental data, including tabular, vector, and raster data, using established Python libraries.\n\n\n\nFind and access datasets from major public environmental databases.\n\n\n\nProduce effective reports that combine text and code to share their data analyses with colleagues."
  },
  {
    "objectID": "slides/syllabus-slides.html#schedule",
    "href": "slides/syllabus-slides.html#schedule",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Tentative Schedule"
  },
  {
    "objectID": "slides/syllabus-slides.html#class-snapshot-1",
    "href": "slides/syllabus-slides.html#class-snapshot-1",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Class snapshot 1",
    "text": "Class snapshot 1"
  },
  {
    "objectID": "slides/syllabus-slides.html#class-snapshot-2",
    "href": "slides/syllabus-slides.html#class-snapshot-2",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Class snapshot 2",
    "text": "Class snapshot 2"
  },
  {
    "objectID": "slides/syllabus-slides.html#code-of-conduct",
    "href": "slides/syllabus-slides.html#code-of-conduct",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Code of Conduct\n\n \nWe expect all course participants (including instructors, guests, and students) to be committed to actively creating, modeling, and maintaining an inclusive climate and supportive learning environment for all.\n\n\nWe expect everyone to treat every member of our learning community with respect.\n\n\n\nHarassment of any kind will not be tolerated.\n\n\n\nEveryone is expected to read and adhere to the Bren School Code of Conduct and the UCSB Code of Conduct."
  },
  {
    "objectID": "slides/syllabus-slides.html#accommodations",
    "href": "slides/syllabus-slides.html#accommodations",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Access & Accommodations\n\n\nIf you have any kind of disability, whether apparent or non-apparent, learning, emotional, physical, or cognitive, you may be eligible to use formal accessibility services on campus.\n\n\nTo arrange class-related accommodations, please contact the Disabled Students Program (DSP). DSP will initiate communication about accommodations with faculty.\n\n\n\nBy making a plan through DSP, appropriate accommodations can be implemented without disclosing your specific condition or diagnosis to course instructors."
  },
  {
    "objectID": "slides/syllabus-slides.html#evaluation-and-grading",
    "href": "slides/syllabus-slides.html#evaluation-and-grading",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Evaluation and Grading",
    "text": "Evaluation and Grading\n\n\n\nGrading Breakdown:\n\nHomework: 75% (4 assignments)\nPortfolio: 20%\nParticipation: 5%\n\n\n\nGrade Cutoffs:\n\nA+ (‚â• 97%), A (‚â• 92%), A- (‚â• 90%),\nB+ (‚â• 87%) , B (‚â• 82%), B- (‚â• 80%),\nC+ (‚â• 77%), C (‚â• 72%), C-(‚â• 70%),\nD+ (‚â• 67%), D (‚â• 62%), D-(‚â• 60%),\n(60&gt;) F."
  },
  {
    "objectID": "slides/syllabus-slides.html#homework",
    "href": "slides/syllabus-slides.html#homework",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Homework Assignments\n\n \n\nThere will be 4 homework assignments.\nAssignments are assigned every other Friday starting on week 1 and should be submitted by 11:59 pm on next week‚Äôs Saturday.\n\n\n\nWorking together and collaborating with peers on homework is highly encouraged!\nSubmissions are individual so make sure you understand everything you are turning in."
  },
  {
    "objectID": "slides/syllabus-slides.html#portoflio",
    "href": "slides/syllabus-slides.html#portoflio",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Portfolio Project\n\nThe final assignment for the course will be creating data science materials for the students‚Äô online professional portfolio.\nFinal Assignment:\nThe 20% grade for the portfolio is divided as follows:\n\n13% Data analysis + GitHub repository: a presentation-ready GitHub repository containing a finalized Jupyter Notebook and associated files for the data analysis,\n7% blog post: a blog post in the student‚Äôs professional portfolio based on previous assignments and discussion sections\n\nBoth a submission and a revised submission addressing all the feedback from the first revision will be needed for these two tasks."
  },
  {
    "objectID": "slides/syllabus-slides.html#participation",
    "href": "slides/syllabus-slides.html#participation",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Participation Requirements\n\nTo obtain full participation credit:\n\nAnswer two short surveys about their course experiences, one at the beginning and one at the end of the course.\nShare coding solutions for exercises or homework during lecture or discussion sections at least once during the course.\n\nA presentation date during the discussion section has been randomly assigned to each student.\nYou can trade dates with others. Please notify the TA or instructor about any presentation updates.\nTime for presentation during class time may also be available.\n\n\n\nWhy come up to present your solutions? Many reasons! To ractice public speaking, get comfortable with technical vocabulary, practice explaining a step-by-step solution, practice the material by teaching others, have a taste of live-coding, among others."
  },
  {
    "objectID": "slides/syllabus-slides.html#GenAI",
    "href": "slides/syllabus-slides.html#GenAI",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Policy on Generative AI (GenAI)\n\nGenAI tools (such as ChatGPT) are strongly discouraged for the following reasons:\n\n\nbecoming proficient in core programming skills comes through practice\nbuilding your own programming proficiency will help you engage with GenAI tools more efficiently and responsibly\nsubscription versions of GenAI tools may induce an inequitable learning environment\n\n\n\nPlease adhere to these guidelines:\n\n\nyou may use spell / grammar check and / or synonym identification tools\nbe prepared to explain each line of code in your assignments and exercises\nassignments that make a low-energy or unreflective use of GenAI will be heavily penalized.\n\n\n\n\n\nPlease read the full policy on the course syllabus"
  },
  {
    "objectID": "slides/syllabus-slides.html#student-resources",
    "href": "slides/syllabus-slides.html#student-resources",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Student Resources\n\n\nBasic Needs Resources & Food Security: https://basicneeds.ucsb.edu/ (schedule a CalFresh Appoinment or Basic Needs Advising Session)\nCounseling & Psychological Services (CAPS): http://caps.sa.ucsb.edu\nResource Center for Sexual and Gender Diversity (RCSGD): https://rcsgd.sa.ucsb.edu/\nUndocumented Student Services (USS) Program: https://uss.sa.ucsb.edu/\nCampus Learning Assistance Services (CLAS): http://clas.sa.ucsb.edu\nStudent Resource Building (SRB): http://www.sa.ucsb.edu/student-resource-building/home\nMulticultural Center (MCC): http://mcc.sa.ucsb.edu/\nCampus Advocacy, Resources, & Education (CARE): http://wgse.sa.ucsb.edu/care/home\nFinancial Crisis Response Team: financialcrisis@sa.ucsb.edu (contact)\nHealth and Wellness: https://wellbeing.ucsb.edu/"
  },
  {
    "objectID": "discussion-sections/discussion-sections-listing.html",
    "href": "discussion-sections/discussion-sections-listing.html",
    "title": "Discussion sections",
    "section": "",
    "text": "week\n\n\ntopic\n\n\n\n\n\n\n\n\n\n¬†\n\n\nRepository setup\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "book/chapters/lesson-3-pandas-subsetting/lesson-3-data-preparation.html",
    "href": "book/chapters/lesson-3-pandas-subsetting/lesson-3-data-preparation.html",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "import pandas as pd\n\nDataset: https://portal.edirepository.org/nis/mapbrowse?packageid=edi.649.6\nMetadata: https://portal.edirepository.org/nis/metadataviewer?packageid=edi.649.6\nMore info: https://marinemitigation.msi.ucsb.edu/data?field_species_target_id=&field_data_category_target_id%5B44%5D=44\nThese data describe annual estimates of bird density collected as part of the SONGS San Dieguito Wetland Restoration mitigation monitoring program designed to evaluate compliance of the restoration project with conditions of the SONGS permit. Monitoring began in 2012 in the San Dieguito Wetlands and Tijuana Estuary in San Diego County, CA, Carpinteria Salt Marsh in Santa Barbara County, CA, and Mugu Lagoon in Ventura County, CA. The abundance of birds is determined from twenty plots spread across each wetland.\nCSM = Carpinteria Salt Marsh MUL = Mugu Lagoon SDW = San Dieguito Wetland TJE = Tijuana Estuary\n\ndf = pd.read_csv('data/wetland_ts_bird_abundance-2024-06-12_14-38-10.csv')\ndf.head()\n\n\n\n\n\n\n\n\nyear\nsurvey\ndate\nwetland_code\nmodule_code\nbird_plot_number\nstart_time\ncloud_cover\ntemperature\nwind_speed\nwind_direction\nprecipitation_code\nbird_count_visibility_code\nspecies_id\nspecies_code\ngenus_name\nspecies_name\nbird_flight_code\ncount\nbird_plot_area_acres\n\n\n\n\n0\n2010\n1\n2010-02-03\nCSM\nCSM\n1\n04:04:00\n-99999\n60\n-99999\nNOT RECORDED\nNONE\nGOOD\n9\nCOHA\nAccipiter\ncooperii\nA\n0\n3.71\n\n\n1\n2010\n1\n2010-02-03\nCSM\nCSM\n1\n04:04:00\n-99999\n60\n-99999\nNOT RECORDED\nNONE\nGOOD\n9\nCOHA\nAccipiter\ncooperii\nG\n0\n3.71\n\n\n2\n2010\n1\n2010-02-03\nCSM\nCSM\n1\n04:04:00\n-99999\n60\n-99999\nNOT RECORDED\nNONE\nGOOD\n13\nSSHA\nAccipiter\nstriatus\nA\n0\n3.71\n\n\n3\n2010\n1\n2010-02-03\nCSM\nCSM\n1\n04:04:00\n-99999\n60\n-99999\nNOT RECORDED\nNONE\nGOOD\n13\nSSHA\nAccipiter\nstriatus\nG\n0\n3.71\n\n\n4\n2010\n1\n2010-02-03\nCSM\nCSM\n1\n04:04:00\n-99999\n60\n-99999\nNOT RECORDED\nNONE\nGOOD\n20\nSPSA\nActitis\nmacularius\nA\n0\n3.71\n\n\n\n\n\n\n\n\ndef month_to_season(month):\n    \"\"\"\n    Returns the season ('winter', 'spring', 'summer', 'fall') for a given month (1-12).\n    \"\"\"    \n    if month in [12, 1, 2]:\n        return 'winter'\n    elif month in [3, 4, 5]:\n        return 'spring'\n    elif month in [6, 7, 8]:\n        return 'summer'\n    elif month in [9, 10, 11]:\n        return 'fall'\n\n# Add column with seasons\ndf['season'] = pd.to_datetime(df.date).dt.month.apply(month_to_season)\n\n# Calculate number of species registered per (year, season, wetland) combination\n# Rows where 'count' is 0 are filtered out to do this\nspecies = df[df['count']&gt;0].groupby(['year','wetland_code','season'])['species_code'].nunique().reset_index()\n\n# Pivot the DataFrame: create columns for wetland-season pairs\nspecies = species.pivot(index='year', columns=['wetland_code','season'], values='species_code')\n\n# Flatten the MultiIndex column names\nspecies.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in species.columns]\n\n# Reset the index to make 'year' a column\nspecies = species.reset_index()\n\n# Reorder column names\nspecies = species[['year', \n    'CSM_winter','CSM_spring','CSM_fall',\n    'MUL_winter','MUL_spring','MUL_fall',\n    'SDW_winter','SDW_spring','SDW_fall',\n    'TJE_winter','TJE_spring','TJE_fall' ]]\nspecies\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n5\n2015\n44.0\n42.0\n45.0\n58.0\n50.0\n51.0\n71.0\n61.0\n65.0\n73.0\n76.0\n64.0\n\n\n6\n2016\n41.0\n36.0\n47.0\n63.0\n48.0\n58.0\n67.0\n62.0\n57.0\n76.0\n76.0\n58.0\n\n\n7\n2017\n46.0\n41.0\n43.0\n57.0\n54.0\n53.0\n66.0\n45.0\n54.0\n72.0\n63.0\n57.0\n\n\n8\n2018\n48.0\n48.0\n44.0\n56.0\n54.0\n57.0\n55.0\n49.0\n51.0\n66.0\n60.0\n55.0\n\n\n9\n2019\n39.0\n39.0\n40.0\n57.0\n52.0\n53.0\n54.0\n55.0\n53.0\n63.0\n54.0\n50.0\n\n\n10\n2020\n46.0\nNaN\n47.0\n56.0\nNaN\n66.0\n57.0\nNaN\n58.0\n54.0\n40.0\n54.0\n\n\n11\n2021\n47.0\n44.0\n53.0\n54.0\n55.0\n60.0\n57.0\n58.0\n57.0\n53.0\n68.0\n51.0\n\n\n12\n2022\n40.0\n46.0\n49.0\n60.0\n55.0\n65.0\n57.0\n60.0\n57.0\n60.0\n61.0\n60.0\n\n\n13\n2023\n56.0\n43.0\n36.0\n72.0\n59.0\n53.0\n64.0\n63.0\n33.0\n60.0\n56.0\n38.0\n\n\n\n\n\n\n\n\nspecies.to_csv('wetlands_seasonal_bird_diversity.csv',index=False)\n\n\n\n# total bird counts by year, wetland, and season\nseasonal = df.groupby(['year','wetland_code','season'])['count'].sum().reset_index()\n\n# make columns = wetland codes x season pairs\npivot = seasonal.pivot(index='year', columns=['wetland_code','season'], values='count')\n\n# flatten column names\npivot.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in pivot.columns]\npivot = pivot.reset_index()\npivot\n\n\n# check: every year,wetland,season combination has the same number of species listed\n# so all species are listed even if they have 0 counts\ndf.groupby(['year','wetland_code','season'])['species_code'].nunique().nunique()\n\n\n# get number of different species seen per season\ndf[df['count']!=0].groupby(['year','wetland_code','season'])['species_code'].nunique().reset_index()\n\n\nspecies.columns\n\n\ndf.groupby(['year','wetland_code','survey'])['count'].sum()\ndf[(df.year==2010) & (df.wetland_code == 'CSM') & (df.survey == 1)]['count'].sum()\n\nsubset = df[(df.year==2010) & (df.wetland_code == 'SDW') & (df.season == 'fall') & (df['count'] != 0)]\nprint(len(subset.bird_plot_number.unique()))\nprint(subset.groupby('bird_plot_number')['count'].sum())\nsubset.sort_values(by='count', ascending=False).head()\n\n\nabundance = pd.read_csv('wetland_ps_bird_abundance_and_richness-2024-06-12_14-47-54.csv')\nabundance.head()\n\n\nabundance.wetland_code.unique()\n\n\n# checking plot numbers are not repeated in a year\nfor year in range(2010,2024):\n plot_num = len(abundance.loc[(abundance.wetland_code == 'SDW') & (abundance.year == 2013),'bird_plot_number'].unique())\n n = len(abundance[(abundance.wetland_code == 'SDW') & (abundance.year == 2013)])\n if n != plot_num:\n    print(\"doesn't match\")"
  },
  {
    "objectID": "book/chapters/lesson-2-series-dataframes.html",
    "href": "book/chapters/lesson-2-series-dataframes.html",
    "title": "1 pandas series and data frames",
    "section": "",
    "text": "In this lesson we introduce the two core objects in the pandas library, the pandas.Series and the pandas.DataFrame. The overall goal is to gain familiarity with these two objects, understand their relation to each other, and review Python data structures such as dictionaries and lists.\n\n\nBy the end of this lesson, students will be able to:\n\nExplain the relation between pandas.Series and pandas.DataFrame\nConstruct simple pandas.Series and pandas.DataFrame from scratch using different initalization methods\nPerform simple operations on pandas.Series\nNavigate the pandas documentation to look for attributes and methods of pandas.Series and pandas.DataFrame\n\n\n\n\npandas [1] [2] is a Python package to wrangle and analyze tabular data. It is built on top of NumPy and has become the core tool for doing data analysis in Python.\nThe standard abbreviation for pandas is pd. Here we will import it together with NumPy:\n\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n\n\nConvention: importing packages\n\n\n\nAlways import all your packages in a single cell at the top of you notebook! Following the PEP 8 - Style Guide for Python Code [3], each package or library import should be in a separate line.\n\n\n\n\n\n\nThe first core object of pandas is the series. A series is a one-dimensional array of indexed data.\n\n\n\nImage adapted from Introduction to GeoPandas.\n\n\nA pandas.Series having an index is the main difference between a pandas.Series and a NumPy array. Let‚Äôs see the difference:\n\n# A numpy array\narr = np.random.randn(4) # random values from std normal distribution\nprint(type(arr))\nprint(arr, \"\\n\")\n\n# A pandas series made from the previous array\ns = pd.Series(arr)\nprint(type(s))\nprint(s)\n\n&lt;class 'numpy.ndarray'&gt;\n[-0.11699598  0.6988026  -0.02075373  0.83663288] \n\n&lt;class 'pandas.core.series.Series'&gt;\n0   -0.116996\n1    0.698803\n2   -0.020754\n3    0.836633\ndtype: float64\n\n\nNotice the index is printed as part of the pandas.Series while, although the np.array is indexable, the index is not part of this data structure. Printing the pandas.Series also shows the values and their data type.\n\n\n\nThe basic method to create a pandas.Series is to call\ns = pd.Series(data, index=index)\nThe data parameter can be:\n\na list or NumPy array,\na Python dictionary, or\na single number, boolean (True/False), or string.\n\nThe index parameter is optional, if we wish to include it, it must be a list of list of indices of the same length as data.\n\n\nLet‚Äôs create a pandas.Series from a NumPy array. To use this method we need to pass a NumPy array (or a list of objects that can be converted to NumPy types) as data. Here, we will also include the list [2023, 2024, 2025] to be used as an index:\n\n# A series from a numpy array \npd.Series(np.arange(3), index=[2023, 2024, 2025])\n\n2023    0\n2024    1\n2025    2\ndtype: int64\n\n\n\n\n\nHere we create a pandas.Series from a list of strings. Remember that the index parameter is optional. If we don‚Äôt include it, the default is to make the index equal to [0,...,len(data)-1]. For example:\n\n# A series from a list of strings with default index\npd.Series(['EDS 220', 'EDS 222', 'EDS 223', 'EDS 242'])\n\n0    EDS 220\n1    EDS 222\n2    EDS 223\n3    EDS 242\ndtype: object\n\n\n\n\n\nRecall that a dictionary is a set of key-value pairs. If we create a pandas.Series via a dictionary the keys will become the index and the values the corresponding data.\n\n# Construct dictionary\nd = {'key_0':2, 'key_1':'3', 'key_2':5}\n\n# Initialize series using a dictionary\npd.Series(d)\n\nkey_0    2\nkey_1    3\nkey_2    5\ndtype: object\n\n\n\n\n\n\n\n\ndtype: object\n\n\n\nNotice that in this and the previous example the data type of the values in the series is object. This data type in pandas usually indicates that the series is made up of strings. However, we can see in this example that the object data type can also indicate a mix of strings and numbers.\n\n\n\n\n\nIf we only provide a single number, boolean, or string as the data for the series, we need to provide an index. The value will be repeated to match the length of the index. Here, we create a series from a single float number with an index given by a list of strings:\n\npd.Series(3.0, index = ['A', 'B', 'C'])\n\nA    3.0\nB    3.0\nC    3.0\ndtype: float64\n\n\n\n\n\n\nArithmetic operations work on series and so most NumPy functions. For example:\n\n# Define a series\ns = pd.Series([98,73,65],index=['Andrea', 'Beth', 'Carolina'])\n\n# Divide each element in series by 10\nprint(s /10, '\\n')\n\n# Take the exponential of each element in series\nprint(np.exp(s), '\\n')\n\n# Original series is unchanged\nprint(s)\n\nAndrea      9.8\nBeth        7.3\nCarolina    6.5\ndtype: float64 \n\nAndrea      3.637971e+42\nBeth        5.052394e+31\nCarolina    1.694889e+28\ndtype: float64 \n\nAndrea      98\nBeth        73\nCarolina    65\ndtype: int64\n\n\nWe can also produce new pandas.Series with True/False values indicating whether the elements in a series satisfy a condition or not:\n\ns &gt; 70\n\nAndrea       True\nBeth         True\nCarolina    False\ndtype: bool\n\n\nThis kind of simple conditions on pandas.Series will be key when we are selecting data from data frames.\n\n\n\nIn pandas we can represent a missing, NULL, or NA value with the float value numpy.nan, which stands for ‚Äúnot a number‚Äù. Let‚Äôs construct a small series with some NA values represented this way:\n\n# Series with NAs in it\ns = pd.Series([1, 2, np.nan, 4, np.nan])\ns\n\n0    1.0\n1    2.0\n2    NaN\n3    4.0\n4    NaN\ndtype: float64\n\n\nNotice the data type of the values it he series is still float64.\nThe hasnans attribute for a pandas.Series returns True if there are any NA values in it and false otherwise:\n\n# Check if series has NAs\ns.hasnans\n\nTrue\n\n\nAfter detecting there are Na values, we might be intersted in knowing which elements in the series are NAs. We can do this using the isna method:\n\ns.isna()\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n\n\nThe ouput is a pandas.Series of boolean values indicating if an element in the row at the given index is np.nan (True = is NA) or not (False = not NA).\n\n\n\n\n\n\nCheck-in\n\n\n\n\nThe integer number -999 is often used to represent missing values. Create a pandas.Series named s with four integer values, two of which are -999. The index of this series should be the the letters A through D.\n\n\n\nIn the pandas.Series documentation, look for the method mask(). Use this method to update the series s so that the -999 values are replaced by NA values. HINT: check the first example in the method‚Äôs documentation.\n\n\n\n\nThere‚Äôs much more to say about pandas.Series, but this is enough to get us going. At this point, we mainly want to know about pandas.Series because pandas.Series are the columns of a pandas.DataFrame.\n\n\n\n\n\nThe pandas.DataFrame is the most used pandas object. It represents tabular data and we can think of it as a spreadhseet. Each column of a pandas.DataFrame is a pandas.Series.\n\n\n\nImage adapted from Introduction to GeoPandas.\n\n\n\n\nThere are many ways of creating a pandas.DataFrame. We present one simple one in this section.\nWe already mentioned each column of a pandas.DataFrame is a pandas.Series. In fact, the pandas.DataFrame is a dictionary of pandas.Series, with each column name being the key and the column values being the key‚Äôs value. Thus, we can create a pandas.DataFrame in this way:\n\n# Initialize dictionary with columns' data \nd = {'col_name_1' : pd.Series(np.arange(3)),\n     'col_name_2' : pd.Series([3.1, 3.2, 3.3]),\n     }\n\n# Create data frame\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n\n\n\ncol_name_1\ncol_name_2\n\n\n\n\n0\n0\n3.1\n\n\n1\n1\n3.2\n\n\n2\n2\n3.3\n\n\n\n\n\n\n\nWe can change the index by changing the index attribute in the data frame:\n\n# Change index\ndf.index = ['a','b','c']\ndf\n\n\n\n\n\n\n\n\ncol_name_1\ncol_name_2\n\n\n\n\na\n0\n3.1\n\n\nb\n1\n3.2\n\n\nc\n2\n3.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWe can access the data frame‚Äôs column names via the columns attribute. Update the column names to C1 and C2 by updating this attribute.\n\n\n\n\n\n\n\nJump to the week 1 discussion section to practice preliminary data exploration with a real world dataset. Then, continue with the next lesson on subsetting data frames.",
    "crumbs": [
      "notes",
      "Tabular data",
      "1 `pandas` series and data frames"
    ]
  },
  {
    "objectID": "book/chapters/lesson-2-series-dataframes.html#learning-objectives",
    "href": "book/chapters/lesson-2-series-dataframes.html#learning-objectives",
    "title": "1 pandas series and data frames",
    "section": "",
    "text": "By the end of this lesson, students will be able to:\n\nExplain the relation between pandas.Series and pandas.DataFrame\nConstruct simple pandas.Series and pandas.DataFrame from scratch using different initalization methods\nPerform simple operations on pandas.Series\nNavigate the pandas documentation to look for attributes and methods of pandas.Series and pandas.DataFrame",
    "crumbs": [
      "notes",
      "Tabular data",
      "1 `pandas` series and data frames"
    ]
  },
  {
    "objectID": "book/chapters/lesson-2-series-dataframes.html#pandas",
    "href": "book/chapters/lesson-2-series-dataframes.html#pandas",
    "title": "1 pandas series and data frames",
    "section": "",
    "text": "pandas [1] [2] is a Python package to wrangle and analyze tabular data. It is built on top of NumPy and has become the core tool for doing data analysis in Python.\nThe standard abbreviation for pandas is pd. Here we will import it together with NumPy:\n\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n\n\nConvention: importing packages\n\n\n\nAlways import all your packages in a single cell at the top of you notebook! Following the PEP 8 - Style Guide for Python Code [3], each package or library import should be in a separate line.",
    "crumbs": [
      "notes",
      "Tabular data",
      "1 `pandas` series and data frames"
    ]
  },
  {
    "objectID": "book/chapters/lesson-2-series-dataframes.html#series",
    "href": "book/chapters/lesson-2-series-dataframes.html#series",
    "title": "1 pandas series and data frames",
    "section": "",
    "text": "The first core object of pandas is the series. A series is a one-dimensional array of indexed data.\n\n\n\nImage adapted from Introduction to GeoPandas.\n\n\nA pandas.Series having an index is the main difference between a pandas.Series and a NumPy array. Let‚Äôs see the difference:\n\n# A numpy array\narr = np.random.randn(4) # random values from std normal distribution\nprint(type(arr))\nprint(arr, \"\\n\")\n\n# A pandas series made from the previous array\ns = pd.Series(arr)\nprint(type(s))\nprint(s)\n\n&lt;class 'numpy.ndarray'&gt;\n[-0.11699598  0.6988026  -0.02075373  0.83663288] \n\n&lt;class 'pandas.core.series.Series'&gt;\n0   -0.116996\n1    0.698803\n2   -0.020754\n3    0.836633\ndtype: float64\n\n\nNotice the index is printed as part of the pandas.Series while, although the np.array is indexable, the index is not part of this data structure. Printing the pandas.Series also shows the values and their data type.\n\n\n\nThe basic method to create a pandas.Series is to call\ns = pd.Series(data, index=index)\nThe data parameter can be:\n\na list or NumPy array,\na Python dictionary, or\na single number, boolean (True/False), or string.\n\nThe index parameter is optional, if we wish to include it, it must be a list of list of indices of the same length as data.\n\n\nLet‚Äôs create a pandas.Series from a NumPy array. To use this method we need to pass a NumPy array (or a list of objects that can be converted to NumPy types) as data. Here, we will also include the list [2023, 2024, 2025] to be used as an index:\n\n# A series from a numpy array \npd.Series(np.arange(3), index=[2023, 2024, 2025])\n\n2023    0\n2024    1\n2025    2\ndtype: int64\n\n\n\n\n\nHere we create a pandas.Series from a list of strings. Remember that the index parameter is optional. If we don‚Äôt include it, the default is to make the index equal to [0,...,len(data)-1]. For example:\n\n# A series from a list of strings with default index\npd.Series(['EDS 220', 'EDS 222', 'EDS 223', 'EDS 242'])\n\n0    EDS 220\n1    EDS 222\n2    EDS 223\n3    EDS 242\ndtype: object\n\n\n\n\n\nRecall that a dictionary is a set of key-value pairs. If we create a pandas.Series via a dictionary the keys will become the index and the values the corresponding data.\n\n# Construct dictionary\nd = {'key_0':2, 'key_1':'3', 'key_2':5}\n\n# Initialize series using a dictionary\npd.Series(d)\n\nkey_0    2\nkey_1    3\nkey_2    5\ndtype: object\n\n\n\n\n\n\n\n\ndtype: object\n\n\n\nNotice that in this and the previous example the data type of the values in the series is object. This data type in pandas usually indicates that the series is made up of strings. However, we can see in this example that the object data type can also indicate a mix of strings and numbers.\n\n\n\n\n\nIf we only provide a single number, boolean, or string as the data for the series, we need to provide an index. The value will be repeated to match the length of the index. Here, we create a series from a single float number with an index given by a list of strings:\n\npd.Series(3.0, index = ['A', 'B', 'C'])\n\nA    3.0\nB    3.0\nC    3.0\ndtype: float64\n\n\n\n\n\n\nArithmetic operations work on series and so most NumPy functions. For example:\n\n# Define a series\ns = pd.Series([98,73,65],index=['Andrea', 'Beth', 'Carolina'])\n\n# Divide each element in series by 10\nprint(s /10, '\\n')\n\n# Take the exponential of each element in series\nprint(np.exp(s), '\\n')\n\n# Original series is unchanged\nprint(s)\n\nAndrea      9.8\nBeth        7.3\nCarolina    6.5\ndtype: float64 \n\nAndrea      3.637971e+42\nBeth        5.052394e+31\nCarolina    1.694889e+28\ndtype: float64 \n\nAndrea      98\nBeth        73\nCarolina    65\ndtype: int64\n\n\nWe can also produce new pandas.Series with True/False values indicating whether the elements in a series satisfy a condition or not:\n\ns &gt; 70\n\nAndrea       True\nBeth         True\nCarolina    False\ndtype: bool\n\n\nThis kind of simple conditions on pandas.Series will be key when we are selecting data from data frames.\n\n\n\nIn pandas we can represent a missing, NULL, or NA value with the float value numpy.nan, which stands for ‚Äúnot a number‚Äù. Let‚Äôs construct a small series with some NA values represented this way:\n\n# Series with NAs in it\ns = pd.Series([1, 2, np.nan, 4, np.nan])\ns\n\n0    1.0\n1    2.0\n2    NaN\n3    4.0\n4    NaN\ndtype: float64\n\n\nNotice the data type of the values it he series is still float64.\nThe hasnans attribute for a pandas.Series returns True if there are any NA values in it and false otherwise:\n\n# Check if series has NAs\ns.hasnans\n\nTrue\n\n\nAfter detecting there are Na values, we might be intersted in knowing which elements in the series are NAs. We can do this using the isna method:\n\ns.isna()\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n\n\nThe ouput is a pandas.Series of boolean values indicating if an element in the row at the given index is np.nan (True = is NA) or not (False = not NA).\n\n\n\n\n\n\nCheck-in\n\n\n\n\nThe integer number -999 is often used to represent missing values. Create a pandas.Series named s with four integer values, two of which are -999. The index of this series should be the the letters A through D.\n\n\n\nIn the pandas.Series documentation, look for the method mask(). Use this method to update the series s so that the -999 values are replaced by NA values. HINT: check the first example in the method‚Äôs documentation.\n\n\n\n\nThere‚Äôs much more to say about pandas.Series, but this is enough to get us going. At this point, we mainly want to know about pandas.Series because pandas.Series are the columns of a pandas.DataFrame.",
    "crumbs": [
      "notes",
      "Tabular data",
      "1 `pandas` series and data frames"
    ]
  },
  {
    "objectID": "book/chapters/lesson-2-series-dataframes.html#data-frames",
    "href": "book/chapters/lesson-2-series-dataframes.html#data-frames",
    "title": "1 pandas series and data frames",
    "section": "",
    "text": "The pandas.DataFrame is the most used pandas object. It represents tabular data and we can think of it as a spreadhseet. Each column of a pandas.DataFrame is a pandas.Series.\n\n\n\nImage adapted from Introduction to GeoPandas.\n\n\n\n\nThere are many ways of creating a pandas.DataFrame. We present one simple one in this section.\nWe already mentioned each column of a pandas.DataFrame is a pandas.Series. In fact, the pandas.DataFrame is a dictionary of pandas.Series, with each column name being the key and the column values being the key‚Äôs value. Thus, we can create a pandas.DataFrame in this way:\n\n# Initialize dictionary with columns' data \nd = {'col_name_1' : pd.Series(np.arange(3)),\n     'col_name_2' : pd.Series([3.1, 3.2, 3.3]),\n     }\n\n# Create data frame\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n\n\n\ncol_name_1\ncol_name_2\n\n\n\n\n0\n0\n3.1\n\n\n1\n1\n3.2\n\n\n2\n2\n3.3\n\n\n\n\n\n\n\nWe can change the index by changing the index attribute in the data frame:\n\n# Change index\ndf.index = ['a','b','c']\ndf\n\n\n\n\n\n\n\n\ncol_name_1\ncol_name_2\n\n\n\n\na\n0\n3.1\n\n\nb\n1\n3.2\n\n\nc\n2\n3.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWe can access the data frame‚Äôs column names via the columns attribute. Update the column names to C1 and C2 by updating this attribute.",
    "crumbs": [
      "notes",
      "Tabular data",
      "1 `pandas` series and data frames"
    ]
  },
  {
    "objectID": "book/chapters/lesson-2-series-dataframes.html#next",
    "href": "book/chapters/lesson-2-series-dataframes.html#next",
    "title": "1 pandas series and data frames",
    "section": "",
    "text": "Jump to the week 1 discussion section to practice preliminary data exploration with a real world dataset. Then, continue with the next lesson on subsetting data frames.",
    "crumbs": [
      "notes",
      "Tabular data",
      "1 `pandas` series and data frames"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html",
    "href": "book/chapters/lesson-4-plotting-pandas.html",
    "title": "3 Basic plotting",
    "section": "",
    "text": "In this lesson we will learn to use the plot() method of a pandas.DataFrame to create simple exploratory graphs from tabular data.\n\n\nBy the end of this lesson students will be able to:\n\nObtain and interpret preliminary information about a pandas.DataFrame using key methods such as info() (structure), describe() (summary statistics), nunique() (unique value counts), unique() (distinct values), and value_counts() (frequency counts)\nCreate simple exploratory plots using the plot() method for pandas.DataFrames o visualize trends and distributions\nUnderstand the concept of performing operations on a pandas.DataFrame in-place\nApply method chaining to combine data selection and plotting, enabling concise and readable code\n\n\n\n\nIn this lesson we will reuse the annual estimates of bird species abundance in four coastal wetlands along the California coast that we used in the previous lesson on subsetting a pandas.DataFrame. This dataset was derived for education purposes for this course from the UCSB SONGS Mitigation Monitoring: Wetland Performance Standard - Bird Abundance and Species Richness dataset [1]. The SONGS dataset was collected as part of the San Onofre Nuclear Generating Station (SONGS) San Dieguito Wetland Restoration monitoring program.\nThe annual bird species abundance estimates is a CSV file with 13 columns and 14 rows. You can see the first three rows below. \n\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n\n\n\n\n\nThe four wetlands where the bird surveys occured are Carpinteria Salt Marsh (CSM), Mugu Lagoon (MUL), the San Dieguito Wetland (SDW), and the Tijuana Estuary (TJE). The values from the second column to the last column correspond to the number of different bird species recorded across the survey sites in each wetland during winter, spring, and fall of a given year. For example, the CSM_fall column has the number of species recorded in fall at Carpinteria Salt Marsh across years. The year column corresponds to the calendar year on which the data was collected. Surveys have happened yearly from 2010 to 2023.\n\n\n\nLet us start by loading the data:\nimport pandas as pd\n\n# Read in file\ndf = pd.read_csv('data/wetlands_seasonal_bird_diversity.csv')\n\n# Check the first five rows\ndf.head()\n\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\nA pandas.DataFrame has a built-in method plot() for plotting. When we call it without specifying any other parameters plot() creates one line plot for each of the columns with numeric data.\n\n# Default plot(): one line plot per column with numeric data\ndf.plot()\n\n\n\n\n\n\n\n\nAs we can see, this doesn‚Äôt make much sense! In particular, look at the x-axis. The default for plot is to use the values of the index as the x-axis values. Let‚Äôs see some examples about how to improve this situation.\n\n\n\nWe can make a line plot of one column against another by using the following the general syntax:\ndf.plot(x='x_values_column', y='y_values_column')\n\n\nIf we want to plot the bird surveys at Carpinteria Salt Marsh across years we can do:\n\n# Birds species registered during winter at CSM yearly\ndf.plot(x='year', y='CSM_winter')\n\n\n\n\n\n\n\n\nWe can do some basic customization specifying other parameters of the plot() method. Some basic ones are:\n\ntitle: title to use for the plot.\nxlabel: name to use for the x-label on x-axis\nylabel: bame to use for the y-label on y-axis\ncolor: change the color of our plot\nlegend: boolean value True or False. True (default) includes the legend, False removes the legend\n\nIn action:\n\ndf.plot(x='year', \n        y='CSM_winter',\n        title='Bird species registered during winter at Carpinteria Salt Marsh',\n        xlabel='Year',\n        ylabel='Number of bird species',\n        color='green',\n        legend=False\n        )\n\n\n\n\n\n\n\n\nYou can see all the optional parameters for the plot() function in the documentation.\n\n\n\n\n\n\nCheck-in\n\n\n\nPlot a graph of the bird surveys at Mugu Lagoon with respect to the years. Include some basic customization.\n\n\n\n\n\n\nWe can plot multiple line plots by updating these parameters in the plot() method:\n\ny : a list of column names that will be plotted against the x-axis\ncolor: a dictionary {'column_1' : 'color_1', 'column_2':'color_2} specifying the color of each column‚Äôs line plot\n\n\n\nLet‚Äôs say we want to compare the bird surveys at the Tijuana Estuary during spring and fall across years.\n\ndf.plot(x='year', \n        y=['TJE_spring', 'TJE_fall'],\n        title = 'Seasonal bird surveys at Tijuana Estuary',\n        xlabel='Year',\n        ylabel='Number of bird species',        \n        color = {'TJE_spring':'#F48FB1',\n                 'TJE_fall': '#AB47BC'\n                 }\n        )\n\n\n\n\n\n\n\n\n\nNotice that for specifying the colors we used a HEX code, this gives us more control over how our graph looks.\nWe can also create separate plots for each column by setting the subset to True.\n\ndf.plot(x='year', \n        y=['TJE_spring', 'TJE_fall'],\n        title = 'Seasonal bird surveys at Tijuana Estuary',\n        xlabel='Year',\n        ylabel='Number of bird species',        \n        color = {'TJE_spring':'#F48FB1',\n                 'TJE_fall': '#AB47BC'\n                 },\n        subplots=True\n        )\n\narray([&lt;Axes: xlabel='Year', ylabel='Number of bird species'&gt;,\n       &lt;Axes: xlabel='Year', ylabel='Number of bird species'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating the index of our data frame‚Äôs to be something other than the default integers numbering the rows can be a useful operation for plotting. To update the index we use the set_index() method for a pandas.DataFrame. The general syntax for setting a column as the new index is:\ndf.set_index('column_name')\nwhere column_name is the name of the column in the data frame df we want to use as new index.\nThis operation does not happen in-place.\n\nA funciton acting in-place means that our original data frame is modified.\nIf the function does not act in-place, a new data frame is created and the original is not modified.\n\nIf we wanted to update our df data frame we could do an explicit assignment to reassign the output of set_index() to df:\ndf = df.set_index('column_name')\nor use the optional inplace parameter:\ndf.set_index('column_name', inplace=True)\n\n\n\n\n\n\nAvoid using the inplace=True argument, favor explicit variable assignments\n\n\n\nCheck the information about the inplace parameter in the set_index() documentation. You will often see the inplace parameter in methods for pandas.DataFrames. The best practice is to avoid inplace=True for better readability and maintainable code. Explicitly assigning the result to a new variable or the same variable makes it clear that an operation has occurred.\n\n\n\n\nIn all our previous examples we used the year column as the x-axis. Since all our bird survey variables are dependent on the year, it makes sense to use the year column as the index of the data frame:\n\n# Update index to be the year column\ndf = df.set_index('year')\ndf.head()\n\n\n\n\n\n\n\n\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\n\n# Simple plot of Carpinteria Salt Marsh winter surveys\ndf.plot(y='CSM_winter')\n\n\n\n\n\n\n\n\nIfn needed, we can reset the index to be the numbering of the rows:\n\ndf = df.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\n\nWithout running the code, give a step-by-step breakdown of what this code is doing:\n\ndf.set_index('year').loc[:,'SDW_winter':'TJE_fall'].plot()\n\nIs this code modifying the data frame df? Why or why not?\nRun the code and examine the graph. Do we have all the necessary information to make sure it make sense to directly compare the surveys at these different sites? \n\n\n\n\n\n\n\nThe code used in the check-in\ndf.set_index('year').loc[:,'SDW_winter':'TJE_fall'].plot()\nis an example of method chaining. Each method in the chain returns an object (typically the same object), allowing the next method to be called directly on the result. This is a powerful technique that makes code concise and readable.\nAn alternative to the previous code chaining could have been:\nyear_index_df = df.set_index('year')\nsubset_df = year_index_df.loc[:,'SDW_winter':'TJE_fall']\nsubset_df.plot()\nWhile this accomplishes the same output, several variables are created along the way and it can be difficult to keep track of what is what. Method chaining is particularly useful in pandas for streamlining multiple data manipulations. However, it should be used with care to avoid overly complex and difficult-to-debug code!\nWe will move on to another dataset for the rest of this lesson.\n\n\n\nFor the next plots we will use the Palmer Penguins dataset [2] developed by Drs. Allison Horst, Alison Hill and Kristen Gorman. This dataset contains size measurements for three penguin species in the Palmer Archipelago, Antarctica.\n\n\n\nThe Palmer Archipelago penguins. Artwork by Dr.¬†Allison Horst.\n\n\nThe dataset has 344 rows and 8 columns. We can see it‚Äôs first three rows below:\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n\nThe data is usually accessed through the palmerpenguins R data package [2]. Today we will access the CSV directly into Python using the URL: https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\nLet‚Äôs start by reading in the data:\n\n# Read in data\nURL = 'https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv'\npenguins = pd.read_csv(URL)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nAnd getting some preliminary information:\n\n# Check column data types and NA values\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n# Simple statistics about numeric columns\npenguins.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n344.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n2008.029070\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n0.818356\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n2007.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n2007.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n2008.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n2009.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n2009.000000\n\n\n\n\n\n\n\nWe can also subset the dataframe to get information about a particular column or groups of columns:\n\n# Count unique values in categorical columns and year\npenguins[['species', 'island', 'sex', 'year']].nunique()\n\nspecies    3\nisland     3\nsex        2\nyear       3\ndtype: int64\n\n\n\n# Get unique values in species column\npenguins.species.unique()\n\narray(['Adelie', 'Gentoo', 'Chinstrap'], dtype=object)\n\n\n\n# Number of values per unique value in species column\npenguins.species.value_counts()\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n\n\n\nAt the beginning of the lesson we talked about how the plot() method creates a line plot by default. The parameter that controls this behaviour is the kind parameter. By changing the value of kind we can create different kinds of plots. Let‚Äôs look at the documentation to see what these values are:\n\n\n\nExtract from the pandas.DataFrame.plot documentation. Accessed on Sept 25,2024\n\n\nNotice the default value of kind is 'line'.\nLet‚Äôs change the kind parameter to create some different plots.\n\n\n\nSuppose we want to visualy compare the flipper length against the body mass, we can do this with a scatterplot:\n\npenguins.plot(kind='scatter',\n              x='flipper_length_mm', \n              y='body_mass_g')\n\n\n\n\n\n\n\n\nWe can update some other arguments to customize the graph:\n\npenguins.plot(kind='scatter',\n        x='flipper_length_mm', \n        y='body_mass_g',\n        title='Flipper length and body mass for Palmer penguins',\n        xlabel='Flipper length (mm)',\n        ylabel='Body mass (g)',\n        color='#ff3b01',\n        alpha=0.4  # controls transparency\n        )\n\n\n\n\n\n\n\n\n\n\n\nWe can create bar plots of our data setting kind='bar' in the plot() method.\nFor example, let‚Äôs say we want to get data about the 10 penguins with lowest body mass. We can first select this data using the nsmallest() method for series:\n\nsmallest = penguins.body_mass_g.nsmallest(10).sort_values()\nsmallest\n\n314    2700.0\n58     2850.0\n64     2850.0\n54     2900.0\n98     2900.0\n116    2900.0\n298    2900.0\n104    2925.0\n47     2975.0\n44     3000.0\nName: body_mass_g, dtype: float64\n\n\nWe can then plot this data as a bar plot\n\nsmallest.plot(kind='bar')\n\n\n\n\n\n\n\n\nIf we wanted to look at other data for these smallest penguins we can use the index of the smallest pandas.Series to select those rows in the original penguins data frame using loc:\n\npenguins.loc[smallest.index]\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n314\nChinstrap\nDream\n46.9\n16.6\n192.0\n2700.0\nfemale\n2008\n\n\n58\nAdelie\nBiscoe\n36.5\n16.6\n181.0\n2850.0\nfemale\n2008\n\n\n64\nAdelie\nBiscoe\n36.4\n17.1\n184.0\n2850.0\nfemale\n2008\n\n\n54\nAdelie\nBiscoe\n34.5\n18.1\n187.0\n2900.0\nfemale\n2008\n\n\n98\nAdelie\nDream\n33.1\n16.1\n178.0\n2900.0\nfemale\n2008\n\n\n116\nAdelie\nTorgersen\n38.6\n17.0\n188.0\n2900.0\nfemale\n2009\n\n\n298\nChinstrap\nDream\n43.2\n16.6\n187.0\n2900.0\nfemale\n2007\n\n\n104\nAdelie\nBiscoe\n37.9\n18.6\n193.0\n2925.0\nfemale\n2009\n\n\n47\nAdelie\nDream\n37.5\n18.9\n179.0\n2975.0\nNaN\n2007\n\n\n44\nAdelie\nDream\n37.0\n16.9\n185.0\n3000.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n\nWe can create a histogram of our data setting kind='hist' in plot().\n\n# Using plot without subsetting data - a mess again\npenguins.plot(kind='hist')\n\n\n\n\n\n\n\n\nTo gain actual information, let‚Äôs subset the data before plotting it. For example, suppose we want to do a preliminary graph for the distribution of flipper length. We could do it in this way:\n\n# Distribution of flipper length measurements\n# First select data, then plot\npenguins['flipper_length_mm'].plot(kind='hist',\n                                title='Penguin flipper lengths',\n                                xlabel='Flipper length (mm)',\n                                grid=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\n\nSelect the bill_length_mm and bill_depth_mm columns in the penguins dataframe and then update the kind parameter to box to make boxplots of the bill length and bill depth.  \nCreate a histogram of the flipper length of female gentoo penguins.",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#learning-objectives",
    "href": "book/chapters/lesson-4-plotting-pandas.html#learning-objectives",
    "title": "3 Basic plotting",
    "section": "",
    "text": "By the end of this lesson students will be able to:\n\nObtain and interpret preliminary information about a pandas.DataFrame using key methods such as info() (structure), describe() (summary statistics), nunique() (unique value counts), unique() (distinct values), and value_counts() (frequency counts)\nCreate simple exploratory plots using the plot() method for pandas.DataFrames o visualize trends and distributions\nUnderstand the concept of performing operations on a pandas.DataFrame in-place\nApply method chaining to combine data selection and plotting, enabling concise and readable code",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#about-the-data",
    "href": "book/chapters/lesson-4-plotting-pandas.html#about-the-data",
    "title": "3 Basic plotting",
    "section": "",
    "text": "In this lesson we will reuse the annual estimates of bird species abundance in four coastal wetlands along the California coast that we used in the previous lesson on subsetting a pandas.DataFrame. This dataset was derived for education purposes for this course from the UCSB SONGS Mitigation Monitoring: Wetland Performance Standard - Bird Abundance and Species Richness dataset [1]. The SONGS dataset was collected as part of the San Onofre Nuclear Generating Station (SONGS) San Dieguito Wetland Restoration monitoring program.\nThe annual bird species abundance estimates is a CSV file with 13 columns and 14 rows. You can see the first three rows below. \n\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n\n\n\n\n\nThe four wetlands where the bird surveys occured are Carpinteria Salt Marsh (CSM), Mugu Lagoon (MUL), the San Dieguito Wetland (SDW), and the Tijuana Estuary (TJE). The values from the second column to the last column correspond to the number of different bird species recorded across the survey sites in each wetland during winter, spring, and fall of a given year. For example, the CSM_fall column has the number of species recorded in fall at Carpinteria Salt Marsh across years. The year column corresponds to the calendar year on which the data was collected. Surveys have happened yearly from 2010 to 2023.",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#plot-method",
    "href": "book/chapters/lesson-4-plotting-pandas.html#plot-method",
    "title": "3 Basic plotting",
    "section": "",
    "text": "Let us start by loading the data:\nimport pandas as pd\n\n# Read in file\ndf = pd.read_csv('data/wetlands_seasonal_bird_diversity.csv')\n\n# Check the first five rows\ndf.head()\n\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\nA pandas.DataFrame has a built-in method plot() for plotting. When we call it without specifying any other parameters plot() creates one line plot for each of the columns with numeric data.\n\n# Default plot(): one line plot per column with numeric data\ndf.plot()\n\n\n\n\n\n\n\n\nAs we can see, this doesn‚Äôt make much sense! In particular, look at the x-axis. The default for plot is to use the values of the index as the x-axis values. Let‚Äôs see some examples about how to improve this situation.",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#line-plots",
    "href": "book/chapters/lesson-4-plotting-pandas.html#line-plots",
    "title": "3 Basic plotting",
    "section": "",
    "text": "We can make a line plot of one column against another by using the following the general syntax:\ndf.plot(x='x_values_column', y='y_values_column')\n\n\nIf we want to plot the bird surveys at Carpinteria Salt Marsh across years we can do:\n\n# Birds species registered during winter at CSM yearly\ndf.plot(x='year', y='CSM_winter')\n\n\n\n\n\n\n\n\nWe can do some basic customization specifying other parameters of the plot() method. Some basic ones are:\n\ntitle: title to use for the plot.\nxlabel: name to use for the x-label on x-axis\nylabel: bame to use for the y-label on y-axis\ncolor: change the color of our plot\nlegend: boolean value True or False. True (default) includes the legend, False removes the legend\n\nIn action:\n\ndf.plot(x='year', \n        y='CSM_winter',\n        title='Bird species registered during winter at Carpinteria Salt Marsh',\n        xlabel='Year',\n        ylabel='Number of bird species',\n        color='green',\n        legend=False\n        )\n\n\n\n\n\n\n\n\nYou can see all the optional parameters for the plot() function in the documentation.\n\n\n\n\n\n\nCheck-in\n\n\n\nPlot a graph of the bird surveys at Mugu Lagoon with respect to the years. Include some basic customization.",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#multiple-line-plots",
    "href": "book/chapters/lesson-4-plotting-pandas.html#multiple-line-plots",
    "title": "3 Basic plotting",
    "section": "",
    "text": "We can plot multiple line plots by updating these parameters in the plot() method:\n\ny : a list of column names that will be plotted against the x-axis\ncolor: a dictionary {'column_1' : 'color_1', 'column_2':'color_2} specifying the color of each column‚Äôs line plot\n\n\n\nLet‚Äôs say we want to compare the bird surveys at the Tijuana Estuary during spring and fall across years.\n\ndf.plot(x='year', \n        y=['TJE_spring', 'TJE_fall'],\n        title = 'Seasonal bird surveys at Tijuana Estuary',\n        xlabel='Year',\n        ylabel='Number of bird species',        \n        color = {'TJE_spring':'#F48FB1',\n                 'TJE_fall': '#AB47BC'\n                 }\n        )\n\n\n\n\n\n\n\n\n\nNotice that for specifying the colors we used a HEX code, this gives us more control over how our graph looks.\nWe can also create separate plots for each column by setting the subset to True.\n\ndf.plot(x='year', \n        y=['TJE_spring', 'TJE_fall'],\n        title = 'Seasonal bird surveys at Tijuana Estuary',\n        xlabel='Year',\n        ylabel='Number of bird species',        \n        color = {'TJE_spring':'#F48FB1',\n                 'TJE_fall': '#AB47BC'\n                 },\n        subplots=True\n        )\n\narray([&lt;Axes: xlabel='Year', ylabel='Number of bird species'&gt;,\n       &lt;Axes: xlabel='Year', ylabel='Number of bird species'&gt;],\n      dtype=object)",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#updating-the-index",
    "href": "book/chapters/lesson-4-plotting-pandas.html#updating-the-index",
    "title": "3 Basic plotting",
    "section": "",
    "text": "Updating the index of our data frame‚Äôs to be something other than the default integers numbering the rows can be a useful operation for plotting. To update the index we use the set_index() method for a pandas.DataFrame. The general syntax for setting a column as the new index is:\ndf.set_index('column_name')\nwhere column_name is the name of the column in the data frame df we want to use as new index.\nThis operation does not happen in-place.\n\nA funciton acting in-place means that our original data frame is modified.\nIf the function does not act in-place, a new data frame is created and the original is not modified.\n\nIf we wanted to update our df data frame we could do an explicit assignment to reassign the output of set_index() to df:\ndf = df.set_index('column_name')\nor use the optional inplace parameter:\ndf.set_index('column_name', inplace=True)\n\n\n\n\n\n\nAvoid using the inplace=True argument, favor explicit variable assignments\n\n\n\nCheck the information about the inplace parameter in the set_index() documentation. You will often see the inplace parameter in methods for pandas.DataFrames. The best practice is to avoid inplace=True for better readability and maintainable code. Explicitly assigning the result to a new variable or the same variable makes it clear that an operation has occurred.\n\n\n\n\nIn all our previous examples we used the year column as the x-axis. Since all our bird survey variables are dependent on the year, it makes sense to use the year column as the index of the data frame:\n\n# Update index to be the year column\ndf = df.set_index('year')\ndf.head()\n\n\n\n\n\n\n\n\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\n\n# Simple plot of Carpinteria Salt Marsh winter surveys\ndf.plot(y='CSM_winter')\n\n\n\n\n\n\n\n\nIfn needed, we can reset the index to be the numbering of the rows:\n\ndf = df.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\nyear\nCSM_winter\nCSM_spring\nCSM_fall\nMUL_winter\nMUL_spring\nMUL_fall\nSDW_winter\nSDW_spring\nSDW_fall\nTJE_winter\nTJE_spring\nTJE_fall\n\n\n\n\n0\n2010\n39.0\n40.0\n50.0\n45.0\nNaN\n61.0\nNaN\n75.0\n85.0\nNaN\nNaN\n81.0\n\n\n1\n2011\n48.0\n44.0\nNaN\n58.0\n52.0\nNaN\n78.0\n74.0\nNaN\n67.0\n70.0\nNaN\n\n\n2\n2012\n51.0\n43.0\n49.0\n57.0\n58.0\n53.0\n71.0\n72.0\n73.0\n70.0\n63.0\n69.0\n\n\n3\n2013\n42.0\n46.0\n38.0\n60.0\n58.0\n62.0\n69.0\n70.0\n70.0\n69.0\n74.0\n64.0\n\n\n4\n2014\n38.0\n43.0\n45.0\n49.0\n52.0\n57.0\n61.0\n78.0\n71.0\n60.0\n81.0\n62.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\n\nWithout running the code, give a step-by-step breakdown of what this code is doing:\n\ndf.set_index('year').loc[:,'SDW_winter':'TJE_fall'].plot()\n\nIs this code modifying the data frame df? Why or why not?\nRun the code and examine the graph. Do we have all the necessary information to make sure it make sense to directly compare the surveys at these different sites?",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#method-chaining",
    "href": "book/chapters/lesson-4-plotting-pandas.html#method-chaining",
    "title": "3 Basic plotting",
    "section": "",
    "text": "The code used in the check-in\ndf.set_index('year').loc[:,'SDW_winter':'TJE_fall'].plot()\nis an example of method chaining. Each method in the chain returns an object (typically the same object), allowing the next method to be called directly on the result. This is a powerful technique that makes code concise and readable.\nAn alternative to the previous code chaining could have been:\nyear_index_df = df.set_index('year')\nsubset_df = year_index_df.loc[:,'SDW_winter':'TJE_fall']\nsubset_df.plot()\nWhile this accomplishes the same output, several variables are created along the way and it can be difficult to keep track of what is what. Method chaining is particularly useful in pandas for streamlining multiple data manipulations. However, it should be used with care to avoid overly complex and difficult-to-debug code!\nWe will move on to another dataset for the rest of this lesson.",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#about-the-data-1",
    "href": "book/chapters/lesson-4-plotting-pandas.html#about-the-data-1",
    "title": "3 Basic plotting",
    "section": "",
    "text": "For the next plots we will use the Palmer Penguins dataset [2] developed by Drs. Allison Horst, Alison Hill and Kristen Gorman. This dataset contains size measurements for three penguin species in the Palmer Archipelago, Antarctica.\n\n\n\nThe Palmer Archipelago penguins. Artwork by Dr.¬†Allison Horst.\n\n\nThe dataset has 344 rows and 8 columns. We can see it‚Äôs first three rows below:\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#data-exploration",
    "href": "book/chapters/lesson-4-plotting-pandas.html#data-exploration",
    "title": "3 Basic plotting",
    "section": "",
    "text": "The data is usually accessed through the palmerpenguins R data package [2]. Today we will access the CSV directly into Python using the URL: https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\nLet‚Äôs start by reading in the data:\n\n# Read in data\nURL = 'https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv'\npenguins = pd.read_csv(URL)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nAnd getting some preliminary information:\n\n# Check column data types and NA values\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n# Simple statistics about numeric columns\npenguins.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n344.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n2008.029070\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n0.818356\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n2007.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n2007.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n2008.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n2009.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n2009.000000\n\n\n\n\n\n\n\nWe can also subset the dataframe to get information about a particular column or groups of columns:\n\n# Count unique values in categorical columns and year\npenguins[['species', 'island', 'sex', 'year']].nunique()\n\nspecies    3\nisland     3\nsex        2\nyear       3\ndtype: int64\n\n\n\n# Get unique values in species column\npenguins.species.unique()\n\narray(['Adelie', 'Gentoo', 'Chinstrap'], dtype=object)\n\n\n\n# Number of values per unique value in species column\npenguins.species.value_counts()\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#kind-argument-in-plot",
    "href": "book/chapters/lesson-4-plotting-pandas.html#kind-argument-in-plot",
    "title": "3 Basic plotting",
    "section": "",
    "text": "At the beginning of the lesson we talked about how the plot() method creates a line plot by default. The parameter that controls this behaviour is the kind parameter. By changing the value of kind we can create different kinds of plots. Let‚Äôs look at the documentation to see what these values are:\n\n\n\nExtract from the pandas.DataFrame.plot documentation. Accessed on Sept 25,2024\n\n\nNotice the default value of kind is 'line'.\nLet‚Äôs change the kind parameter to create some different plots.",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#scatter-plots",
    "href": "book/chapters/lesson-4-plotting-pandas.html#scatter-plots",
    "title": "3 Basic plotting",
    "section": "",
    "text": "Suppose we want to visualy compare the flipper length against the body mass, we can do this with a scatterplot:\n\npenguins.plot(kind='scatter',\n              x='flipper_length_mm', \n              y='body_mass_g')\n\n\n\n\n\n\n\n\nWe can update some other arguments to customize the graph:\n\npenguins.plot(kind='scatter',\n        x='flipper_length_mm', \n        y='body_mass_g',\n        title='Flipper length and body mass for Palmer penguins',\n        xlabel='Flipper length (mm)',\n        ylabel='Body mass (g)',\n        color='#ff3b01',\n        alpha=0.4  # controls transparency\n        )",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#bar-plots",
    "href": "book/chapters/lesson-4-plotting-pandas.html#bar-plots",
    "title": "3 Basic plotting",
    "section": "",
    "text": "We can create bar plots of our data setting kind='bar' in the plot() method.\nFor example, let‚Äôs say we want to get data about the 10 penguins with lowest body mass. We can first select this data using the nsmallest() method for series:\n\nsmallest = penguins.body_mass_g.nsmallest(10).sort_values()\nsmallest\n\n314    2700.0\n58     2850.0\n64     2850.0\n54     2900.0\n98     2900.0\n116    2900.0\n298    2900.0\n104    2925.0\n47     2975.0\n44     3000.0\nName: body_mass_g, dtype: float64\n\n\nWe can then plot this data as a bar plot\n\nsmallest.plot(kind='bar')\n\n\n\n\n\n\n\n\nIf we wanted to look at other data for these smallest penguins we can use the index of the smallest pandas.Series to select those rows in the original penguins data frame using loc:\n\npenguins.loc[smallest.index]\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n314\nChinstrap\nDream\n46.9\n16.6\n192.0\n2700.0\nfemale\n2008\n\n\n58\nAdelie\nBiscoe\n36.5\n16.6\n181.0\n2850.0\nfemale\n2008\n\n\n64\nAdelie\nBiscoe\n36.4\n17.1\n184.0\n2850.0\nfemale\n2008\n\n\n54\nAdelie\nBiscoe\n34.5\n18.1\n187.0\n2900.0\nfemale\n2008\n\n\n98\nAdelie\nDream\n33.1\n16.1\n178.0\n2900.0\nfemale\n2008\n\n\n116\nAdelie\nTorgersen\n38.6\n17.0\n188.0\n2900.0\nfemale\n2009\n\n\n298\nChinstrap\nDream\n43.2\n16.6\n187.0\n2900.0\nfemale\n2007\n\n\n104\nAdelie\nBiscoe\n37.9\n18.6\n193.0\n2925.0\nfemale\n2009\n\n\n47\nAdelie\nDream\n37.5\n18.9\n179.0\n2975.0\nNaN\n2007\n\n\n44\nAdelie\nDream\n37.0\n16.9\n185.0\n3000.0\nfemale\n2007",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/chapters/lesson-4-plotting-pandas.html#histograms",
    "href": "book/chapters/lesson-4-plotting-pandas.html#histograms",
    "title": "3 Basic plotting",
    "section": "",
    "text": "We can create a histogram of our data setting kind='hist' in plot().\n\n# Using plot without subsetting data - a mess again\npenguins.plot(kind='hist')\n\n\n\n\n\n\n\n\nTo gain actual information, let‚Äôs subset the data before plotting it. For example, suppose we want to do a preliminary graph for the distribution of flipper length. We could do it in this way:\n\n# Distribution of flipper length measurements\n# First select data, then plot\npenguins['flipper_length_mm'].plot(kind='hist',\n                                title='Penguin flipper lengths',\n                                xlabel='Flipper length (mm)',\n                                grid=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\n\nSelect the bill_length_mm and bill_depth_mm columns in the penguins dataframe and then update the kind parameter to box to make boxplots of the bill length and bill depth.  \nCreate a histogram of the flipper length of female gentoo penguins.",
    "crumbs": [
      "notes",
      "Tabular data",
      "3 Basic plotting"
    ]
  },
  {
    "objectID": "book/appendices/comments-guidelines.html",
    "href": "book/appendices/comments-guidelines.html",
    "title": "Commenting code",
    "section": "",
    "text": "In this section, we‚Äôll explore how to write clear and helpful comments in your code. Good comments make your code easier to understand and maintain, for both you and others. By following a few guidelines, you can write comments that are professional, consistent, and aligned with best practices.\nAs you read through this, remember: Your comments will evolve hand-in-hand with your programming. Don‚Äôt get caught up on ‚Äúwriting the perfect comments‚Äù! Start with correctly formatting your comments and thinking about their content and move on from there.\n\n\n\nComment should start with a pound sign # followed by a single space, then the text.\nThe first word in the comment should be capitalized [1].\nAlways use proper spelling.\nPeriods at the end of short comments are optional, but you should be consistent across your code.\n\n\n\nüö´ Typos, inconsistent capitalization, spacing, and punctuation\n#calculate teh average temp from dataset.\naverage_temp = sum(temperatures) / len(temperatures)\n\n    ##      Apply the TEMPERATURE correction\ncorrected_temp = average_temp + correction_factor\n‚úÖ Comments follow all the basic standards\n# Calculate average temperature from dataset\naverage_temp = sum(temperatures) / len(temperatures)\n\n# Apply temperature correction\ncorrected_temp = average_temp + correction_factor\n\n\n\n\n\n\nAs stated in the Python PEP 8 [1] guide:\n\nComments that contradict the code are worse than no comments. Always make a priority of keeping the comments up-to-date when the code changes!\n\nOutdated comments and comments that contradict the code can cause confusion and lead to bugs.\n\n\nüö´ Comment is outdated and contradicts the code\n# Convert temperature from Fahrenheit to Celsius\ndf['temp_f'] = df['temp_c'] * 9/5 + 32\n‚úÖ Comments must be consistent with the code\n# Convert temperature from Celsius to Fahrenheit\ndf['temp_f'] = df['temp_c'] * 9/5 + 32\n\n\n\n\nMake comments concise and easy to understand. Avoid long-winded explanations. If the code is self-explanatory (which may vary by audience) then minimal commenting is needed.\n\n\nüö´ Too long and redundant\n# In this part of the code, we are filtering the DataFrame 'df' to keep only \n# the rows where the value in the 'region_type' column is equal to 'wetland'. \n# This will give us data specifically for wetland regions.\nwetland_data = df[df['region_type'] == 'wetland']\n‚úÖ Concise and clear comments\n# Filter rows where the region is a wetland\nwetland_data = df[df['region_type'] == 'wetland']\n\n\n\n\n\n\nWant a longer explanation? Use a markdown cell!\n\n\n\nKeeping comments short does not mean there‚Äôs no place for in-depth, detailed explanations while learning programming. If you are using a Jupyter notebook (or another file format that supports combining markdown with code), use a markdown cell for longer explanations instead of adding them as comments to your code.\n\n\n\n\n\n\nComments should be professional and avoid jokes, personal remarks, or irrelevant information.\n\n\nüö´ Unprofessional comment with casual remarks\n# Time to crunch some numbers and save the planet! üåç\ntotal_emissions = df['emissions'].sum()\n‚úÖ Professional and relevant comment\n# Calculate the total carbon emissions for the region\ntotal_emissions = df['emissions'].sum()\n\n\n\n\nWhile long or redundant comments can clutter our code and decrease readability, under-commenting can make our code obscure and difficult to share with others (including our future selves!)\n\n\nüö´ No comments make it unclear what the code is doing\nyear = '2017'\nbbox = [-112.826843, 32.974108, -111.184387, 33.863574]\ncollection = 'io-biodiversity'\n\nsearch = catalog.search(collections=[collection], \n                        bbox=bbox,\n                        datetime=year)\n‚úÖ Comments clarify code\n# Parameters for search in cloud catalog\nyear = '2017'\nbbox = [-112.826843, 32.974108, -111.184387, 33.863574]  # Phoenix bounding box\ncollection = 'io-biodiversity'  # Biodiversity Intactness data\n\nsearch = catalog.search(collections=[collection], \n                        bbox=bbox,\n                        datetime=year)\n\n\n\n\n\n\n\nInline comments are comments on the same line as the code. Best practices are:\n\nUse in-line comments sparingly.\nKeep them short and place them two spaces after the code.\n\n\n\nüö´ In-line comments are overused and don‚Äôt follow regular spacing and capitalization\nyears = [2000, 2005, 2010, 2015, 2020]#list of years for x-axis\nco2_levels = [370, 380, 390, 400, 410]  #CO2 levels for y-axis\n\nplt.plot(years, co2_levels)        # Plot years against CO2 levels\nplt.title('CO2 Levels Over Time')  # Add a title\nplt.xlabel('Year')                 # x-axis = Year\nplt.ylabel('CO2 Levels (ppm)')     # y-axis = CO2 Levels (ppm)\nplt.grid(True) # Enable grid on the plot\nplt.axhline(y=400, color='r', linestyle='--', label='400 ppm Threshold')\nplt.show()         #display the plot\n‚úÖ In-line comments are sparingly used and focused\nyears = [2000, 2005, 2010, 2015, 2020]\nco2_levels = [370, 380, 390, 400, 410]\n\n# Plot to see trend in atmospheric CO2 concentrations\nplt.plot(years, co2_levels)\n\nplt.title('CO2 Levels Over Time')\nplt.xlabel('Year')\nplt.ylabel('CO2 Levels (ppm)')\n\nplt.grid(True)  # Show grid for better readability\nplt.axhline(y=400,  # Add a threshold line for 400 ppm\n            color='r', \n            linestyle='--', \n            label='400 ppm Threshold')  \nplt.show()\n\n\n\n\nFor more complex explanations, use block comments spanning multiple lines. Each line should start with a # and be aligned with the code it describes.\n\n\nüö´ Wordy block comment with inconsistent spacing\n# In this code I: \n    # calculate the avg temp from a list of temperatures and then adjust the result.\n    # We first sum the temperatures, divide by the number of entries,\n    # and finally add the correction factor to the calculated average.\n\ntemperatures = [20.5, 21.0, 19.8, 22.3]\naverage_temp = sum(temperatures) / len(temperatures)\ncorrected_temp = average_temp + 1.2\n‚úÖ Concise block comments focusing on the code purpose\n# Calculate the average temperature from a list of temperature readings.\n# Then, apply a correction factor to account for measurement adjustments.\n\ntemperatures = [20.5, 21.0, 19.8, 22.3]\naverage_temp = sum(temperatures) / len(temperatures)\ncorrected_temp = average_temp + 1.2\n\n\n\n\n\n\nAs you advance in your programming journey and code looks more familiar, you can take next steps to improve your code via comments.\n\n\nWhen we are learning to code, it can be super useful to comment on ‚Äòwhat‚Äô each line of code does. That‚Äôs ok! Over time, as you become more comfortable with the syntax, try to focus your comments on ‚Äòwhy‚Äô certain choices were made or when something might not be immediately obvious to others. Use comments to explain why a piece of code exists, what it‚Äôs doing at a high level, or to describe a complex algorithm. Great comments add value to the code, going beyond describing what is clear from the function and variable names.\nYour comments will become more streamlined and naturally shift from technical to conceptual as your coding skills improve!\n\n\n‚ö†Ô∏è Comment restates in plain language what the code does\n# Assign the maximum value of the array to x\nx = find_max_value(array)  \n‚úÖ Comment explains the rationale behind the code\n# Find the largest value to normalize the data\nx = find_max_value(array)  \n\n\n\n\nIt‚Äôs ok to err on over-commenting when you are beginning your coding journey. As you advance, focus on not over-commenting obvious code, as this can clutter your code and reduce readability.\n\n\n‚ö†Ô∏è Redundant comments clutter the code\ntemperatures = [20.5, 21.0, 19.8, 22.3, 24.1]  # List of temperatures\n\n# Initialize total to 0\ntotal = 0  \n\n# Loop through each temperature in the list\nfor temp in temperatures:\n    total += temp  # Add the current temperature to the total\n\n# Calculate the average by dividing the total by the number of temperatures\naverage_temp = total / len(temperatures)\n\nprint(average_temp)  # Print the average temperature\n‚úÖ Comments focus on important information\ntemperatures = [20.5, 21.0, 19.8, 22.3, 24.1]\n\n# Calculate the total and average temperature\ntotal = 0\nfor temp in temperatures:\n    total += temp\n\naverage_temp = total / len(temperatures)\nprint(average_temp)\n\n\n\n\nThe phrase ‚Äúcode smells‚Äù refers to symptoms in our code that may indicate deeper problems with its design or structure. Sometimes comments are used to explain overly-complicated, instead of making the code as self-explanatory as possible. If that happens, then comments are being used as deodorant for smelly code [2]. Avoid comments as deodorant and, instead, work on making your code simple and understandable. Of course, you will learn better techniques to improve your code with time!\n\n\n‚ö†Ô∏è Comments compensate for obscure variable names and overly-complicated code\n# List of pollutant concentrations (in ppm)\na = [50, 120, 85, 30, 95, 110, 70]  # 'a' stands for air quality measurements\n\n# Filter out pollutant readings that are greater than 100 ppm (considered outliers)\nb = []  # 'b' stands for valid pollutant readings\nfor x in a:  # 'x' is the individual pollutant reading\n    if x &lt;= 100: \n        b.append(x)  # Add valid readings to 'b'\n\nprint(b)\n‚úÖ Self-explanatory code needs less comments\n# List of pollutant concentrations (in ppm)\npollutant_readings = [50, 120, 85, 30, 95, 110, 70]\n\n# Filter out pollutant readings that are greater than 100 ppm (considered outliers)\nvalid_readings = [reading for reading in pollutant_readings if reading &lt;= 100]\n\nprint(valid_readings)\n\n\n\n\n\n\nKeep it clean and consistent: Consistently use proper capitalization, spacing, spelling, and indentation.\nKeep comments up-to-date: Make sure your comments always match what the code is doing.\nBe clear and concise: Write concise comments. Keep them focused, and avoid adding personal notes or jokes. When possible, write comments that explain why your code is doing something, not just how.\nWrite clean code first: Focus on making your code clear and easy to understand. Use comments to make things even clearer, not to explain complicated or messy code.\nCommenting is a learning process! Don‚Äôt stress about writing ‚Äúperfect comments‚Äù. Focus on the basics, and let your commenting style evolve naturally as you improve your coding skills!\n\n\n\n\nHappy Commenting image created with Dall-E 4.",
    "crumbs": [
      "notes",
      "Appendices",
      "Commenting code"
    ]
  },
  {
    "objectID": "book/appendices/comments-guidelines.html#the-basics",
    "href": "book/appendices/comments-guidelines.html#the-basics",
    "title": "Commenting code",
    "section": "",
    "text": "Comment should start with a pound sign # followed by a single space, then the text.\nThe first word in the comment should be capitalized [1].\nAlways use proper spelling.\nPeriods at the end of short comments are optional, but you should be consistent across your code.\n\n\n\nüö´ Typos, inconsistent capitalization, spacing, and punctuation\n#calculate teh average temp from dataset.\naverage_temp = sum(temperatures) / len(temperatures)\n\n    ##      Apply the TEMPERATURE correction\ncorrected_temp = average_temp + correction_factor\n‚úÖ Comments follow all the basic standards\n# Calculate average temperature from dataset\naverage_temp = sum(temperatures) / len(temperatures)\n\n# Apply temperature correction\ncorrected_temp = average_temp + correction_factor",
    "crumbs": [
      "notes",
      "Appendices",
      "Commenting code"
    ]
  },
  {
    "objectID": "book/appendices/comments-guidelines.html#content",
    "href": "book/appendices/comments-guidelines.html#content",
    "title": "Commenting code",
    "section": "",
    "text": "As stated in the Python PEP 8 [1] guide:\n\nComments that contradict the code are worse than no comments. Always make a priority of keeping the comments up-to-date when the code changes!\n\nOutdated comments and comments that contradict the code can cause confusion and lead to bugs.\n\n\nüö´ Comment is outdated and contradicts the code\n# Convert temperature from Fahrenheit to Celsius\ndf['temp_f'] = df['temp_c'] * 9/5 + 32\n‚úÖ Comments must be consistent with the code\n# Convert temperature from Celsius to Fahrenheit\ndf['temp_f'] = df['temp_c'] * 9/5 + 32\n\n\n\n\nMake comments concise and easy to understand. Avoid long-winded explanations. If the code is self-explanatory (which may vary by audience) then minimal commenting is needed.\n\n\nüö´ Too long and redundant\n# In this part of the code, we are filtering the DataFrame 'df' to keep only \n# the rows where the value in the 'region_type' column is equal to 'wetland'. \n# This will give us data specifically for wetland regions.\nwetland_data = df[df['region_type'] == 'wetland']\n‚úÖ Concise and clear comments\n# Filter rows where the region is a wetland\nwetland_data = df[df['region_type'] == 'wetland']\n\n\n\n\n\n\nWant a longer explanation? Use a markdown cell!\n\n\n\nKeeping comments short does not mean there‚Äôs no place for in-depth, detailed explanations while learning programming. If you are using a Jupyter notebook (or another file format that supports combining markdown with code), use a markdown cell for longer explanations instead of adding them as comments to your code.\n\n\n\n\n\n\nComments should be professional and avoid jokes, personal remarks, or irrelevant information.\n\n\nüö´ Unprofessional comment with casual remarks\n# Time to crunch some numbers and save the planet! üåç\ntotal_emissions = df['emissions'].sum()\n‚úÖ Professional and relevant comment\n# Calculate the total carbon emissions for the region\ntotal_emissions = df['emissions'].sum()\n\n\n\n\nWhile long or redundant comments can clutter our code and decrease readability, under-commenting can make our code obscure and difficult to share with others (including our future selves!)\n\n\nüö´ No comments make it unclear what the code is doing\nyear = '2017'\nbbox = [-112.826843, 32.974108, -111.184387, 33.863574]\ncollection = 'io-biodiversity'\n\nsearch = catalog.search(collections=[collection], \n                        bbox=bbox,\n                        datetime=year)\n‚úÖ Comments clarify code\n# Parameters for search in cloud catalog\nyear = '2017'\nbbox = [-112.826843, 32.974108, -111.184387, 33.863574]  # Phoenix bounding box\ncollection = 'io-biodiversity'  # Biodiversity Intactness data\n\nsearch = catalog.search(collections=[collection], \n                        bbox=bbox,\n                        datetime=year)",
    "crumbs": [
      "notes",
      "Appendices",
      "Commenting code"
    ]
  },
  {
    "objectID": "book/appendices/comments-guidelines.html#special-types-of-comments",
    "href": "book/appendices/comments-guidelines.html#special-types-of-comments",
    "title": "Commenting code",
    "section": "",
    "text": "Inline comments are comments on the same line as the code. Best practices are:\n\nUse in-line comments sparingly.\nKeep them short and place them two spaces after the code.\n\n\n\nüö´ In-line comments are overused and don‚Äôt follow regular spacing and capitalization\nyears = [2000, 2005, 2010, 2015, 2020]#list of years for x-axis\nco2_levels = [370, 380, 390, 400, 410]  #CO2 levels for y-axis\n\nplt.plot(years, co2_levels)        # Plot years against CO2 levels\nplt.title('CO2 Levels Over Time')  # Add a title\nplt.xlabel('Year')                 # x-axis = Year\nplt.ylabel('CO2 Levels (ppm)')     # y-axis = CO2 Levels (ppm)\nplt.grid(True) # Enable grid on the plot\nplt.axhline(y=400, color='r', linestyle='--', label='400 ppm Threshold')\nplt.show()         #display the plot\n‚úÖ In-line comments are sparingly used and focused\nyears = [2000, 2005, 2010, 2015, 2020]\nco2_levels = [370, 380, 390, 400, 410]\n\n# Plot to see trend in atmospheric CO2 concentrations\nplt.plot(years, co2_levels)\n\nplt.title('CO2 Levels Over Time')\nplt.xlabel('Year')\nplt.ylabel('CO2 Levels (ppm)')\n\nplt.grid(True)  # Show grid for better readability\nplt.axhline(y=400,  # Add a threshold line for 400 ppm\n            color='r', \n            linestyle='--', \n            label='400 ppm Threshold')  \nplt.show()\n\n\n\n\nFor more complex explanations, use block comments spanning multiple lines. Each line should start with a # and be aligned with the code it describes.\n\n\nüö´ Wordy block comment with inconsistent spacing\n# In this code I: \n    # calculate the avg temp from a list of temperatures and then adjust the result.\n    # We first sum the temperatures, divide by the number of entries,\n    # and finally add the correction factor to the calculated average.\n\ntemperatures = [20.5, 21.0, 19.8, 22.3]\naverage_temp = sum(temperatures) / len(temperatures)\ncorrected_temp = average_temp + 1.2\n‚úÖ Concise block comments focusing on the code purpose\n# Calculate the average temperature from a list of temperature readings.\n# Then, apply a correction factor to account for measurement adjustments.\n\ntemperatures = [20.5, 21.0, 19.8, 22.3]\naverage_temp = sum(temperatures) / len(temperatures)\ncorrected_temp = average_temp + 1.2",
    "crumbs": [
      "notes",
      "Appendices",
      "Commenting code"
    ]
  },
  {
    "objectID": "book/appendices/comments-guidelines.html#next-level",
    "href": "book/appendices/comments-guidelines.html#next-level",
    "title": "Commenting code",
    "section": "",
    "text": "As you advance in your programming journey and code looks more familiar, you can take next steps to improve your code via comments.\n\n\nWhen we are learning to code, it can be super useful to comment on ‚Äòwhat‚Äô each line of code does. That‚Äôs ok! Over time, as you become more comfortable with the syntax, try to focus your comments on ‚Äòwhy‚Äô certain choices were made or when something might not be immediately obvious to others. Use comments to explain why a piece of code exists, what it‚Äôs doing at a high level, or to describe a complex algorithm. Great comments add value to the code, going beyond describing what is clear from the function and variable names.\nYour comments will become more streamlined and naturally shift from technical to conceptual as your coding skills improve!\n\n\n‚ö†Ô∏è Comment restates in plain language what the code does\n# Assign the maximum value of the array to x\nx = find_max_value(array)  \n‚úÖ Comment explains the rationale behind the code\n# Find the largest value to normalize the data\nx = find_max_value(array)  \n\n\n\n\nIt‚Äôs ok to err on over-commenting when you are beginning your coding journey. As you advance, focus on not over-commenting obvious code, as this can clutter your code and reduce readability.\n\n\n‚ö†Ô∏è Redundant comments clutter the code\ntemperatures = [20.5, 21.0, 19.8, 22.3, 24.1]  # List of temperatures\n\n# Initialize total to 0\ntotal = 0  \n\n# Loop through each temperature in the list\nfor temp in temperatures:\n    total += temp  # Add the current temperature to the total\n\n# Calculate the average by dividing the total by the number of temperatures\naverage_temp = total / len(temperatures)\n\nprint(average_temp)  # Print the average temperature\n‚úÖ Comments focus on important information\ntemperatures = [20.5, 21.0, 19.8, 22.3, 24.1]\n\n# Calculate the total and average temperature\ntotal = 0\nfor temp in temperatures:\n    total += temp\n\naverage_temp = total / len(temperatures)\nprint(average_temp)\n\n\n\n\nThe phrase ‚Äúcode smells‚Äù refers to symptoms in our code that may indicate deeper problems with its design or structure. Sometimes comments are used to explain overly-complicated, instead of making the code as self-explanatory as possible. If that happens, then comments are being used as deodorant for smelly code [2]. Avoid comments as deodorant and, instead, work on making your code simple and understandable. Of course, you will learn better techniques to improve your code with time!\n\n\n‚ö†Ô∏è Comments compensate for obscure variable names and overly-complicated code\n# List of pollutant concentrations (in ppm)\na = [50, 120, 85, 30, 95, 110, 70]  # 'a' stands for air quality measurements\n\n# Filter out pollutant readings that are greater than 100 ppm (considered outliers)\nb = []  # 'b' stands for valid pollutant readings\nfor x in a:  # 'x' is the individual pollutant reading\n    if x &lt;= 100: \n        b.append(x)  # Add valid readings to 'b'\n\nprint(b)\n‚úÖ Self-explanatory code needs less comments\n# List of pollutant concentrations (in ppm)\npollutant_readings = [50, 120, 85, 30, 95, 110, 70]\n\n# Filter out pollutant readings that are greater than 100 ppm (considered outliers)\nvalid_readings = [reading for reading in pollutant_readings if reading &lt;= 100]\n\nprint(valid_readings)",
    "crumbs": [
      "notes",
      "Appendices",
      "Commenting code"
    ]
  },
  {
    "objectID": "book/appendices/comments-guidelines.html#takeaways",
    "href": "book/appendices/comments-guidelines.html#takeaways",
    "title": "Commenting code",
    "section": "",
    "text": "Keep it clean and consistent: Consistently use proper capitalization, spacing, spelling, and indentation.\nKeep comments up-to-date: Make sure your comments always match what the code is doing.\nBe clear and concise: Write concise comments. Keep them focused, and avoid adding personal notes or jokes. When possible, write comments that explain why your code is doing something, not just how.\nWrite clean code first: Focus on making your code clear and easy to understand. Use comments to make things even clearer, not to explain complicated or messy code.\nCommenting is a learning process! Don‚Äôt stress about writing ‚Äúperfect comments‚Äù. Focus on the basics, and let your commenting style evolve naturally as you improve your coding skills!\n\n\n\n\nHappy Commenting image created with Dall-E 4.",
    "crumbs": [
      "notes",
      "Appendices",
      "Commenting code"
    ]
  },
  {
    "objectID": "book/setup.html",
    "href": "book/setup.html",
    "title": "Setup",
    "section": "",
    "text": "All the course materials use the Python programming language and git for version control. To install or make sure you have these in your computer, follow the MEDS Software Installation Guide from 4. Check for git to 9. Install VS Code.\n\n\n\nTo install the libraries needed to execute the code in these notes you should create a conda environment using the environment.yml file in the notes repository.\nTo build the environment:\n\nDownload the environment.yml file in the notes repository. Place it in the directory where you will store the notebooks associated with these notes.\nUsing the terminal, navigate to the directory where the environment.yml file is.\nRun the following command to build the environment:\n\nconda env create -f environment.yml\n\nOnce the building finishes, run the following command to check the new environment was created:\n\nconda env list\nTo activate the environment:\n\nIn your terminal, ‚Ä¶ Finish\nIf you are using VSCode, you should be able open a Python notebok and select the new environment by accessing a drop-down list by clicking on the top right corner.\n\nAdd picture\n\n\n\nYou‚Äôre ready to start coding! The course starts with a Python review in the next section."
  },
  {
    "objectID": "book/setup.html#software-installation",
    "href": "book/setup.html#software-installation",
    "title": "Setup",
    "section": "",
    "text": "All the course materials use the Python programming language and git for version control. To install or make sure you have these in your computer, follow the MEDS Software Installation Guide from 4. Check for git to 9. Install VS Code."
  },
  {
    "objectID": "book/setup.html#python-environment",
    "href": "book/setup.html#python-environment",
    "title": "Setup",
    "section": "",
    "text": "To install the libraries needed to execute the code in these notes you should create a conda environment using the environment.yml file in the notes repository.\nTo build the environment:\n\nDownload the environment.yml file in the notes repository. Place it in the directory where you will store the notebooks associated with these notes.\nUsing the terminal, navigate to the directory where the environment.yml file is.\nRun the following command to build the environment:\n\nconda env create -f environment.yml\n\nOnce the building finishes, run the following command to check the new environment was created:\n\nconda env list\nTo activate the environment:\n\nIn your terminal, ‚Ä¶ Finish\nIf you are using VSCode, you should be able open a Python notebok and select the new environment by accessing a drop-down list by clicking on the top right corner.\n\nAdd picture"
  },
  {
    "objectID": "book/setup.html#next",
    "href": "book/setup.html#next",
    "title": "Setup",
    "section": "",
    "text": "You‚Äôre ready to start coding! The course starts with a Python review in the next section."
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "",
    "text": "This discussion section will guide you through answering questions about water-related conflicts at the Colorado River Basin using data from the U.S. Geological Survey (USGS). In this discussion section, you will:"
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#setup",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#setup",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "Setup",
    "text": "Setup\n\n\n\n\n\n\n\nIn the Taylor server, start a new JupyterLab session or access an active one.\nIn the terminal, use cd to navigate into the eds-220-sections directory. Use pwd to verify eds-220-sections is your current working directory.\nCreate a new Python Notebook inside your eds-220-sections directory and rename it to section-2-co-basin-water-conflicts.ipynb.\nUse the terminal to stage, commit, and push this file to the remote repository. Remember:\n\ngit status : check git status\ngit add FILE-NAME : stage updated file\ngit status : check git status again to confirm\ngit commit -m \"Commit message\" : commit with message\ngit pull : check local repo is up to date (best practice)\ngit push : push changes to upstream repository\n\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOU‚ÄôVE ALL SUCCESSFULLY SET UP YOUR NOTEBOOKS BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#general-directions",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#general-directions",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "General directions",
    "text": "General directions\n\n\n\n\n\n\n\nAdd comments in each one of your code cells.\nOn each exercise, include markdown cells in between your code cells to add titles and information.\nIndications about when to commit and push changes are included, but you are encouraged to commit and push more often."
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#about-the-data",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#about-the-data",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "About the data",
    "text": "About the data\nFor these exercises we will use data about Water Conflict and Crisis Events in the Colorado River Basin [1]. This dataset is stored at ScienceBase,a digital repository from the U.S. Geological Survey (USGS) created to share scientific data products and USGS resources.\nThe dataset is a CSV file containing conflict or crisis around water resource management in the Colorado River Basin. The Colorado River Basin, inhabited by several Native American tribes for centuries, is a crucial water source in the southwestern United States and northern Mexico, supporting over 40 million people, extensive agricultural lands, and diverse ecosystems. Its management is vital due to the region‚Äôs arid climate and the competing demands for water, leading to significant challenges related to water allocation and conservation.\n\n\n\nColorado River Basin. U.S. Bureau of Reclamation."
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#archive-exploration",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#archive-exploration",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "1. Archive exploration",
    "text": "1. Archive exploration\nTake some time to look through the dataset‚Äôs description in the ScienceBase epository. Discuss the following questions with your team:\n\nWhere was the data collected from? \nDuring what time frame were the observations in the dataset collected? \nWhta was the author‚Äôs perceived value of this dataset? \nBriefly discuss anything else that seems like relevant information.\n\nIn your notebook, use a markdown cell to add a brief description of the dataset, including a citation, date of access, and a link to the archive.\n\ncheck git status -&gt; stage changes -&gt; check git status -&gt; commit with message -&gt; pull -&gt; push changes"
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#data-loading",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#data-loading",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "2. Data loading",
    "text": "2. Data loading\n\nIn class we have (so far) loaded data into our workspace both by downloading the file and storing a copy of the dataset in our computer and by accessing directly through a URL. With your team, discuss what can be, in general, the advantages and disadvantages of these two methods of data access.\nImport the Colorado River Basin Water Conflict Table.csv file from the Science Base repository into your workspace using its URL and store it as a variable named df.\n\n\ncheck git status -&gt; stage changes -&gt; check git status -&gt; commit with message -&gt; pull -&gt; push changes\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOU‚ÄôVE ALL SUCCESSFULLY LOADED THE DATA BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#preliminary-data-exploration",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#preliminary-data-exploration",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "3. Preliminary data exploration",
    "text": "3. Preliminary data exploration"
  },
  {
    "objectID": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#unique-stakeholders",
    "href": "discussion-sections-upcoming/ds-2-water-crisis-exploration.html#unique-stakeholders",
    "title": "Exploring water conflicts in the Colorado River Basin",
    "section": "4. Unique stakeholders",
    "text": "4. Unique stakeholders\nWhich were the unique stakeholders?"
  }
]