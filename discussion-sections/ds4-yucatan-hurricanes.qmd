---
title: Yucatan Peninsula Hurricanes
subtitle: Week 4 - Discussion section
week: 4
image: images/yucatan-huracanes.jpg
sidebar: false
jupyter: mpc-env-kernel
---

In this discussion section you will inspect data historical data about hurricanes at the Yucatan Peninsula in Mexico.  In this discussion section, you will:

- Practice breaking down a question into accessible data wrangling steps
- Choose approrpiate packages and methods to carry out your analysis
- Practice finding additional guidance to carry out your analysis online

## Setup

:::{.callout-tip appearance="minimal"}
1. Access the workbench-1 server.

2. Navigate to the`eds-220-sections` directory in the file navigation panel and the terminal.

3. Create a new Python notebook inside your `eds-220-sections` directory and rename it to `section-4-yucatan-hurricanes.ipynb`. 

4. Use the terminal to push this file to you remote repository. 
:::

## General directions
:::{.callout-tip appearance="minimal"}
- Add comments as appropriate along your code.
- Include markdown cells in between your code cells to add titles and information to each exercise
- Commit every time you finish a major step. **Remember to write your commits in the imperative mood.**
:::

## About the data
In this discussion section we will use [historical data about hurricanes in the Yucatan Peninsula  @boose_ecological_2023](https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-hfr.71.23). This dataset is stored in the [Environmental Data Initiative (EDI)](https://edirepository.org) data repository. 


## 1. Archive exploration
a. Take some time to look through the dataset's description and metadata in EDI.

b. In your notebook: use a markdown cell to add a *brief* description of the dataset, including a citation, date of access, and a link to the archive. 


## 2. Data loading and preliminary exploration

a. Back in your notebook, import the `hf071-01-hurricanes.csv` file. Store it in a variable named `hares`. 

<!--
```{python}
import numpy as np
import pandas as pd

URL = 'https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-bnz.55.22&entityid=f01f5d71be949b8c700b6ecd1c42c701'
hares = pd.read_csv(URL)

hares.head()
```
-->

b. Using `pandas` methods, obtain preliminary information and explore this data frame. Consider answering some of these questions:

- What are the dimensions of the dataframe and what are the data types of the columns? Do the data types match what you would expect from each column?
- Are there any columns that have a significant number of NA values?
- What are the minimum and maximum values for the weight and hind feet measurements?
- What are the unique values for some of the categorical columns?
- An explroatory question about the data frame you come up with!


<p style="text-align: center;">
**CHECK IN WITH YOUR TEAM** 
</p>
<p style="text-align: center;">
**MAKE SURE YOU'VE ALL SUCCESSFULLY ACCESSED THE DATA BEFORE CONTINUING**
</p>

<p style="text-align: center;">
**commit, pull, and push changes**
</p>

## 4. Detecting messy values

a. In the metadata section of the EDI repository, find which are the allowed values for the hares' sex. Create a small [table in a markdown cell](https://www.markdownguide.org/extended-syntax/#tables) showing the values and their definitions.

<!--
*Allowed values are:*

| Value | Definition |
| ------| ---------- |
| f | female |
| m | male |
| m?| male not confirmed |
-->

b. Get the number of times each unique sex non-NA value appears.

<!--
```{python}
hares['sex'].value_counts()
```
-->

c. Check the [documentation of `value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html). What is the purpose of the `dropna` parameter and what is its default value? Repeat step (a), this time adding the `dropna=False` parameter to `value_counts()`.
<!--
```{python}
hares['sex'].value_counts(dropna=False)
```
-->

d. Discuss with your team the output of the unique value counts. In particular: 
    i. Do the values in the `sex` column correspond to the values declared in the metadata?
    ii. What could have been potential causes for multiple codes?
    iii. Are there seemingly repated values? If so, what could be the cause?

e. Write code to confirm your suspicions about c-iii.

<!--
```{python}
hares['sex'].unique()
```
-->

<p style="text-align: center;">
**commit, pull, and push changes**
</p>

## 5. Brainstorm

a. Individually, write step-by-step instructions on how you would wrangle the `hares` data frame to clean the values in the `sex` column to have only two classes `female` and `male`. Which codes would you assign to each new class? **Remember:** It’s ok if you don’t know how to code each step - it’s more important to have an idea of what you would like to do.

b. Discuss your step-by-step instructions with your team.

The next exercise will guide you through cleaning the sex codes. There are *many* ways of doing this. The one presented here might not be the same way you thought about doing it - that's ok! This one was designed to practice using the `numpy.select()` function.

## 6. Clean values

a. Create a new column called `sex_simple` using the `numpy.select()` function so that 

- 'F', 'f', and 'f_' in the `sex` column get assigned to 'female', 
- 'M', 'm', and 'm_' get assigned to 'male', and 
- anything else gets assigned `np.nan`

<!--
```{python}
conditions = [(hares.sex=='F') | (hares.sex=='f') |(hares.sex=='f '),
              (hares.sex=='M') | (hares.sex=='m') |(hares.sex=='m ')]

choices = ['female', 
            'male']

hares['sex_simple'] = np.select(conditions, choices, default=np.nan)
hares.head()
```
-->

b. Check the counts of unique values (including NAs) in the new `sex_simple` column.
<!--
```{python}
hares['sex_simple'].value_counts()
```
-->

<p style="text-align: center;">
**commit, pull, and push changes**
</p>

## 7. Calculate mean weight

a. Use `groupby()` to calculate the mean weight by sex using the new column. 

<!--
```{python}
hares.groupby('sex_simple')['weight'].mean()
```
-->

b. Write a full sentence explaining the results you obtained. Don't forget to include units.

<p style="text-align: center;">
**commit, pull, and push changes**
</p>

## 8. Collect your code and explain your results

In a new code cell, collect all the relevant code to obtain to create a streamlined workflow to obtain the final result from exercise 7 starting from importing the data. Your code cell should:

- Only print the final results for mean weight by `sex_simple`. 
- Not include output from intermediate variables or checks.
- Not include methods or functions that do not directly contribute to the analysis (even if they don't print anything ex: `df.head()`).
- If appropriate, combine methods using code chaining instead of creating intermediate variables.
- Comment your code following our class comments guidelines. 
- Use appropriate line breaks and indentation to make code readable.

<!-- 
```{python}
# Load data from EDI data repository
URL = 'https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-bnz.55.22&entityid=f01f5d71be949b8c700b6ecd1c42c701'
hares = pd.read_csv(URL)

# Create new sex_simpel column dropping the uncertain sex codes and 
# simplifying codes without no question mark
conditions = [(hares.sex=='F') | (hares.sex=='f') |(hares.sex=='f '),
              (hares.sex=='M') | (hares.sex=='m') |(hares.sex=='m ')]
choices = ['female', 'male']

hares['sex_simple'] = np.select(conditions, choices, default=np.nan)

# Calculate average hare weight by sex
hares.groupby('sex_simple')['weight'].mean()
```
-->

<!--TODO: add explanation of why this is important-->

<p style="text-align: center;">
**commit, pull, and push changes**
</p>